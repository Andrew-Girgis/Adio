Directory structure:
â””â”€â”€ smallest-inc-cookbook/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ CONTRIBUTING.md
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ best-practices/
    â”‚   â””â”€â”€ voice_agent_prompting_guide.md
    â”œâ”€â”€ blog-code-samples/
    â”‚   â””â”€â”€ pulse-stt-developer-guide/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ env.example
    â”‚       â”œâ”€â”€ package.json
    â”‚       â”œâ”€â”€ requirements.txt
    â”‚       â”œâ”€â”€ demo-app/
    â”‚       â”‚   â”œâ”€â”€ README.md
    â”‚       â”‚   â”œâ”€â”€ eslint.config.mjs
    â”‚       â”‚   â”œâ”€â”€ next.config.ts
    â”‚       â”‚   â”œâ”€â”€ package.json
    â”‚       â”‚   â”œâ”€â”€ postcss.config.mjs
    â”‚       â”‚   â”œâ”€â”€ tsconfig.json
    â”‚       â”‚   â”œâ”€â”€ ws-proxy.js
    â”‚       â”‚   â”œâ”€â”€ ws-server.js
    â”‚       â”‚   â””â”€â”€ src/
    â”‚       â”‚       â””â”€â”€ app/
    â”‚       â”‚           â”œâ”€â”€ globals.css
    â”‚       â”‚           â”œâ”€â”€ layout.tsx
    â”‚       â”‚           â”œâ”€â”€ page.tsx
    â”‚       â”‚           â””â”€â”€ api/
    â”‚       â”‚               â””â”€â”€ transcribe/
    â”‚       â”‚                   â””â”€â”€ route.ts
    â”‚       â”œâ”€â”€ nodejs/
    â”‚       â”‚   â”œâ”€â”€ rest/
    â”‚       â”‚   â”‚   â””â”€â”€ transcription.ts
    â”‚       â”‚   â””â”€â”€ websocket/
    â”‚       â”‚       â”œâ”€â”€ browser_client.js
    â”‚       â”‚       â””â”€â”€ server_client.ts
    â”‚       â”œâ”€â”€ python/
    â”‚       â”‚   â”œâ”€â”€ rest/
    â”‚       â”‚   â”‚   â”œâ”€â”€ async_batch.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ basic_transcription.py
    â”‚       â”‚   â”‚   â””â”€â”€ url_transcription.py
    â”‚       â”‚   â””â”€â”€ websocket/
    â”‚       â”‚       â””â”€â”€ streaming_client.py
    â”‚       â””â”€â”€ utils/
    â”‚           â”œâ”€â”€ audio_preprocessing.py
    â”‚           â””â”€â”€ ffmpeg_commands.sh
    â”œâ”€â”€ speech-to-text/
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ emotion-analyzer/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”‚   â”œâ”€â”€ .env.sample
    â”‚   â”‚   â”œâ”€â”€ backend/
    â”‚   â”‚   â”‚   â””â”€â”€ app.py
    â”‚   â”‚   â””â”€â”€ frontend/
    â”‚   â”‚       â”œâ”€â”€ app.js
    â”‚   â”‚       â”œâ”€â”€ index.html
    â”‚   â”‚       â””â”€â”€ style.css
    â”‚   â”œâ”€â”€ file-transcription/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ .env.sample
    â”‚   â”‚   â”œâ”€â”€ javascript/
    â”‚   â”‚   â”‚   â””â”€â”€ transcribe.js
    â”‚   â”‚   â””â”€â”€ python/
    â”‚   â”‚       â””â”€â”€ transcribe.py
    â”‚   â”œâ”€â”€ getting-started/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ .env.sample
    â”‚   â”‚   â”œâ”€â”€ javascript/
    â”‚   â”‚   â”‚   â””â”€â”€ transcribe.js
    â”‚   â”‚   â””â”€â”€ python/
    â”‚   â”‚       â””â”€â”€ transcribe.py
    â”‚   â”œâ”€â”€ online-meeting-notetaking-bot/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ bot.py
    â”‚   â”‚   â””â”€â”€ .env.sample
    â”‚   â”œâ”€â”€ podcast-summarizer/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ summarize.py
    â”‚   â”‚   â””â”€â”€ .env.sample
    â”‚   â”œâ”€â”€ subtitle-generation/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ .env.sample
    â”‚   â”‚   â”œâ”€â”€ javascript/
    â”‚   â”‚   â”‚   â””â”€â”€ transcribe.js
    â”‚   â”‚   â””â”€â”€ python/
    â”‚   â”‚       â””â”€â”€ transcribe.py
    â”‚   â”œâ”€â”€ websocket/
    â”‚   â”‚   â”œâ”€â”€ jarvis/
    â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ jarvis.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”‚   â”‚   â”œâ”€â”€ stt.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ tts.py
    â”‚   â”‚   â”‚   â””â”€â”€ .env.sample
    â”‚   â”‚   â”œâ”€â”€ realtime-microphone-transcription/
    â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”‚   â”œâ”€â”€ app.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”‚   â”‚   â””â”€â”€ .env.sample
    â”‚   â”‚   â””â”€â”€ streaming-text-output-transcription/
    â”‚   â”‚       â”œâ”€â”€ README.md
    â”‚   â”‚       â”œâ”€â”€ requirements.txt
    â”‚   â”‚       â”œâ”€â”€ .env.sample
    â”‚   â”‚       â”œâ”€â”€ javascript/
    â”‚   â”‚       â”‚   â”œâ”€â”€ package.json
    â”‚   â”‚       â”‚   â””â”€â”€ transcribe.js
    â”‚   â”‚       â””â”€â”€ python/
    â”‚   â”‚           â””â”€â”€ transcribe.py
    â”‚   â”œâ”€â”€ word-level-outputs/
    â”‚   â”‚   â”œâ”€â”€ README.md
    â”‚   â”‚   â”œâ”€â”€ .env.sample
    â”‚   â”‚   â”œâ”€â”€ javascript/
    â”‚   â”‚   â”‚   â””â”€â”€ transcribe.js
    â”‚   â”‚   â””â”€â”€ python/
    â”‚   â”‚       â””â”€â”€ transcribe.py
    â”‚   â””â”€â”€ youtube-summarizer/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ analysis.py
    â”‚       â”œâ”€â”€ app.py
    â”‚       â”œâ”€â”€ requirements.txt
    â”‚       â”œâ”€â”€ transcription.py
    â”‚       â””â”€â”€ youtube.py
    â””â”€â”€ voice-agents/
        â”œâ”€â”€ README.md
        â”œâ”€â”€ agent_with_tools/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ app.py
        â”‚   â”œâ”€â”€ assistant_agent.py
        â”‚   â””â”€â”€ requirements.txt
        â”œâ”€â”€ analytics/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ configure_post_call.py
        â”‚   â”œâ”€â”€ export_transcripts.py
        â”‚   â”œâ”€â”€ get_call_details.py
        â”‚   â”œâ”€â”€ get_calls.py
        â”‚   â””â”€â”€ requirements.txt
        â”œâ”€â”€ atoms_sdk_web_agent/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ eslint.config.mjs
        â”‚   â”œâ”€â”€ next.config.ts
        â”‚   â”œâ”€â”€ package.json
        â”‚   â”œâ”€â”€ postcss.config.mjs
        â”‚   â”œâ”€â”€ tsconfig.json
        â”‚   â”œâ”€â”€ agents-config/
        â”‚   â”‚   â”œâ”€â”€ hackernews-agent.md
        â”‚   â”‚   â”œâ”€â”€ tic-tac-toe-agent.md
        â”‚   â”‚   â””â”€â”€ weather-agent.md
        â”‚   â””â”€â”€ app/
        â”‚       â”œâ”€â”€ globals.css
        â”‚       â”œâ”€â”€ layout.tsx
        â”‚       â”œâ”€â”€ page.tsx
        â”‚       â”œâ”€â”€ api/
        â”‚       â”‚   â”œâ”€â”€ hackernews/
        â”‚       â”‚   â”‚   â””â”€â”€ top/
        â”‚       â”‚   â”‚       â””â”€â”€ route.ts
        â”‚       â”‚   â”œâ”€â”€ invite-agent/
        â”‚       â”‚   â”‚   â””â”€â”€ route.ts
        â”‚       â”‚   â””â”€â”€ tictactoe/
        â”‚       â”‚       â”œâ”€â”€ get-state/
        â”‚       â”‚       â”‚   â””â”€â”€ route.ts
        â”‚       â”‚       â”œâ”€â”€ make-move/
        â”‚       â”‚       â”‚   â””â”€â”€ route.ts
        â”‚       â”‚       â””â”€â”€ new-game/
        â”‚       â”‚           â””â”€â”€ route.ts
        â”‚       â”œâ”€â”€ components/
        â”‚       â”‚   â”œâ”€â”€ TicTacToeBoard.tsx
        â”‚       â”‚   â””â”€â”€ WaveAvatar.tsx
        â”‚       â””â”€â”€ lib/
        â”‚           â””â”€â”€ tictactoe-engine.ts
        â”œâ”€â”€ background_agent/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ app.py
        â”‚   â”œâ”€â”€ requirements.txt
        â”‚   â”œâ”€â”€ sentiment_analyzer.py
        â”‚   â””â”€â”€ support_agent.py
        â”œâ”€â”€ bank_csr/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ app.py
        â”‚   â”œâ”€â”€ audit_logger.py
        â”‚   â”œâ”€â”€ csr_agent.py
        â”‚   â”œâ”€â”€ database.py
        â”‚   â”œâ”€â”€ pyproject.toml
        â”‚   â””â”€â”€ requirements.txt
        â”œâ”€â”€ call_control/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ app.py
        â”‚   â”œâ”€â”€ requirements.txt
        â”‚   â””â”€â”€ support_agent.py
        â”œâ”€â”€ campaigns/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ add_contacts.py
        â”‚   â”œâ”€â”€ create_audience.py
        â”‚   â”œâ”€â”€ create_campaign.py
        â”‚   â”œâ”€â”€ manage_campaign.py
        â”‚   â””â”€â”€ requirements.txt
        â”œâ”€â”€ getting_started/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ app.py
        â”‚   â”œâ”€â”€ my_agent.py
        â”‚   â””â”€â”€ requirements.txt
        â”œâ”€â”€ inbound_ivr/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ app.py
        â”‚   â”œâ”€â”€ ivr_agent.py
        â”‚   â””â”€â”€ requirements.txt
        â”œâ”€â”€ interrupt_control/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ app.py
        â”‚   â”œâ”€â”€ configurable_agent.py
        â”‚   â””â”€â”€ requirements.txt
        â”œâ”€â”€ knowledge_base_rag/
        â”‚   â”œâ”€â”€ README.md
        â”‚   â”œâ”€â”€ requirements.txt
        â”‚   â””â”€â”€ setup_kb.py
        â””â”€â”€ language_switching/
            â”œâ”€â”€ README.md
            â”œâ”€â”€ app.py
            â”œâ”€â”€ language_detector.py
            â”œâ”€â”€ profanity_filter.py
            â”œâ”€â”€ requirements.txt
            â””â”€â”€ support_agent.py

================================================
FILE: README.md
================================================
![Smallest AI](assets/smallest-banner.png)

<div align="center">
  <a href="https://twitter.com/smallest_AI">
    <img src="https://img.shields.io/twitter/url/https/twitter.com/smallest_AI.svg?style=social&label=Follow%20smallest_AI" alt="Twitter">
  </a>
  <a href="https://discord.gg/ywShEyXHBW">
    <img src="https://img.shields.io/discord/1212257329559642112?style=flat&logo=discord&logoColor=white&label=Discord&color=5865F2" alt="Discord">
  </a>
  <a href="https://www.linkedin.com/company/smallest">
    <img src="https://img.shields.io/badge/LinkedIn-Connect-blue" alt="LinkedIn">
  </a>
  <a href="https://www.youtube.com/@smallest_ai">
    <img src="https://img.shields.io/static/v1?message=smallest_ai&logo=youtube&label=&color=FF0000&logoColor=white&labelColor=&style=for-the-badge" height=20 alt="YouTube">
  </a>
</div>

# Smallest AI Cookbook

Smallest AI offers an end-to-end Voice AI suite for developers building real-time voice agents. You can use our Speech-to-Text APIs through Pulse STT for high-accuracy transcription, our Text-to-Speech APIs through Lightning TTS for natural-sounding speech synthesis, or use the Atoms Client to build and operate enterprise-ready Voice Agents with features like tool calling, knowledge bases, and campaign management.

This cookbook contains practical examples and tutorials for building with Smallest AI's APIs. Each example is self-contained and demonstrates a real-world use case â€” from basic transcription to fully autonomous voice agents.

**Documentation:** [Waves (STT & TTS)](https://waves-docs.smallest.ai) Â· [Atoms (Voice Agents)](https://atoms-docs.smallest.ai/dev) Â· [Python SDK](https://github.com/smallest-inc/smallest-python-sdk)

---

## Usage

### Prerequisites

- [uv](https://docs.astral.sh/uv/) (Python package manager)
- Python >= 3.10 (install via `uv python install 3.13` if needed)
- A Smallest AI API key â€” get one at [smallest.ai/console](https://smallest.ai/console)

### Quick Start

Clone the repo, set up a virtual environment, and install the shared dependencies:

```bash
git clone https://github.com/smallest-inc/cookbook.git
cd cookbook
uv venv && uv pip install -r requirements.txt
```

### Set up your API key

Each example reads keys from the environment. The easiest way is to copy the `.env.sample` included in every example directory:

```bash
cd speech-to-text/getting-started
cp .env.sample .env
# Add your keys to .env
```

Or export directly in your shell:

```bash
export SMALLEST_API_KEY="your-api-key-here"
```

### Run an example

```bash
uv run speech-to-text/getting-started/python/transcribe.py recording.wav
```

Some examples need additional dependencies beyond the root `requirements.txt`. Each one has its own `requirements.txt` â€” install before running:

```bash
uv pip install -r speech-to-text/websocket/jarvis/requirements.txt
uv run speech-to-text/websocket/jarvis/jarvis.py
```

For voice agent examples:

```bash
uv pip install -r voice-agents/bank_csr/requirements.txt
uv run voice-agents/bank_csr/app.py
```

### API Keys

- `SMALLEST_API_KEY` â€” [smallest.ai/console](https://smallest.ai/console) â€” Required by all examples
- `OPENAI_API_KEY` â€” [platform.openai.com](https://platform.openai.com/api-keys) â€” Podcast Summarizer, Meeting Notes, Voice Agents
- `GROQ_API_KEY` â€” [console.groq.com](https://console.groq.com) â€” YouTube Summarizer, Jarvis
- `RECALL_API_KEY` â€” [recall.ai](https://recall.ai) â€” Meeting Notes

---

## Speech-to-Text Examples

Convert audio and video to text with industry-leading accuracy. Supports 30+ languages with features like speaker diarization, word timestamps, and emotion detection. Powered by [Pulse STT](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/overview).

- [Getting Started](./speech-to-text/getting-started/) â€” Basic transcription, the simplest way to start
- [Jarvis Voice Assistant](./speech-to-text/websocket/jarvis/) â€” Always-on assistant with wake word detection, LLM reasoning, and TTS
- [Online Meeting Notetaker](./speech-to-text/online-meeting-notetaking-bot/) â€” Join Google Meet / Zoom / Teams via Recall.ai, auto-identify speakers by name, generate structured notes
- [Podcast Summarizer](./speech-to-text/podcast-summarizer/) â€” Transcribe and summarize podcasts with key takeaways using GPT
- [Emotion Analyzer](./speech-to-text/emotion-analyzer/) â€” Visualize speaker emotions across a conversation with interactive charts

**[See all Speech-to-Text examples &rarr;](./speech-to-text/)**

---

## Voice Agents Examples

Build AI voice agents that can talk to anyone on voice or text, in any language, in any voice. The Atoms SDK provides abstractions like KnowledgeBase, Campaigns, and graph-based Workflows to let you build the smartest voice agent for your use case. Powered by the [Atoms SDK](https://atoms-docs.smallest.ai/dev).

### Basics

- [Getting Started](./voice-agents/getting_started/) â€” Create your first agent with `OutputAgentNode`, `generate_response()`, and `AtomsApp`
- [Agent with Tools](./voice-agents/agent_with_tools/) â€” Add tool calling with `@function_tool` and `ToolRegistry`
- [Call Control](./voice-agents/call_control/) â€” Cold/warm transfers and ending a call with `SDKAgentTransferConversationEvent`

### Multi-Node Patterns

- [Background Agent](./voice-agents/background_agent/) â€” `BackgroundAgentNode` for parallel processing, cross-node state sharing
- [Observability](./voice-agents/observability/) â€” Langfuse integration via `BackgroundAgentNode` â€” live traces, tool spans, transcript events
- [Language Switching](./voice-agents/language_switching/) â€” Multi-node agents with dynamic language detection and switching

### Call Handling

- [Inbound IVR](./voice-agents/inbound_ivr/) â€” Intent routing, department transfers, mute/unmute control
- [Interrupt Control](./voice-agents/interrupt_control/) â€” Mute/unmute events, blocking user interruptions during critical speech

### Platform Features

- [Knowledge Base RAG](./voice-agents/knowledge_base_rag/) â€” Attach a knowledge base with PDF upload and URL scraping for grounded responses
- [Campaigns](./voice-agents/campaigns/) â€” Provision bulk outbound calling with audiences and campaign management
- [Analytics](./voice-agents/analytics/) â€” Call logs, transcript exports, post-call metrics

### Advanced

- [Bank CSR](./voice-agents/bank_csr/) â€” Full banking agent â€” SQL queries, multi-round tool chaining, identity verification, FD management, audit logging
- [Multi-Agent Voice AI Dashboard](./voice-agents/atoms_sdk_web_agent/) â€” Real-time dashboard with specialized agents for gaming and utility powered by Atoms SDK.

**[See all Voice Agents examples &rarr;](./voice-agents/)**

---

## Language Support

Each example includes implementations in:

- **Python** â€” Uses `requests`, `websockets`, and standard libraries
- **JavaScript** â€” Uses `node-fetch`, `ws`, and Node.js built-ins

## Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines. In short:

1. Create a folder with a descriptive name
2. Add implementations in `python/` and/or `javascript/` subdirectories
3. Include a `README.md` and `.env.sample`
4. If the example needs deps beyond the root `requirements.txt`, add a local `requirements.txt`
5. Update this root README with your new example

---

## Get Help

- [Discord Community](https://discord.gg/5evETqguJs)
- [Contact Support](https://smallest.ai/contact)



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Smallest AI Cookbook

Thanks for your interest in contributing! This guide will help you add new examples or improve existing ones.

---

## Adding a New Example

### 1. Create the Directory Structure

If your example has **only Python** code, put files directly in the example root:

```
speech-to-text/your-example-name/
â”œâ”€â”€ your_script.py
â”œâ”€â”€ .env.sample
â”œâ”€â”€ requirements.txt      # only if extra deps are needed
â””â”€â”€ README.md
```

If your example has **both Python and JavaScript**, use subdirectories:

```
speech-to-text/your-example-name/
â”œâ”€â”€ python/
â”‚   â””â”€â”€ your_script.py
â”œâ”€â”€ javascript/
â”‚   â””â”€â”€ your_script.js
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ .env.sample
â”œâ”€â”€ requirements.txt      # only if extra deps are needed
â””â”€â”€ README.md
```

### 2. Dependencies

Common libraries are already installed from the root `requirements.txt` (`requests`, `websockets`, `python-dotenv`, `smallestai`, `openai`, `groq`, `loguru`, `streamlit`, `gradio`). **Do not duplicate these.**

If your example needs **extra** packages beyond the root, add a local `requirements.txt` with only those packages:

```txt
# System dependency: sudo apt install ffmpeg (Linux) / brew install ffmpeg (macOS) / winget install ffmpeg (Windows)

yt-dlp>=2024.0.0
```

If your example has **no extra dependencies**, do not create a `requirements.txt`.

### 3. Environment Variables

Create a `.env.sample` listing all required variables:

```bash
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here

# Any other keys needed
OTHER_API_KEY=your-other-api-key-here
```

### 4. Code Guidelines

**Python:**
- Use `#!/usr/bin/env python3` shebang
- Add a docstring with usage and output description
- Read API keys from environment variables using `os.environ.get()` or `python-dotenv`
- Use `requests` for HTTP, `websockets` for WebSocket
- Include type hints where helpful
- Run with `uv run your_script.py`

**JavaScript:**
- Use ES modules or CommonJS consistently
- Read API keys from `process.env`
- Use `node-fetch` for HTTP, `ws` for WebSocket
- Include a `package.json` if dependencies are needed

### 5. README Template

Follow this structure (you can slightly deviate but keep the major sections):

```markdown
# Example Name

One-line description of what this example does.

## Features

- Feature 1
- Feature 2
- Feature 3

## Demo

<!-- gif or screenshot if applicable -->

## Requirements

Base dependencies are already installed from the root
[`requirements.txt`](../../requirements.txt). Additionally:

- Add `SMALLEST_API_KEY` to your `.env`

<!-- If extra deps are needed: -->
Install example-specific dependencies:

\`\`\`bash
uv pip install -r requirements.txt
\`\`\`

## Usage

\`\`\`bash
uv run your_script.py
\`\`\`

## Recommended Usage

- Use case 1
- Use case 2
- For [alternative use case], see [Other Example](../other-example/)

## Key Snippets

<!-- Use "Key Snippets" when showing code blocks,
     "How It Works" when explaining a process/workflow in prose -->

\`\`\`python
# highlight the most important part of your code
\`\`\`

## Example Output

<!-- terminal output, screenshots, or generated files -->

## Documentation

- [Relevant docs link](https://waves-docs.smallest.ai/...)

## Next Steps

- Link to related examples or docs
```

**Section guidelines:**
- **Requirements** â€” Reference the root `requirements.txt` for base deps. Only mention extra deps and required API keys.
- **Recommended Usage** â€” At least 2 actual use-case points, at most 1 alternative suggestion with a link.
- **Key Snippets** vs **How It Works** â€” Use "Key Snippets" when the section primarily contains code blocks. Use "How It Works" for prose explanations of processes or workflows.
- **Structure** vs **Scripts Included** â€” If your example has multiple files, use "Scripts Included" for script-heavy examples or "Structure" for application-like examples, with a codeblock directory tree.

### 6. Update the Category README

Add your example to the table in the relevant category README:
- [`speech-to-text/README.md`](./speech-to-text/README.md) for STT examples
- [`voice-agents/README.md`](./voice-agents/README.md) for Voice Agent examples

---

## Pull Request Process

1. Fork the repository
2. Create a feature branch: `git checkout -b feat/your-example-name`
3. Make your changes
4. Test your example end-to-end with `uv run`
5. Submit a pull request with a clear description

---

## Commit Guidelines

We use [Conventional Commits](https://www.conventionalcommits.org/) for clear commit history.

**Format:** `<type>: <description>`

| Type | Description |
|------|-------------|
| `feat` | New example or feature |
| `fix` | Bug fix |
| `docs` | Documentation changes |
| `refactor` | Code restructuring |
| `chore` | Maintenance tasks |

**Examples:**

```
feat: add youtube-summarizer example
fix: correct API endpoint in meeting-notes
docs: update getting-started README
refactor: simplify transcription parsing
chore: add .env.sample files
```

---

## Code Style

- Keep examples simple and focused on one concept
- Prefer clarity over cleverness
- Add comments for non-obvious logic
- Don't over-engineer â€” these are learning examples

---

## Questions?

- [Discord Community](https://discord.gg/ywShEyXHBW)
- [Open an Issue](https://github.com/smallest-ai/cookbook/issues)



================================================
FILE: requirements.txt
================================================
# Common dependencies used across cookbook examples
# Install with: uv pip install -r requirements.txt
#
# System dependency:
#   sudo apt install ffmpeg    (Linux)
#   brew install ffmpeg         (macOS)
#   winget install ffmpeg       (Windows)

# HTTP requests
requests>=2.28.0

# WebSocket streaming
websockets>=11.0

# Environment variable management
python-dotenv>=1.0.0

# Smallest AI SDK
smallestai>=4.3.0

# Logging
loguru

# LLM integration
openai
groq

# UI frameworks
streamlit
gradio



================================================
FILE: best-practices/voice_agent_prompting_guide.md
================================================
# Voice Agent Prompting Guide

A well-crafted prompt is the difference between a voice agent that frustrates users and one that delights them. This guide distills our learnings from building hundreds of production voice agents into actionable advice.

## Why Prompting Voice Agents is Different

Voice agents operate under constraints that chatbots don't face:

* **Real-time pressure**: Users expect immediate responses. There's no time for the agent to "think" visibly.
* **No visual fallback**: You can't show a list, a form, or a "click here" button. Everything must be spoken.
* **Linear attention**: Users can't skim or scroll back. Information must be delivered in digestible chunks.
* **Interruptions happen**: Users talk over agents, change their minds, go on tangents.
* **Listening is hard**: Users mishear, forget, and zone out. Critical information needs confirmation.

These constraints mean your prompt needs to be clearer, more structured, and more anticipatory than a typical chatbot prompt.


---

## The Anatomy of a Great Voice Agent Prompt

### Recommended Structure

We recommend organizing your prompt into these sections (in this order):

```
1. Role & Objective
2. Personality & Tone
3. Context (if applicable)
4. Instructions / Rules
5. Tools (if applicable)
6. Conversation Flow
7. Guardrails (if applicable)
```

This isn't a rigid templateâ€”you can add, remove, or reorder sections based on your use case. But this structure gives the agent a clear mental model: *who am I, how should I behave, what do I know, what can I do, how should the conversation go, and what are my limits?*

Let's break down each section.


---

### 1. Role & Objective

Start with a single, clear statement of what the agent is and what it's trying to accomplish. This anchors everything else.

**Good example:**

```
You are Maya, a customer service agent for Acme Electronics. Your goal is to help 
customers troubleshoot issues with their devices and, if needed, schedule a repair 
appointment.
```

**Why it works:** The agent knows its name, its company, and its two primary functions. It won't try to sell products or discuss unrelated topics.

**Common mistake:** Vague objectives like "help the customer" or "be useful." These give the agent no direction on *how* to help or what success looks like.


---

### 2. Personality & Tone

Voice agents need personality. Users form impressions within seconds, and a flat, robotic agent creates a poor experienceâ€”even if it's technically correct.

Define:

* **Tone**: Professional? Casual? Warm? Witty?
* **Energy level**: Calm and measured? Upbeat and enthusiastic?
* **Conversational style**: Formal sentences or relaxed, spoken language?

**Good example:**

```
You are friendly and professionalâ€”warm without being overly casual. You speak naturally, 
like a helpful colleague, not like a script. You're patient with confused customers 
and never condescending. When things go wrong, you stay calm and solution-focused.
```

**Why it works:** This gives the agent guardrails for *how* to speak, not just *what* to say. It can adapt its exact words while maintaining a consistent personality.

**Common mistake:** Over-specifying personality with contradictions ("be professional but also super fun and quirky!") or leaving it out entirely.


---

### 3. Context (For Outbound or Pre-Loaded Information)

If your agent already knows something about the user before the conversation starts (common in outbound calls), provide it in a dedicated context section.

**Good example:**

```
## Context
- Customer Name: {{customer_name}}
- Order ID: {{order_id}}
- Issue: {{issue_summary}}
- Previous Interactions: {{interaction_history}}
```

**Why it works:** The agent knows what it knows. It won't ask for information it already has, and it can personalize the conversation from the start.

**Common mistake:** Scattering context throughout the prompt, or worse, not providing it and expecting the agent to ask for information it should already have.


---

### 4. Instructions / Rules

This is where you define the agent's general behaviorâ€”the "how" of the conversation. Think of these as principles the agent should internalize, not a checklist to recite.

**Good instructions are:**

* **Actionable**: The agent knows what to do
* **Specific**: No room for misinterpretation
* **Prioritized**: Most important rules first

**Good example:**

```
## Instructions

- Always verify the customer's identity before discussing account details. Ask for their 
  registered phone number or the last 4 digits of their account number.
- If the customer is frustrated, acknowledge it briefly and focus on solving the problem. 
  Don't over-apologize or dwell on the frustration.
- Never quote exact prices for repairsâ€”direct customers to the pricing page or offer 
  to transfer them to billing.
```

**Common mistake:** Writing instructions as a wall of text. Use bullet points. The agent (and you, when debugging) will thank you.


---

### 5. Tools

If your agent can take actions (check databases, send emails, schedule appointments), define the tools clearly. For each tool, specify:

* **What it does**
* **When to use it**
* **What information is needed first**
* **How to handle the result**

**Good example:**

```
## Tools

### check_order_status
Use this tool to look up the current status of a customer's order.
- **Required**: Order ID
- **Returns**: Order status, estimated delivery date, and tracking link
- **When to use**: After the customer asks about their order and you've confirmed their 
  identity

After calling this tool, summarize the key information conversationally. Don't read 
out raw dataâ€”translate it into natural language.
```

**Why it works:** The agent knows the tool exists, what triggers its use, what it needs beforehand, and how to communicate the results.

**Common mistake:** Listing tools without explaining when to use them, or expecting the agent to figure out the workflow on its own.


---

### 6. Conversation Flow

This is often the most complex section. You're describing how the conversation should unfoldâ€”not as a rigid script, but as a flexible guide.

#### Guide, Don't Script

The most important principle: **give instructions on what to do, not exact words to say.**

**Bad (scripted):**

```
When the user asks for their balance, say: "Your current account balance is $X. 
Is there anything else I can help you with today?"
```

**Good (guided):**

```
When the user asks for their balance, retrieve it using the get_balance tool and 
share it with them. Keep it conversationalâ€”don't just read out numbers.
```

**Why this matters:** Scripted responses sound robotic and repetitive. Guided instructions let the agent adapt to the specific conversation while achieving the same goal.

**Exception:** Some exact phrases *are* requiredâ€”legal disclaimers, compliance statements, or specific brand language. In those cases, be explicit:

```
When closing a sale, you MUST say: "This call may be recorded for quality and 
training purposes."
```

#### Describe Conditional Logic Naturally

Real conversations branch. Users say yes, no, maybe, or something completely unexpected. Your prompt should handle this with natural "if/then" logic.

**Good example:**

```
## Conversation Flow

1. **Opening**: Greet the customer and ask how you can help.

2. **Identify the issue**: Listen to their problem. If it's a billing issue, proceed 
   to the billing flow. If it's a technical issue, proceed to troubleshooting.

3. **Troubleshooting flow**:
   - Ask clarifying questions to understand the problem
   - If the issue can be resolved with basic steps (restart, check connections), 
     guide them through it
   - If the issue persists, offer to schedule a repair appointment
   - If the customer declines the appointment, provide alternative support options 
     (email support, FAQ link)

4. **Closing**: Summarize what was done, confirm next steps if any, and thank them 
   for calling.
```

**Why it works:** The agent understands the overall shape of the conversation and can navigate different paths based on user responses.


---

### 7. Guardrails

Guardrails define what the agent should *never* do. These are your safety netsâ€”protecting users, your brand, and your business.

**Good guardrails are:**

* **Specific**: "Never discuss competitor products" is clearer than "stay on topic"
* **Justified** (to yourself): Each guardrail should exist for a reason
* **Silent**: The agent should embody them without announcing them to the user

**Good example:**

```
## Guardrails

- Never provide medical, legal, or financial advice. If asked, suggest the customer 
  consult a professional.
- Do not discuss pricing for services not in your catalog.
- If a customer becomes abusive or threatening, calmly end the call: "I'm not able 
  to continue this conversation. Please contact us again when you're ready."
- Never share internal processes, employee names, or system details.
```

**Common mistake:** Listing guardrails the agent will never realistically encounter (over-engineering), or forgetting guardrails for obvious edge cases (under-engineering).


---

## Common Prompting Mistakes (And How to Fix Them)

### 1. The Wall of Text

**Problem:** A massive, unstructured paragraph that's hard for both humans and AI to parse.

**Fix:** Use headers, bullet points, and clear sections. Structure is information.


---

### 2. Shouting at the Agent

**Problem:** ALL CAPS, excessive exclamation marks, threatening language ("YOU MUST NEVER!!!").

**Fix:** Write calmly and clearly. The agent responds to clarity, not volume. "Never share pricing" works just as well as "NEVER EVER SHARE PRICING!!!"


---

### 3. Contradictory Instructions

**Problem:** "Be concise" + "Always explain things thoroughly" + "Keep responses under 20 words."

**Fix:** Prioritize. If brevity matters most, say so. If thoroughness matters for certain topics, specify which ones.


---

### 4. Over-Scripting

**Problem:** Exact dialogues for every scenario, leaving no room for natural conversation.

**Fix:** Describe the *goal* of each interaction, not the exact words. Trust the agent to generate natural responses.


---

### 5. Missing the "What If"

**Problem:** The prompt only covers the happy path. When users go off-script, the agent is lost.

**Fix:** Think through common edge cases:

* What if the user doesn't have the information you need?
* What if they want something you can't provide?
* What if they're angry? Confused? In a hurry?


---

### 6. Vague Tool Instructions

**Problem:** "Use the booking tool to make appointments" with no guidance on when, what's needed, or how to handle errors.

**Fix:** Be explicit about triggers, prerequisites, and error handling for every tool.


---

### 7. No Personality

**Problem:** A purely functional prompt that produces robotic responses.

**Fix:** Add a personality section. Even "professional and helpful" is better than nothing.


---

## Advanced Tips

### Information Chunking

Voice users can only hold so much in working memory. If your agent needs to convey multiple pieces of information, break them up with natural pauses and confirmations.

**Instead of:**

```
Your order #12345 containing a blue wireless mouse, black keyboard, and USB hub 
is currently in transit with an estimated delivery of March 15th via FedEx tracking 
number 789456123.
```

**Guide the agent to:**

```
Share order information in digestible pieces. Start with the status, then ask if 
they want details like tracking number or delivery date.
```


---

### The Power of Confirmation

For critical information (phone numbers, dates, amounts), instruct the agent to confirm.

```
For important values like phone numbers, dates, or booking times, repeat them back 
naturally to confirm. Mishearing happensâ€”a quick confirmation prevents bigger problems.
```


---

### Graceful Boundaries

When the agent can't help with something, it shouldn't be rigid or robotic about it.

```
If you can't help with something, say so naturally and offer what you can do instead. 
"I can't change that directly, but I can connect you with someone who can" is better 
than "I am not authorized to perform that action."
```


---

## Quick Reference Checklist

Before deploying your prompt, check:

- [ ] **Clear objective**: Does the agent know what success looks like?
- [ ] **Defined personality**: Will the agent sound like a human or a robot?
- [ ] **Structured flow**: Are the main conversation paths clear?
- [ ] **Tool instructions**: Does the agent know when and how to use each tool?
- [ ] **Edge cases**: Have you handled the common "what ifs"?
- [ ] **Guardrails**: Are the boundaries clear and appropriate?
- [ ] **No contradictions**: Do all instructions align?
- [ ] **Readable format**: Is the prompt easy to scan and understand?


---

## Blueprint: A Complete Example Prompt

Below is a full example prompt that demonstrates everything we've covered. This is an outbound delivery scheduling agentâ€”notice how the conversation flow is organized into clear sections with explicit routing between them.

```markdown
# Role & Objective

You are Sam, a delivery coordinator for HomeStyle Appliances. You're calling customers who have recently purchased large appliances to schedule their delivery.

Your goal is to confirm a delivery time that works for the customer, ensure someone will be available to receive the delivery, and answer any questions about the delivery process.

---

# Personality & Tone

You're friendly, efficient, and helpful. You sound like someone who genuinely wants to make the delivery process easyâ€”not like you're reading from a script. You're patient if customers need to check their calendars or have questions, but you also respect their time and don't ramble.

When customers are frustrated (delayed orders, limited availability), you stay calm and solution-focused. You acknowledge their frustration briefly and move toward solving the problem.

---

# Context

You have the following information about this customer:

- **Customer Name**: {{customer_name}}
- **Phone Number**: {{phone_number}} (last 4 digits: {{phone_last_4}})
- **Order Number**: {{order_number}}
- **Items Ordered**: {{items_ordered}}
- **Delivery Address**: {{delivery_address}}
- **Earliest Available Delivery Date**: {{earliest_delivery_date}}

---

# Instructions

- Start by confirming you're speaking with the right person. Use the last 4 digits of their phone number for verification if needed.
- Always confirm the delivery address before booking a slotâ€”customers sometimes want delivery to a different location.
- If the customer asks about delivery windows, explain that slots are 4-hour windows (morning: 8am-12pm, afternoon: 12pm-4pm, evening: 4pm-8pm).
- Large appliances require someone 18+ to be present to receive the delivery and sign for it. Make sure the customer understands this.
- If no slots work for the customer in the next 2 weeks, offer to add them to the waitlist for cancellations.
- Keep the conversation focusedâ€”friendly, but don't let it drift into unrelated topics.

---

# Tools

## check_availability
Checks available delivery slots for the customer's area.
- **Required**: Delivery address, date range (start and end date)
- **Returns**: List of available slots with date, time window, and slot ID
- **When to use**: After confirming the delivery address, when ready to offer time slots

## book_delivery_slot
Books a specific delivery slot for the customer.
- **Required**: Order number, slot ID, special instructions (if any)
- **Returns**: Confirmation number, confirmed date/time, what to expect
- **When to use**: After the customer has chosen a slot and confirmed the details

## cancel_order
Cancels the customer's order entirely.
- **Required**: Order number, cancellation reason
- **Returns**: Cancellation confirmation, refund timeline
- **When to use**: Only if the customer explicitly wants to cancel their order

## transfer_to_support
Transfers the call to customer support.
- **Required**: Reason for transfer
- **When to use**: For issues outside delivery scheduling (order changes, complaints, refunds)

---

# Conversation Flow

## Opening

Greet the customer and introduce yourself briefly. State the purpose of the call in one line.

**Keep it short**â€”don't overwhelm them with details upfront. Just:
1. Say who you are and where you're calling from
2. Mention it's about their recent order
3. Confirm you're speaking with the right person

If they confirm, proceed to [Confirm Delivery Details]. If they say it's a bad time, offer to call back and ask when works better.

---

## Confirm Delivery Details

Before scheduling, confirm two things:
1. **The items**: Briefly mention what they ordered to make sure they know what this is about
2. **The address**: Confirm the delivery address on file, ask if that's still correct

â†’ If the address is correct: proceed to [Schedule Delivery]
â†’ If they want a different address: note the new address and proceed to [Schedule Delivery]
â†’ If they have questions about their order: go to [Handle Questions]
â†’ If they want to cancel the order: go to [Cancellation Flow]

---

## Schedule Delivery

This is the main flow. Walk through these steps:

### Step 1: Check availability
Use `check_availability` with the confirmed address and the next 2 weeks as the date range.

### Step 2: Offer options
Share 2-3 available options conversationally. Don't read out a long listâ€”offer a few good options and ask what works.

If they need a specific date or time that's not available, check if adjacent slots work. Be flexible in how you present options.

### Step 3: Confirm their choice
Once they pick a slot, confirm the details:
- Date and time window
- Delivery address
- Requirement that someone 18+ must be present

### Step 4: Book the slot
Use `book_delivery_slot` to finalize. Share the confirmation number and let them know they'll receive a confirmation via SMS/email.

### Step 5: Wrap up
â†’ Go to [Closing]

**What if no slots work?**
If nothing in the next 2 weeks works for them:
- Offer to check availability further out
- Offer to add them to the waitlist for cancellations (they'd get a call if an earlier slot opens)
- If they're frustrated, acknowledge it and focus on finding a solution

---

## Handle Questions

Customers often have questions about the delivery process. Common ones:

**"What's the delivery window?"**
Explain that deliveries are scheduled in 4-hour windows: morning (8am-12pm), afternoon (12pm-4pm), or evening (4pm-8pm). The delivery team will call 30 minutes before arrival.

**"Do I need to be there?"**
Yesâ€”someone 18 or older must be present to receive the delivery and sign for it. If they can't be there, they can designate someone else.

**"Will they install it?"**
Basic installation is included for most appliances. For complex installations (gas lines, custom configurations), they may need to schedule separately with our installation team.

**"Can I track the delivery?"**
Yesâ€”on the delivery day, they'll receive a tracking link via SMS with real-time updates.

After answering their questions:
â†’ If they still need to schedule: go back to [Schedule Delivery]
â†’ If they're all set: go to [Closing]
â†’ If they want to cancel: go to [Cancellation Flow]

---

## Cancellation Flow

If the customer wants to cancel their order:

1. Ask if there's anything you can help with to keep the order (different delivery date, address issue, etc.)
2. If they still want to cancel, confirm: "Just to make sure I understandâ€”you'd like to cancel the entire order for {{items_ordered}}?"
3. Ask for the reason (required for processing)
4. Use `cancel_order` to process
5. Confirm the cancellation and explain the refund timeline (typically 5-7 business days)
6. â†’ Go to [Closing]

If the cancellation involves a complaint or issue beyond delivery scheduling, use `transfer_to_support` to connect them with customer support.

---

## Closing

- Summarize what was done (delivery scheduled, questions answered, or cancellation processed)
- Confirm any next steps they should expect (confirmation SMS, delivery day call, refund)
- Ask if there's anything else you can help with
- Thank them and end warmly

---

# Guardrails

- Do not discuss or negotiate pricing, refunds, or order modificationsâ€”transfer to support for those issues.
- Never guarantee delivery times more specific than the 4-hour windows. Don't promise "we'll be there at 2pm."
- If a customer becomes abusive, calmly offer to transfer them to a supervisor or end the call.
- Don't share internal logistics details (warehouse locations, driver routes, etc.).
- If you're unsure about a policy or capability, say so and offer to connect them with someone who can help.
```

### What This Example Demonstrates


1. **Clear routing between sections**: Each flow explicitly states where to go next ("â†’ Go to \[Schedule Delivery\]"). This prevents dead ends and makes the logic scannable.
2. **Context section for outbound calls**: The agent knows who they're calling and whyâ€”no need to ask for information it should already have.
3. **Detailed conversation flow with sub-steps**: The \[Schedule Delivery\] section breaks down into numbered steps, making the sequence clear.
4. **"What if" handling inline**: Each flow includes common edge cases ("What if no slots work?") right where they're relevant.
5. **Questions as a dedicated section**: Instead of scattering Q&A throughout, there's a \[Handle Questions\] section with routing back to other flows.
6. **Instructions for flow-level rules**: The Instructions section contains rules that apply across the conversation but don't fit in Guardrails (which are about boundaries, not process).
7. **Natural jump patterns**: Notice how flows connect:
   * Opening â†’ Confirm Delivery Details (or callback)
   * Confirm Delivery Details â†’ Schedule Delivery / Handle Questions / Cancellation Flow
   * Handle Questions â†’ back to Schedule Delivery / Closing / Cancellation Flow
   * All paths â†’ Closing


---

## Final Thought

The best voice agent prompts feel like onboarding a new employee. You're not giving them a script to memorizeâ€”you're explaining who they are, what they're trying to achieve, how they should conduct themselves, and what tools they have at their disposal. Then you trust them to handle the conversation.

Clear structure, specific guidance, and room for natural adaptation: that's the formula for a voice agent that users actually enjoy talking to.



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/README.md
================================================
# Pulse STT Developer Guide - Code Samples

Code samples from the blog post: **"Developer Guide: Integrating AI Voice & Speech APIs with Smallest AI Pulse"**

## ðŸŽ¬ Demos

### Python Transcription 
![Python Streaming](./screenshots/blog_pulse_stt_guide_python_transcription.gif)

### Node.js WebSocket Streaming
![Node.js Streaming](./screenshots/blog_pulse_stt_guide_node_js_websocket.gif)

---

## ðŸ“ Structure

```
pulse-stt-developer-guide/
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ rest/
â”‚   â”‚   â”œâ”€â”€ basic_transcription.py      # Basic file transcription
â”‚   â”‚   â”œâ”€â”€ url_transcription.py        # Transcribe from URL
â”‚   â”‚   â””â”€â”€ async_batch.py              # Async batch processing
â”‚   â””â”€â”€ websocket/
â”‚       â””â”€â”€ streaming_client.py         # Real-time streaming (file or mic)
â”œâ”€â”€ nodejs/
â”‚   â”œâ”€â”€ rest/
â”‚   â”‚   â””â”€â”€ transcription.ts            # File transcription
â”‚   â””â”€â”€ websocket/
â”‚       â”œâ”€â”€ browser_client.js           # Browser-based ASR
â”‚       â””â”€â”€ server_client.ts            # Node.js streaming client
â”œâ”€â”€ demo-app/                           # Next.js demo application
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ audio_preprocessing.py          # Audio format conversion
â”‚   â””â”€â”€ ffmpeg_commands.sh              # FFmpeg one-liners
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ package.json                        # Node.js dependencies
â””â”€â”€ env.example                         # Environment variables template
```

## ðŸš€ Quick Start

### Prerequisites

1. Get your API key from [console.smallest.ai](https://console.smallest.ai/apikeys)
2. Set environment variable:
   ```bash
   export SMALLEST_API_KEY="your_api_key_here"
   ```

### Get a Test Audio File

```bash
# Download a sample WAV file
curl -L "https://www.voiptroubleshooter.com/open_speech/american/OSR_us_000_0010_8k.wav" -o test_audio.wav
```

---

## ðŸ Python Examples

### Setup with uv (Recommended)

```bash
# Create virtual environment
uv venv python/.venv

# Activate it
source python/.venv/bin/activate

# Install dependencies
uv pip install -r requirements.txt
```

### Test REST API (Pre-Recorded)

```bash
cd python/rest

# Basic transcription
python basic_transcription.py ../../test_audio.wav

# Transcribe from URL
python url_transcription.py "https://www.voiptroubleshooter.com/open_speech/american/OSR_us_000_0010_8k.wav"
```

### Test WebSocket API (Streaming)

```bash
cd python/websocket

# Stream a file (recommended for testing)
python streaming_client.py --file ../../test_audio.wav

# With different language
python streaming_client.py --file ../../test_audio.wav --language multi
```

### Optional: Microphone Support

To use real-time microphone input, install `portaudio` first:

```bash
# macOS
brew install portaudio

# Ubuntu/Debian
sudo apt-get install portaudio19-dev

# Then install pyaudio
uv pip install pyaudio

# Run with microphone
python websocket/streaming_client.py --language en
```

---

## ðŸ“¦ Node.js Examples

### Setup

```bash
# Install dependencies
npm install
```

### Test REST API

```bash
cd nodejs/rest

# Transcribe a file
npx ts-node transcription.ts ../../test_audio.wav
```

### Test WebSocket API

```bash
cd nodejs/websocket

# Stream a file
npx ts-node server_client.ts ../../test_audio.wav

# With different language
npx ts-node server_client.ts ../../test_audio.wav --language hi
```

---

## ðŸŽ¨ Demo App (Next.js)

Interactive web app with file upload and real-time microphone transcription.

```bash
cd demo-app

# Install dependencies
npm install

# Set API key
echo 'SMALLEST_API_KEY=your_api_key_here' > .env.local

# Run both Next.js app and WebSocket proxy
npm run dev:all

# Open http://localhost:3000
```

See [demo-app/README.md](./demo-app/README.md) for detailed instructions.

---

## ðŸ“‹ API Reference

### REST API (Pre-Recorded)

**Endpoint:** `POST https://waves-api.smallest.ai/api/v1/pulse/get_text`

| Parameter | Type | Description |
|-----------|------|-------------|
| `model` | string | `pulse` |
| `language` | string | `en`, `hi`, `multi` (auto-detect) |
| `word_timestamps` | boolean | Include word-level timing |
| `diarize` | boolean | Speaker diarization |
| `emotion_detection` | boolean | Detect emotions |

### WebSocket API (Real-Time)

**Endpoint:** `wss://waves-api.smallest.ai/api/v1/pulse/get_text`

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `language` | string | `en` | Language code or `multi` |
| `encoding` | string | `linear16` | Audio encoding format |
| `sample_rate` | string | `16000` | Sample rate in Hz |
| `word_timestamps` | string | `true` | Word-level timestamps |
| `full_transcript` | string | `false` | Cumulative transcript |
| `diarize` | string | `false` | Speaker diarization |
| `redact_pii` | string | `false` | Redact personal info |
| `redact_pci` | string | `false` | Redact payment info |

---

## âœ… Test Checklist

| Test | Command | Expected |
|------|---------|----------|
| Python REST | `python python/rest/basic_transcription.py test_audio.wav` | Transcription text |
| Python WS | `python python/websocket/streaming_client.py -f test_audio.wav` | Streaming transcripts |
| Node REST | `npx ts-node nodejs/rest/transcription.ts test_audio.wav` | Transcription text |
| Node WS | `npx ts-node nodejs/websocket/server_client.ts test_audio.wav` | Streaming transcripts |
| Demo App | `cd demo-app && npm run dev:all` | Web UI at localhost:3000 |

---

## ðŸ”— Resources

- [Waves Documentation](https://waves-docs.smallest.ai)
- [Smallest AI Console](https://console.smallest.ai)
- [Python SDK](https://github.com/smallest-inc/smallest-python-sdk)
- [Discord Community](https://discord.gg/5evETqguJs)

## ðŸ“ License

MIT License - See [LICENSE](../../LICENSE) for details.



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/env.example
================================================
# Smallest AI API Configuration
# Copy this file to .env and fill in your API key

# Get your API key from: https://console.smallest.ai/apikeys
SMALLEST_API_KEY=your_api_key_here

# Optional: Server port for demo app (default: 3000)
PORT=3000



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/package.json
================================================
{
  "name": "pulse-stt-developer-guide",
  "version": "1.0.0",
  "description": "Code samples for Pulse STT Developer Guide blog post",
  "main": "demo-app/server.ts",
  "scripts": {
    "start": "cd demo-app && npx ts-node server.ts",
    "rest:transcribe": "npx ts-node nodejs/rest/transcription.ts",
    "ws:stream": "npx ts-node nodejs/websocket/server_client.ts"
  },
  "dependencies": {
    "express": "^4.18.2",
    "multer": "^1.4.5-lts.1",
    "ws": "^8.14.0"
  },
  "devDependencies": {
    "@types/express": "^4.17.21",
    "@types/multer": "^1.4.11",
    "@types/node": "^20.10.0",
    "@types/ws": "^8.5.10",
    "ts-node": "^10.9.2",
    "typescript": "^5.3.0"
  },
  "keywords": [
    "speech-to-text",
    "stt",
    "asr",
    "transcription",
    "smallest-ai",
    "pulse"
  ],
  "author": "Smallest AI",
  "license": "MIT"
}



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/requirements.txt
================================================
# Python dependencies for Pulse STT code samples
# Install with: uv pip install -r requirements.txt

# REST API
requests>=2.28.0

# WebSocket streaming
websockets>=11.0

# Audio processing (for file-based streaming)
librosa>=0.10.0
numpy>=1.24.0

# Optional: For microphone input (requires portaudio system library)
# On macOS: brew install portaudio
# On Ubuntu: sudo apt-get install portaudio19-dev
# pyaudio>=0.2.13



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/README.md
================================================
# Pulse STT Demo App

A Next.js demo application showcasing Smallest AI's Pulse Speech-to-Text API with both file upload and real-time microphone transcription.

## Demo

![Demo App](./screenshots/blog_pulse_stt_guide_next_js_demo_app.gif)

## Features

- ðŸ“ **File Upload Transcription** - Upload audio files and get transcription with:
  - Word-level timestamps
  - Speaker diarization
  - Emotion detection
  - Age/gender estimation

- ðŸŽ¤ **Real-time Microphone Transcription** - Live streaming with:
  - Instant transcription as you speak
  - Interim (partial) and final results
  - Secure WebSocket proxy (API key stays server-side)

## Quick Start

### 1. Install Dependencies

```bash
npm install
```

### 2. Set Environment Variable

Create a `.env.local` file:

```bash
echo 'SMALLEST_API_KEY=your_api_key_here' > .env.local
```

### 3. Run Both Servers

```bash
npm run dev:all
```

This starts:
- **Next.js** on `http://localhost:3000` (UI)
- **WebSocket Proxy** on `ws://localhost:3001` (real-time STT)

### 4. Open in Browser

Go to **http://localhost:3000** in Chrome or Safari (for microphone access).

## Running Servers Separately

If you prefer to run them in separate terminals:

```bash
# Terminal 1: Next.js
npm run dev

# Terminal 2: WebSocket Proxy
npm run dev:ws
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Browser     â”‚â”€â”€â”€â”€â–¶â”‚   Next.js API   â”‚â”€â”€â”€â”€â–¶â”‚    Pulse STT    â”‚
â”‚   (localhost)   â”‚     â”‚  (File Upload)  â”‚     â”‚   (REST API)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ WebSocket
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WS Proxy :3001 â”‚â”€â”€â”€â”€â–¶â”‚    Pulse STT    â”‚
â”‚   (Audio Stream)â”‚     â”‚  (WebSocket)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## API Routes

| Route | Method | Description |
|-------|--------|-------------|
| `/api/transcribe` | POST | Upload audio file for transcription |
| `ws://localhost:3001` | WebSocket | Real-time audio streaming proxy |

## Project Structure

```
demo-app/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â””â”€â”€ transcribe/
â”‚       â”‚       â””â”€â”€ route.ts     # REST API for file upload
â”‚       â”œâ”€â”€ page.tsx             # Main UI component
â”‚       â”œâ”€â”€ layout.tsx
â”‚       â””â”€â”€ globals.css
â”œâ”€â”€ ws-server.js                 # WebSocket proxy server
â”œâ”€â”€ .env.local                   # Your API key
â””â”€â”€ package.json
```

## Scripts

| Script | Description |
|--------|-------------|
| `npm run dev` | Start Next.js only |
| `npm run dev:ws` | Start WebSocket proxy only |
| `npm run dev:all` | Start both (recommended) |
| `npm run build` | Build for production |

## Tech Stack

- **Next.js 16** - React framework with App Router
- **Tailwind CSS** - Styling
- **TypeScript** - Type safety
- **ws** - WebSocket client/server for Node.js
- **concurrently** - Run multiple commands

## Troubleshooting

### Microphone not working?
- Open in **Chrome** or **Safari** (not embedded browsers)
- Allow microphone permission when prompted
- Check that WebSocket proxy is running (`npm run dev:ws`)

### WebSocket connection failed?
- Ensure both servers are running: `npm run dev:all`
- Check port 3001 is not blocked

### API errors?
- Verify your API key in `.env.local`
- Check console for detailed error messages

## Resources

- [Smallest AI Console](https://console.smallest.ai) - Get API key
- [Waves Documentation](https://waves-docs.smallest.ai) - Full API reference
- [GitHub](https://github.com/smallest-inc) - Official repositories



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/eslint.config.mjs
================================================
import { defineConfig, globalIgnores } from "eslint/config";
import nextVitals from "eslint-config-next/core-web-vitals";
import nextTs from "eslint-config-next/typescript";

const eslintConfig = defineConfig([
  ...nextVitals,
  ...nextTs,
  // Override default ignores of eslint-config-next.
  globalIgnores([
    // Default ignores of eslint-config-next:
    ".next/**",
    "out/**",
    "build/**",
    "next-env.d.ts",
  ]),
]);

export default eslintConfig;



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/next.config.ts
================================================
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  // Fix for monorepo/nested package structure
  // Tells turbopack to use this directory as root, not parent
  experimental: {
    turbo: {
      root: ".",
    },
  },
};

export default nextConfig;



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/package.json
================================================
{
  "name": "demo-app",
  "version": "0.1.0",
  "private": true,
  "type": "module",
  "scripts": {
    "dev": "next dev --port 3000",
    "dev:ws": "node --env-file=.env.local ws-server.js",
    "dev:all": "concurrently \"npm run dev\" \"npm run dev:ws\"",
    "build": "next build",
    "start": "next start",
    "lint": "eslint"
  },
  "dependencies": {
    "concurrently": "^9.2.1",
    "dotenv": "^17.2.3",
    "next": "16.1.6",
    "react": "19.2.3",
    "react-dom": "19.2.3",
    "ws": "^8.19.0"
  },
  "devDependencies": {
    "@tailwindcss/postcss": "^4",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "16.1.6",
    "tailwindcss": "^4",
    "typescript": "^5"
  }
}



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/postcss.config.mjs
================================================
const config = {
  plugins: {
    "@tailwindcss/postcss": {},
  },
};

export default config;



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    ".next/dev/types/**/*.ts",
    "**/*.mts"
  ],
  "exclude": ["node_modules"]
}



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/ws-proxy.js
================================================
/**
 * WebSocket Proxy Server for Pulse STT
 * 
 * This proxy keeps your API key secure on the server while allowing
 * browser clients to stream audio for real-time transcription.
 * 
 * Run: node ws-proxy.js
 */

const WebSocket = require('ws');
const http = require('http');
require('dotenv').config({ path: '.env.local' });

const PORT = 3001;
const API_KEY = process.env.SMALLEST_API_KEY;

if (!API_KEY) {
  console.error('âŒ SMALLEST_API_KEY not found in .env.local');
  process.exit(1);
}

// Create HTTP server
const server = http.createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('WebSocket Proxy Server for Pulse STT');
});

// Create WebSocket server
const wss = new WebSocket.Server({ server });

wss.on('connection', (clientWs, req) => {
  console.log('ðŸ”Œ Client connected');
  
  // Parse query params from client connection
  const url = new URL(req.url, `http://localhost:${PORT}`);
  const language = url.searchParams.get('language') || 'en';
  
  // Build Pulse STT WebSocket URL
  const params = new URLSearchParams({
    language: language,
    encoding: 'linear16',
    sample_rate: '16000',
    word_timestamps: 'true',
    full_transcript: 'true'
  });
  
  const pulseUrl = `wss://waves-api.smallest.ai/api/v1/pulse/get_text?${params}`;
  
  // Connect to Pulse STT
  const pulseWs = new WebSocket(pulseUrl, {
    headers: { 
      'Authorization': `Bearer ${API_KEY}` 
    }
  });
  
  pulseWs.on('open', () => {
    console.log('âœ… Connected to Pulse STT');
    clientWs.send(JSON.stringify({ type: 'connected', message: 'Ready for audio' }));
  });
  
  pulseWs.on('message', (data) => {
    // Forward transcription to client
    try {
      const message = JSON.parse(data.toString());
      console.log('ðŸ“ Transcript:', message.transcript?.substring(0, 50) || '(partial)');
      clientWs.send(data.toString());
    } catch (e) {
      clientWs.send(data.toString());
    }
  });
  
  pulseWs.on('error', (error) => {
    console.error('âŒ Pulse STT error:', error.message);
    clientWs.send(JSON.stringify({ 
      type: 'error', 
      message: error.message 
    }));
  });
  
  pulseWs.on('close', (code, reason) => {
    console.log(`ðŸ”Œ Pulse STT closed: ${code} - ${reason}`);
    clientWs.send(JSON.stringify({ 
      type: 'closed', 
      code, 
      reason: reason.toString() 
    }));
  });
  
  // Forward audio from client to Pulse
  clientWs.on('message', (data) => {
    if (pulseWs.readyState === WebSocket.OPEN) {
      // Check if it's binary audio data or JSON control message
      if (Buffer.isBuffer(data) || data instanceof ArrayBuffer) {
        pulseWs.send(data);
      } else {
        try {
          const msg = JSON.parse(data.toString());
          if (msg.type === 'end') {
            pulseWs.send(JSON.stringify({ type: 'end' }));
          }
        } catch (e) {
          // Binary data, forward as-is
          pulseWs.send(data);
        }
      }
    }
  });
  
  clientWs.on('close', () => {
    console.log('ðŸ”Œ Client disconnected');
    if (pulseWs.readyState === WebSocket.OPEN) {
      pulseWs.close();
    }
  });
  
  clientWs.on('error', (error) => {
    console.error('âŒ Client error:', error.message);
  });
});

server.listen(PORT, () => {
  console.log(`
ðŸŽ™ï¸  Pulse STT WebSocket Proxy
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   Proxy:    ws://localhost:${PORT}
   Status:   Ready
   
   Connect your browser to this proxy.
   Audio will be forwarded to Pulse STT.
`);
});



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/ws-server.js
================================================
/**
 * WebSocket Proxy Server for Pulse STT
 * 
 * This server acts as a secure proxy between the browser and Smallest AI's
 * WebSocket STT endpoint, keeping your API key server-side.
 */

import { WebSocketServer, WebSocket } from 'ws';
import { createServer } from 'http';
import { parse } from 'url';

const WS_PORT = 3001;
// Pulse STT WebSocket endpoint (official v4.0.0 endpoint)
const PULSE_WS_URL = 'wss://waves-api.smallest.ai/api/v1/pulse/get_text';

// Get API key from environment
const API_KEY = process.env.SMALLEST_API_KEY;

if (!API_KEY) {
  console.error('âŒ SMALLEST_API_KEY environment variable is required');
  process.exit(1);
}

// Create HTTP server for WebSocket upgrade
const server = createServer((req, res) => {
  res.writeHead(200, { 'Content-Type': 'text/plain' });
  res.end('Pulse STT WebSocket Proxy Server');
});

// Create WebSocket server
const wss = new WebSocketServer({ server });

wss.on('connection', (clientWs, req) => {
  console.log('ðŸ”Œ Client connected');
  
  // Parse query parameters from client
  const queryParams = parse(req.url, true).query;
  const language = queryParams.language || 'en';
  const sampleRate = queryParams.sample_rate || '16000';
  
  // Build Lightning STT WebSocket URL with parameters
  const pulseParams = new URLSearchParams({
    language: language,
    sample_rate: sampleRate,
    encoding: 'linear16',
    word_timestamps: 'true',
    full_transcript: 'true',
    numerals: 'auto'
  });
  
  const pulseUrl = `${PULSE_WS_URL}?${pulseParams}`;
  console.log(`ðŸ“¡ Connecting to Pulse STT: ${pulseUrl}`);
  
  // Connect to Pulse STT WebSocket
  const pulseWs = new WebSocket(pulseUrl, {
    headers: {
      'Authorization': `Bearer ${API_KEY}`
    }
  });
  
  pulseWs.on('open', () => {
    console.log('âœ… Connected to Pulse STT');
    clientWs.send(JSON.stringify({ type: 'connected', message: 'Ready to transcribe' }));
  });
  
  pulseWs.on('message', (data) => {
    // Forward transcription results to client
    const msgStr = data.toString();
    console.log('ðŸ“© Received from Pulse STT:', msgStr.substring(0, 200));
    try {
      const message = JSON.parse(msgStr);
      clientWs.send(JSON.stringify(message));
    } catch (e) {
      clientWs.send(msgStr);
    }
  });
  
  pulseWs.on('error', (error) => {
    console.error('âŒ Pulse STT error:', error.message);
    clientWs.send(JSON.stringify({ 
      type: 'error', 
      message: `STT connection error: ${error.message}` 
    }));
  });
  
  pulseWs.on('close', (code, reason) => {
    console.log(`ðŸ”Œ Pulse STT disconnected: ${code} - ${reason}`);
    clientWs.send(JSON.stringify({ type: 'disconnected' }));
  });
  
  // Forward audio data from client to Pulse STT
  let audioChunkCount = 0;
  clientWs.on('message', (data) => {
    if (pulseWs.readyState === WebSocket.OPEN) {
      // Check if it's a control message (JSON) or audio data (binary)
      if (Buffer.isBuffer(data) || data instanceof ArrayBuffer) {
        audioChunkCount++;
        if (audioChunkCount % 50 === 1) {
          console.log(`ðŸŽµ Sending audio chunk #${audioChunkCount}, size: ${data.length} bytes`);
        }
        pulseWs.send(data);
      } else {
        try {
          const message = JSON.parse(data.toString());
          console.log('ðŸ“¤ Control message:', message);
          if (message.type === 'end') {
            // Signal end of audio stream
            pulseWs.send(JSON.stringify({ type: 'end' }));
          }
        } catch (e) {
          // Not JSON, treat as audio data
          pulseWs.send(data);
        }
      }
    }
  });
  
  clientWs.on('close', () => {
    console.log('ðŸ”Œ Client disconnected');
    if (pulseWs.readyState === WebSocket.OPEN) {
      pulseWs.close();
    }
  });
  
  clientWs.on('error', (error) => {
    console.error('âŒ Client WebSocket error:', error.message);
  });
});

server.listen(WS_PORT, () => {
  console.log(`\nðŸŽ™ï¸  Pulse STT WebSocket Proxy`);
  console.log(`   Running on ws://localhost:${WS_PORT}`);
  console.log(`   API Key: ${API_KEY.slice(0, 8)}...${API_KEY.slice(-4)}\n`);
});



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/src/app/globals.css
================================================
@import "tailwindcss";

:root {
  --background: #ffffff;
  --foreground: #171717;
}

@theme inline {
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --font-sans: var(--font-geist-sans);
  --font-mono: var(--font-geist-mono);
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: Arial, Helvetica, sans-serif;
}



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/src/app/layout.tsx
================================================
import type { Metadata } from "next";
import { Inter } from "next/font/google";
import "./globals.css";

const inter = Inter({ subsets: ["latin"] });

export const metadata: Metadata = {
  title: "Pulse STT Demo | Smallest AI",
  description: "Speech-to-Text demo powered by Smallest AI Pulse",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body className={inter.className}>{children}</body>
    </html>
  );
}



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/src/app/page.tsx
================================================
'use client';

import { useState, useRef, useCallback } from 'react';

interface Word {
  word: string;
  start: number;
  end: number;
  speaker?: number;
  confidence?: number;
}

interface TranscriptionResult {
  status: string;
  transcription: string;
  words?: Word[];
  emotions?: Record<string, number>;
  age?: string;
  gender?: string;
  metadata?: {
    duration: number;
    fileSize: number;
  };
}

export default function Home() {
  const [activeTab, setActiveTab] = useState<'file' | 'realtime'>('file');
  
  // File upload state
  const [file, setFile] = useState<File | null>(null);
  const [language, setLanguage] = useState('en');
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [result, setResult] = useState<TranscriptionResult | null>(null);
  const [error, setError] = useState<string | null>(null);
  
  // Real-time state
  const [isRecording, setIsRecording] = useState(false);
  const [realtimeTranscript, setRealtimeTranscript] = useState<string[]>([]);
  const [interimTranscript, setInterimTranscript] = useState('');
  const wsRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  const handleFileChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    if (e.target.files && e.target.files[0]) {
      setFile(e.target.files[0]);
      setResult(null);
      setError(null);
    }
  };

  const handleTranscribe = async () => {
    if (!file) return;

    setIsTranscribing(true);
    setError(null);
    setResult(null);

    try {
      const formData = new FormData();
      formData.append('audio', file);
      formData.append('language', language);

      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData,
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || 'Transcription failed');
      }

      setResult(data);
    } catch (err) {
      setError(err instanceof Error ? err.message : 'An error occurred');
    } finally {
      setIsTranscribing(false);
    }
  };

  const startRecording = useCallback(async () => {
    try {
      setRealtimeTranscript([]);
      setInterimTranscript('');
      setError(null);

      // Get microphone access
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 16000,
          echoCancellation: true,
          noiseSuppression: true,
        },
      });
      streamRef.current = stream;

      // Connect to WebSocket proxy server
      const wsUrl = `ws://localhost:3001?language=${language}&sample_rate=16000`;
      const ws = new WebSocket(wsUrl);
      wsRef.current = ws;

      ws.onopen = () => {
        console.log('âœ… Connected to WebSocket proxy');
        setIsRecording(true);
        
        // Set up audio processing - use browser's native sample rate
        audioContextRef.current = new AudioContext();
        const actualSampleRate = audioContextRef.current.sampleRate;
        console.log(`ðŸŽ¤ Audio sample rate: ${actualSampleRate}Hz`);
        
        const source = audioContextRef.current.createMediaStreamSource(stream);
        processorRef.current = audioContextRef.current.createScriptProcessor(4096, 1, 1);

        processorRef.current.onaudioprocess = (e) => {
          if (ws.readyState === WebSocket.OPEN) {
            const input = e.inputBuffer.getChannelData(0);
            
            // Resample to 16kHz using linear interpolation
            let resampled: Float32Array;
            if (actualSampleRate !== 16000) {
              const ratio = actualSampleRate / 16000;
              const newLength = Math.round(input.length / ratio);
              resampled = new Float32Array(newLength);
              for (let i = 0; i < newLength; i++) {
                const srcIndex = i * ratio;
                const srcIndexFloor = Math.floor(srcIndex);
                const srcIndexCeil = Math.min(srcIndexFloor + 1, input.length - 1);
                const fraction = srcIndex - srcIndexFloor;
                resampled[i] = input[srcIndexFloor] * (1 - fraction) + input[srcIndexCeil] * fraction;
              }
            } else {
              resampled = input;
            }
            
            // Convert Float32 to Int16 PCM
            const outputData = new Int16Array(resampled.length);
            for (let i = 0; i < resampled.length; i++) {
              const s = Math.max(-1, Math.min(1, resampled[i]));
              outputData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
            }
            
            ws.send(outputData.buffer);
          }
        };

        source.connect(processorRef.current);
        processorRef.current.connect(audioContextRef.current.destination);
      };

      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          
          // Handle connection status messages
          if (data.type === 'connected') {
            console.log('ðŸŽ™ï¸ Ready to transcribe');
            return;
          }
          if (data.type === 'error') {
            setError(data.message);
            return;
          }
          if (data.type === 'disconnected') {
            console.log('ðŸ”Œ Disconnected from STT');
            return;
          }
          
          // Handle transcription results
          if (data.transcript) {
            if (data.is_final) {
              setRealtimeTranscript(prev => [...prev, data.transcript]);
              setInterimTranscript('');
            } else {
              setInterimTranscript(data.transcript);
            }
          }
        } catch (e) {
          console.error('Parse error:', e);
        }
      };

      ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        setError('WebSocket connection failed. Make sure the proxy server is running: node ws-server.js');
        stopRecording();
      };
      
      ws.onclose = () => {
        console.log('ðŸ”Œ WebSocket closed');
        setIsRecording(false);
      };
    } catch (err) {
      const message = err instanceof Error ? err.message : 'Unknown error';
      if (message.includes('Permission') || message.includes('denied') || message.includes('not allowed')) {
        setError('Microphone access denied. Please open this page in Chrome/Safari and allow microphone permission.');
      } else {
        setError(message);
      }
    }
  }, [language]);

  const stopRecording = useCallback(() => {
    setIsRecording(false);

    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }

    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
  }, []);

  const languages = [
    { code: 'en', name: 'English' },
    { code: 'hi', name: 'Hindi' },
    { code: 'es', name: 'Spanish' },
    { code: 'fr', name: 'French' },
    { code: 'de', name: 'German' },
    { code: 'pt', name: 'Portuguese' },
    { code: 'multi', name: 'Auto-detect' },
  ];

  return (
    <main className="min-h-screen bg-[#092023]">
      <div className="container mx-auto px-4 py-12 max-w-4xl">
        {/* Header */}
        <div className="text-center mb-12">
          <h1 className="text-4xl md:text-5xl font-bold text-[#FBFAF5] mb-4">
            ðŸŽ™ï¸ Pulse STT Demo
          </h1>
          <p className="text-[#43B6B6] text-lg">
            Speech-to-Text powered by Smallest AI
          </p>
        </div>

        {/* Tab Navigation */}
        <div className="flex justify-center mb-8">
          <div className="bg-[#1D4E52] rounded-xl p-1">
            <button
              onClick={() => setActiveTab('file')}
              className={`px-6 py-3 rounded-lg font-medium transition-all ${
                activeTab === 'file'
                  ? 'bg-[#43B6B6] text-[#092023] shadow-lg'
                  : 'text-[#FBFAF5]/70 hover:text-[#FBFAF5]'
              }`}
            >
              ðŸ“ File Upload
            </button>
            <button
              onClick={() => setActiveTab('realtime')}
              className={`px-6 py-3 rounded-lg font-medium transition-all ${
                activeTab === 'realtime'
                  ? 'bg-[#43B6B6] text-[#092023] shadow-lg'
                  : 'text-[#FBFAF5]/70 hover:text-[#FBFAF5]'
              }`}
            >
              ðŸŽ¤ Real-time
            </button>
          </div>
        </div>

        {/* Content Card */}
        <div className="bg-[#1D4E52] rounded-2xl p-8 shadow-2xl border border-[#43B6B6]/20">
          {activeTab === 'file' ? (
            /* File Upload Tab */
            <div className="space-y-6">
              {/* File Input */}
              <div>
                <label className="block text-[#43B6B6] text-sm font-medium mb-2">
                  Audio File
                </label>
                <div className="relative">
                  <input
                    type="file"
                    accept="audio/*"
                    onChange={handleFileChange}
                    className="block w-full text-sm text-[#FBFAF5]/70
                      file:mr-4 file:py-3 file:px-6
                      file:rounded-lg file:border-0
                      file:text-sm file:font-medium
                      file:bg-[#43B6B6] file:text-[#092023]
                      hover:file:bg-[#3aa3a3]
                      file:cursor-pointer cursor-pointer
                      bg-[#092023] rounded-lg border border-[#43B6B6]/30"
                  />
                </div>
                {file && (
                  <p className="mt-2 text-sm text-[#43B6B6]">
                    Selected: {file.name} ({(file.size / 1024).toFixed(1)} KB)
                  </p>
                )}
              </div>

              {/* Language Select */}
              <div>
                <label className="block text-[#43B6B6] text-sm font-medium mb-2">
                  Language
                </label>
                <select
                  value={language}
                  onChange={(e) => setLanguage(e.target.value)}
                  className="w-full bg-[#092023] border border-[#43B6B6]/30 text-[#FBFAF5] rounded-lg px-4 py-3 focus:ring-2 focus:ring-[#43B6B6] focus:border-transparent"
                >
                  {languages.map((lang) => (
                    <option key={lang.code} value={lang.code}>
                      {lang.name}
                    </option>
                  ))}
                </select>
              </div>

              {/* Transcribe Button */}
              <button
                onClick={handleTranscribe}
                disabled={!file || isTranscribing}
                className="w-full bg-[#FFCF72] text-[#092023] font-semibold py-4 px-6 rounded-xl
                  hover:bg-[#ffc94d] disabled:opacity-50 disabled:cursor-not-allowed
                  transition-all transform hover:scale-[1.02] active:scale-[0.98]"
              >
                {isTranscribing ? (
                  <span className="flex items-center justify-center">
                    <svg className="animate-spin -ml-1 mr-3 h-5 w-5 text-[#092023]" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                      <circle className="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" strokeWidth="4"></circle>
                      <path className="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                    </svg>
                    Transcribing...
                  </span>
                ) : (
                  'ðŸ“ Transcribe'
                )}
              </button>

              {/* Error Display */}
              {error && (
                <div className="bg-[#FF5E5E]/20 border border-[#FF5E5E] text-[#FF5E5E] rounded-lg p-4">
                  {error}
                </div>
              )}

              {/* Results */}
              {result && (
                <div className="space-y-6 mt-8">
                  {/* Transcription */}
                  <div>
                    <h3 className="text-lg font-medium text-[#FBFAF5] mb-3">ðŸ“ Transcription</h3>
                    <div className="bg-[#092023] rounded-lg p-4 text-[#FBFAF5]/90 border border-[#43B6B6]/20">
                      {result.transcription}
                    </div>
                  </div>

                  {/* Word Timestamps */}
                  {result.words && result.words.length > 0 && (
                    <div>
                      <h3 className="text-lg font-medium text-[#FBFAF5] mb-3">â±ï¸ Word Timestamps</h3>
                      <div className="bg-[#092023] rounded-lg p-4 flex flex-wrap gap-2 border border-[#43B6B6]/20">
                        {result.words.map((word, i) => (
                          <span
                            key={i}
                            className="inline-block bg-[#43B6B6]/20 text-[#43B6B6] px-2 py-1 rounded text-sm cursor-help hover:bg-[#43B6B6]/30 transition-colors"
                            title={`${word.start.toFixed(2)}s - ${word.end.toFixed(2)}s${word.speaker !== undefined ? ` | Speaker ${word.speaker}` : ''}`}
                          >
                            {word.word}
                          </span>
                        ))}
                      </div>
                    </div>
                  )}

                  {/* Emotions */}
                  {result.emotions && Object.keys(result.emotions).length > 0 && (
                    <div>
                      <h3 className="text-lg font-medium text-[#FBFAF5] mb-3">ðŸ˜Š Emotions</h3>
                      <div className="grid grid-cols-2 md:grid-cols-5 gap-3">
                        {Object.entries(result.emotions).map(([emotion, score]) => (
                          <div key={emotion} className="bg-[#092023] rounded-lg p-3 text-center border border-[#43B6B6]/20">
                            <div className="text-2xl mb-1">
                              {emotion === 'happiness' && 'ðŸ˜Š'}
                              {emotion === 'sadness' && 'ðŸ˜¢'}
                              {emotion === 'anger' && 'ðŸ˜ '}
                              {emotion === 'fear' && 'ðŸ˜¨'}
                              {emotion === 'disgust' && 'ðŸ¤¢'}
                            </div>
                            <div className="text-xs text-[#FBFAF5]/50 capitalize">{emotion}</div>
                            <div className="text-sm font-medium text-[#FFCF72]">{(score * 100).toFixed(0)}%</div>
                          </div>
                        ))}
                      </div>
                    </div>
                  )}

                  {/* Metadata */}
                  <div className="flex flex-wrap gap-4 text-sm">
                    {result.age && (
                      <span className="bg-[#092023] px-3 py-1 rounded-full text-[#43B6B6] border border-[#43B6B6]/30">
                        Age: {result.age}
                      </span>
                    )}
                    {result.gender && (
                      <span className="bg-[#092023] px-3 py-1 rounded-full text-[#43B6B6] border border-[#43B6B6]/30">
                        Gender: {result.gender}
                      </span>
                    )}
                    {result.metadata?.duration && (
                      <span className="bg-[#092023] px-3 py-1 rounded-full text-[#43B6B6] border border-[#43B6B6]/30">
                        Duration: {result.metadata.duration.toFixed(2)}s
                      </span>
                    )}
                  </div>
                </div>
              )}
            </div>
          ) : (
            /* Real-time Tab */
            <div className="space-y-6">
              {/* Language Select */}
              <div>
                <label className="block text-[#43B6B6] text-sm font-medium mb-2">
                  Language
                </label>
                <select
                  value={language}
                  onChange={(e) => setLanguage(e.target.value)}
                  disabled={isRecording}
                  className="w-full bg-[#092023] border border-[#43B6B6]/30 text-[#FBFAF5] rounded-lg px-4 py-3 focus:ring-2 focus:ring-[#43B6B6] focus:border-transparent disabled:opacity-50"
                >
                  {languages.map((lang) => (
                    <option key={lang.code} value={lang.code}>
                      {lang.name}
                    </option>
                  ))}
                </select>
              </div>

              {/* Record Button */}
              <button
                onClick={isRecording ? stopRecording : startRecording}
                className={`w-full font-semibold py-4 px-6 rounded-xl transition-all transform hover:scale-[1.02] active:scale-[0.98] ${
                  isRecording
                    ? 'bg-[#FF5E5E] hover:bg-[#e85555] text-white'
                    : 'bg-[#FFCF72] hover:bg-[#ffc94d] text-[#092023]'
                }`}
              >
                {isRecording ? (
                  <span className="flex items-center justify-center">
                    <span className="w-3 h-3 bg-white rounded-full mr-3 animate-pulse"></span>
                    Stop Recording
                  </span>
                ) : (
                  'ðŸŽ¤ Start Recording'
                )}
              </button>

              {/* Error Display */}
              {error && (
                <div className="bg-[#FFCF72]/20 border border-[#FFCF72] text-[#FFCF72] rounded-lg p-4">
                  <strong>Note:</strong> {error}
                </div>
              )}

              {/* Real-time Transcript */}
              <div>
                <h3 className="text-lg font-medium text-[#FBFAF5] mb-3">ðŸ“ Live Transcript</h3>
                <div className="bg-[#092023] rounded-lg p-4 min-h-[200px] text-[#FBFAF5]/90 border border-[#43B6B6]/20">
                  {realtimeTranscript.map((text, i) => (
                    <p key={i} className="mb-2">{text}</p>
                  ))}
                  {interimTranscript && (
                    <p className="text-[#43B6B6] italic">{interimTranscript}...</p>
                  )}
                  {!isRecording && realtimeTranscript.length === 0 && !interimTranscript && (
                    <p className="text-[#FBFAF5]/40 italic">Click &quot;Start Recording&quot; to begin...</p>
                  )}
                </div>
              </div>
            </div>
          )}
        </div>

        {/* Footer */}
        <div className="text-center mt-8 text-[#FBFAF5]/60 text-sm">
          <p>
            Powered by{' '}
            <a href="https://smallest.ai" target="_blank" rel="noopener noreferrer" className="text-[#43B6B6] hover:text-[#FFCF72] transition-colors">
              Smallest AI
            </a>
            {' '}â€¢{' '}
            <a href="https://waves-docs.smallest.ai" target="_blank" rel="noopener noreferrer" className="text-[#43B6B6] hover:text-[#FFCF72] transition-colors">
              Documentation
            </a>
          </p>
        </div>
      </div>
    </main>
  );
}



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/demo-app/src/app/api/transcribe/route.ts
================================================
import { NextRequest, NextResponse } from 'next/server';

export async function POST(request: NextRequest) {
  const apiKey = process.env.SMALLEST_API_KEY;
  
  if (!apiKey) {
    return NextResponse.json(
      { error: 'SMALLEST_API_KEY not configured' },
      { status: 500 }
    );
  }

  try {
    const formData = await request.formData();
    const file = formData.get('audio') as File;
    const language = (formData.get('language') as string) || 'en';

    if (!file) {
      return NextResponse.json(
        { error: 'No audio file provided' },
        { status: 400 }
      );
    }

    // Convert file to buffer
    const buffer = Buffer.from(await file.arrayBuffer());

    // Call Pulse STT API
    const params = new URLSearchParams({
      model: 'pulse',
      language: language,
      word_timestamps: 'true',
      diarize: 'true',
      emotion_detection: 'true'
    });

    const response = await fetch(
      `https://waves-api.smallest.ai/api/v1/pulse/get_text?${params}`,
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': file.type || 'audio/wav'
        },
        body: buffer
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`API Error: ${response.status} - ${errorText}`);
      return NextResponse.json(
        { error: `Transcription failed: ${response.status}` },
        { status: response.status }
      );
    }

    const result = await response.json();
    return NextResponse.json(result);

  } catch (error) {
    console.error('Transcription error:', error);
    return NextResponse.json(
      { error: 'Internal server error' },
      { status: 500 }
    );
  }
}



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/nodejs/rest/transcription.ts
================================================
/**
 * Audio Transcription with Pulse STT REST API (TypeScript)
 *
 * Usage:
 *   npx ts-node transcription.ts path/to/audio.wav
 *   npx ts-node transcription.ts audio.mp3 --language es
 *
 * Requirements:
 *   npm install typescript ts-node @types/node
 */

import fs from 'fs';
import path from 'path';

interface TranscriptionWord {
  start: number;
  end: number;
  word: string;
}

interface Utterance {
  start: number;
  end: number;
  text: string;
}

interface TranscriptionResponse {
  status: string;
  transcription: string;
  words?: TranscriptionWord[];
  utterances?: Utterance[];
  metadata?: {
    duration: number;
    fileSize: number;
  };
}

async function transcribeAudioFile(
  filePath: string,
  language: string = 'en'
): Promise<TranscriptionResponse> {
  const apiKey = process.env.SMALLEST_API_KEY;
  if (!apiKey) {
    throw new Error('SMALLEST_API_KEY environment variable not set');
  }

  const audioBuffer = fs.readFileSync(filePath);
  const ext = path.extname(filePath).toLowerCase();

  const contentTypes: Record<string, string> = {
    '.wav': 'audio/wav',
    '.mp3': 'audio/mpeg',
    '.flac': 'audio/flac',
    '.m4a': 'audio/mp4',
    '.webm': 'audio/webm'
  };

  const params = new URLSearchParams({
    model: 'pulse',
    language: language,
    word_timestamps: 'true'
  });

  console.log(`ðŸ“ File: ${filePath}`);
  console.log(`ðŸŒ Language: ${language}`);
  console.log();

  const response = await fetch(
    `https://waves-api.smallest.ai/api/v1/pulse/get_text?${params}`,
    {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': contentTypes[ext] || 'audio/wav'
      },
      body: audioBuffer
    }
  );

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`HTTP ${response.status}: ${errorText}`);
  }

  return await response.json() as TranscriptionResponse;
}

async function transcribeWithRetry(
  filePath: string,
  language: string = 'en',
  maxRetries: number = 3
): Promise<TranscriptionResponse> {
  let lastError: Error | null = null;

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await transcribeAudioFile(filePath, language);
    } catch (error) {
      lastError = error as Error;

      // Don't retry auth errors
      if (error instanceof Error && error.message.includes('401')) {
        throw error;
      }

      // Exponential backoff for rate limits
      if (error instanceof Error && error.message.includes('429')) {
        const delay = 1000 * Math.pow(2, attempt);
        console.log(`â³ Rate limited, waiting ${delay}ms...`);
        await new Promise(r => setTimeout(r, delay));
        continue;
      }

      if (attempt < maxRetries) {
        console.log(`âš ï¸  Attempt ${attempt} failed, retrying...`);
        await new Promise(r => setTimeout(r, 500 * attempt));
      }
    }
  }

  throw lastError || new Error('Transcription failed after retries');
}

async function main() {
  const args = process.argv.slice(2);

  if (args.length === 0) {
    console.log('Usage: npx ts-node transcription.ts <audio_file> [--language <code>]');
    process.exit(1);
  }

  const filePath = args[0];
  let language = 'en';

  const langIndex = args.indexOf('--language');
  if (langIndex !== -1 && args[langIndex + 1]) {
    language = args[langIndex + 1];
  }

  if (!fs.existsSync(filePath)) {
    console.error(`âŒ File not found: ${filePath}`);
    process.exit(1);
  }

  try {
    const result = await transcribeWithRetry(filePath, language);

    console.log('='.repeat(60));
    console.log('ðŸ“ TRANSCRIPTION');
    console.log('='.repeat(60));
    console.log(result.transcription || '(no transcription)');
    console.log();

    if (result.words && result.words.length > 0) {
      console.log('='.repeat(60));
      console.log('â±ï¸  WORD TIMESTAMPS');
      console.log('='.repeat(60));
      for (const word of result.words) {
        console.log(`  [${word.start.toFixed(2)}s - ${word.end.toFixed(2)}s] ${word.word}`);
      }
    }

    if (result.metadata) {
      console.log();
      console.log(`ðŸ“Š Duration: ${result.metadata.duration}s`);
      console.log(`ðŸ“Š File size: ${result.metadata.fileSize} bytes`);
    }

  } catch (error) {
    console.error(`âŒ Error: ${error}`);
    process.exit(1);
  }
}

main();



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/nodejs/websocket/browser_client.js
================================================
/**
 * Browser-based Real-time ASR with Microphone
 *
 * This script captures microphone audio and streams it to Pulse STT.
 * Include this in your HTML page.
 *
 * Note: In browsers, WebSocket doesn't support custom headers.
 * Use a backend proxy (see demo-app/server.ts) for secure API key handling.
 *
 * Usage in HTML:
 *   <script src="browser_client.js"></script>
 *   <button onclick="startASR()">Start</button>
 *   <button onclick="stopASR()">Stop</button>
 *   <div id="transcription"></div>
 *   <div id="interim"></div>
 *   <div id="status"></div>
 */

let ws;
let audioContext;
let processor;
let source;
let stream;

/**
 * Start real-time transcription
 * @param {string} wsUrl - WebSocket URL (use your backend proxy URL)
 */
async function startASR(wsUrl = 'ws://localhost:3000/ws/transcribe?language=en') {
  ws = new WebSocket(wsUrl);

  ws.onopen = async () => {
    console.log('âœ… Connected to ASR service');
    updateStatus('Connected');
    await setupMicrophone();
  };

  ws.onmessage = (event) => {
    try {
      const response = JSON.parse(event.data);
      handleTranscription(response);
    } catch (err) {
      console.error('âŒ Parse error:', err);
    }
  };

  ws.onerror = (error) => {
    console.error('âŒ WebSocket error:', error);
    updateStatus('Error');
  };

  ws.onclose = (event) => {
    console.log(`ðŸ”Œ Connection closed: ${event.code} - ${event.reason}`);
    stopASR();
  };
}

async function setupMicrophone() {
  try {
    // Request microphone access
    stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        channelCount: 1,
        sampleRate: 16000,
        echoCancellation: true,
        noiseSuppression: true
      }
    });

    // Create audio processing pipeline
    audioContext = new AudioContext({ sampleRate: 16000 });
    source = audioContext.createMediaStreamSource(stream);

    // ScriptProcessorNode for capturing raw PCM
    processor = audioContext.createScriptProcessor(4096, 1, 1);

    processor.onaudioprocess = (event) => {
      if (ws && ws.readyState === WebSocket.OPEN) {
        const inputData = event.inputBuffer.getChannelData(0);

        // Convert Float32 to Int16 PCM
        const pcmData = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          const s = Math.max(-1, Math.min(1, inputData[i]));
          pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }

        ws.send(pcmData.buffer);
      }
    };

    source.connect(processor);
    processor.connect(audioContext.destination);

    updateStatus('ðŸŽ¤ Listening...');

  } catch (err) {
    console.error('âŒ Microphone error:', err);
    alert('Microphone access required for ASR functionality');
  }
}

function handleTranscription(response) {
  if (response.status === 'error') {
    console.error('âŒ API Error:', response.message);
    return;
  }

  if (response.transcript) {
    const isFinal = response.is_final;

    if (isFinal) {
      // Final transcription - append to results
      appendFinalTranscript(response.transcript);
    } else {
      // Partial - update interim display
      updateInterimTranscript(response.transcript);
    }
  }
}

function appendFinalTranscript(text) {
  const container = document.getElementById('transcription');
  if (container) {
    const div = document.createElement('div');
    div.className = 'final-transcript';
    div.textContent = text;
    container.appendChild(div);
  }

  // Clear interim
  const interim = document.getElementById('interim');
  if (interim) {
    interim.textContent = '';
  }
}

function updateInterimTranscript(text) {
  const interim = document.getElementById('interim');
  if (interim) {
    interim.textContent = text;
  }
}

function updateStatus(message) {
  const status = document.getElementById('status');
  if (status) {
    status.textContent = message;
  }
  console.log('Status:', message);
}

function stopASR() {
  if (processor) {
    processor.disconnect();
    processor = null;
  }
  if (source) {
    source.disconnect();
    source = null;
  }
  if (audioContext) {
    audioContext.close();
    audioContext = null;
  }
  if (stream) {
    stream.getTracks().forEach(track => track.stop());
    stream = null;
  }
  if (ws) {
    ws.close();
    ws = null;
  }

  updateStatus('â¹ï¸ Stopped');
}

// Export for module usage
if (typeof module !== 'undefined' && module.exports) {
  module.exports = { startASR, stopASR };
}



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/nodejs/websocket/server_client.ts
================================================
/**
 * Real-time STT Streaming Client (Node.js/TypeScript)
 *
 * Streams a WAV file to Pulse STT WebSocket API and displays transcripts.
 *
 * Usage:
 *   npx ts-node server_client.ts path/to/audio.wav
 *   npx ts-node server_client.ts audio.wav --language hi
 *
 * Requirements:
 *   npm install ws typescript ts-node @types/ws @types/node
 *
 * Note: WebSocket API requires Enterprise subscription.
 */

import WebSocket from 'ws';
import fs from 'fs';

interface ASRResponse {
  session_id?: string;
  transcript?: string;
  full_transcript?: string;
  is_final?: boolean;
  is_last?: boolean;
  language?: string;
  status?: string;
  message?: string;
}

class PulseStreamingClient {
  private ws: WebSocket | null = null;
  private apiKey: string;
  private language: string;
  private sampleRate: number;

  constructor(
    apiKey: string,
    language: string = 'en',
    sampleRate: number = 16000
  ) {
    this.apiKey = apiKey;
    this.language = language;
    this.sampleRate = sampleRate;
  }

  private buildUrl(): string {
    const params = new URLSearchParams({
      language: this.language,
      encoding: 'linear16',
      sample_rate: this.sampleRate.toString(),
      word_timestamps: 'true',
      full_transcript: 'true'
    });

    return `wss://waves-api.smallest.ai/api/v1/pulse/get_text?${params}`;
  }

  async connect(
    onTranscript: (text: string, isFinal: boolean) => void
  ): Promise<void> {
    return new Promise((resolve, reject) => {
      this.ws = new WebSocket(this.buildUrl(), {
        headers: { Authorization: `Bearer ${this.apiKey}` }
      });

      this.ws.on('open', () => {
        console.log('âœ… Connected to Pulse STT');
        resolve();
      });

      this.ws.on('message', (data: Buffer) => {
        try {
          const response: ASRResponse = JSON.parse(data.toString());

          if (response.status === 'error') {
            const msg = response.message || 'Unknown error';
            if (!msg.toLowerCase().includes('timed out')) {
              console.error('âŒ API Error:', msg);
            }
            return;
          }

          if (response.transcript) {
            onTranscript(response.transcript, response.is_final || false);
          }
        } catch (err) {
          console.error('âŒ Parse error:', err);
        }
      });

      this.ws.on('error', (error) => {
        console.error('âŒ WebSocket error:', error.message);
        reject(error);
      });

      this.ws.on('close', (code, reason) => {
        console.log(`ðŸ”Œ Connection closed: ${code}`);
      });
    });
  }

  sendAudio(audioChunk: Buffer): void {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(audioChunk);
    }
  }

  sendEnd(): void {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify({ type: 'end' }));
    }
  }

  async streamFile(filePath: string): Promise<string[]> {
    const transcripts: string[] = [];

    await this.connect((text, isFinal) => {
      if (isFinal) {
        transcripts.push(text);
        console.log(`\nâœ… ${text}`);
      } else {
        process.stdout.write(`\râ³ ${text}...`);
      }
    });

    console.log(`ðŸ“ Streaming: ${filePath}\n`);

    // Read file (assuming WAV with 44-byte header)
    const audioBuffer = fs.readFileSync(filePath);
    const audioData = audioBuffer.slice(44); // Skip WAV header

    const chunkSize = 3200; // ~100ms of 16kHz 16-bit audio

    for (let i = 0; i < audioData.length; i += chunkSize) {
      const chunk = audioData.slice(i, i + chunkSize);
      this.sendAudio(chunk);

      // Simulate real-time streaming pace
      await new Promise(r => setTimeout(r, 50));
    }

    // Signal end of audio
    this.sendEnd();

    // Wait for final transcripts
    await new Promise(r => setTimeout(r, 3000));
    this.close();

    return transcripts;
  }

  close(): void {
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
  }
}

async function main() {
  const args = process.argv.slice(2);

  if (args.length === 0) {
    console.log('Usage: npx ts-node server_client.ts <audio.wav> [--language <code>]');
    process.exit(1);
  }

  const filePath = args[0];
  let language = 'en';

  const langIndex = args.indexOf('--language');
  if (langIndex !== -1 && args[langIndex + 1]) {
    language = args[langIndex + 1];
  }

  const apiKey = process.env.SMALLEST_API_KEY;
  if (!apiKey) {
    console.error('âŒ SMALLEST_API_KEY environment variable not set');
    process.exit(1);
  }

  if (!fs.existsSync(filePath)) {
    console.error(`âŒ File not found: ${filePath}`);
    process.exit(1);
  }

  console.log('='.repeat(60));
  console.log('ðŸŽ™ï¸  Pulse STT Real-time Streaming');
  console.log('='.repeat(60));
  console.log(`ðŸŒ Language: ${language}`);
  console.log();

  try {
    const client = new PulseStreamingClient(apiKey, language, 16000);
    const transcripts = await client.streamFile(filePath);

    console.log();
    console.log('='.repeat(60));
    console.log('ðŸ“ FULL TRANSCRIPT');
    console.log('='.repeat(60));
    console.log(transcripts.join(' ') || '(no speech detected)');

  } catch (error: any) {
    if (error.message && error.message.includes('403')) {
      console.error('âŒ HTTP 403 - WebSocket STT requires Enterprise subscription');
    } else {
      console.error(`âŒ Error: ${error.message || error}`);
    }
    process.exit(1);
  }
}

main().catch(console.error);



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/python/rest/async_batch.py
================================================
#!/usr/bin/env python3
"""
Async Batch Audio Transcription with Pulse STT

Transcribe multiple audio files concurrently with rate limiting.

Usage:
    python async_batch.py ./episodes/*.mp3
    python async_batch.py file1.wav file2.wav file3.wav --concurrent 3

Requirements:
    pip install httpx
"""

import os
import sys
import asyncio
import argparse
from pathlib import Path
from typing import Optional

import httpx


class AsyncPulseClient:
    """Async client for batch transcription with Pulse STT."""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("SMALLEST_API_KEY")
        if not self.api_key:
            raise ValueError("SMALLEST_API_KEY environment variable not set")
        self.base_url = "https://waves-api.smallest.ai/api/v1/pulse/get_text"

    async def transcribe_batch(
        self,
        file_paths: list[str],
        language: str = "en",
        max_concurrent: int = 5
    ) -> list[dict]:
        """
        Transcribe multiple files concurrently with rate limiting.
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async with httpx.AsyncClient(timeout=120.0) as client:
            tasks = [
                self._transcribe_with_semaphore(client, path, language, semaphore)
                for path in file_paths
            ]
            return await asyncio.gather(*tasks, return_exceptions=True)

    async def _transcribe_with_semaphore(
        self,
        client: httpx.AsyncClient,
        file_path: str,
        language: str,
        semaphore: asyncio.Semaphore
    ) -> dict:
        async with semaphore:
            return await self._transcribe_one(client, file_path, language)

    async def _transcribe_one(
        self,
        client: httpx.AsyncClient,
        file_path: str,
        language: str
    ) -> dict:
        ext = file_path.lower().split('.')[-1]
        content_type = {
            'wav': 'audio/wav',
            'mp3': 'audio/mpeg',
            'flac': 'audio/flac',
            'm4a': 'audio/mp4'
        }.get(ext, 'audio/wav')

        with open(file_path, "rb") as f:
            response = await client.post(
                self.base_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": content_type
                },
                params={"model": "pulse", "language": language},
                content=f.read()
            )

        response.raise_for_status()
        return {"file": file_path, "result": response.json()}


async def process_files(files: list[str], language: str, concurrent: int):
    """Process multiple audio files."""
    client = AsyncPulseClient()

    print(f"ðŸ“ Processing {len(files)} file(s)")
    print(f"ðŸ”„ Max concurrent: {concurrent}")
    print(f"ðŸŒ Language: {language}")
    print()

    results = await client.transcribe_batch(
        files,
        language=language,
        max_concurrent=concurrent
    )

    print("=" * 60)
    print("ðŸ“Š RESULTS")
    print("=" * 60)

    success = 0
    failed = 0

    for item in results:
        if isinstance(item, Exception):
            print(f"âŒ Error: {item}")
            failed += 1
        else:
            file_name = Path(item['file']).name
            transcript = item['result'].get('transcription', '')
            char_count = len(transcript)
            print(f"âœ… {file_name}: {char_count} chars")
            success += 1

    print()
    print(f"ðŸ“ˆ Success: {success}, Failed: {failed}")


def main():
    parser = argparse.ArgumentParser(description="Batch transcribe audio files")
    parser.add_argument("files", nargs="+", help="Audio files to transcribe")
    parser.add_argument("--language", "-l", default="en",
                        help="Language code (en, hi, es, multi)")
    parser.add_argument("--concurrent", "-c", type=int, default=3,
                        help="Max concurrent requests (default: 3)")

    args = parser.parse_args()

    # Validate files exist
    valid_files = []
    for f in args.files:
        if os.path.exists(f):
            valid_files.append(f)
        else:
            print(f"âš ï¸  Skipping: {f} (not found)")

    if not valid_files:
        print("âŒ No valid files to process")
        sys.exit(1)

    asyncio.run(process_files(valid_files, args.language, args.concurrent))


if __name__ == "__main__":
    main()



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/python/rest/basic_transcription.py
================================================
#!/usr/bin/env python3
"""
Basic Audio Transcription with Pulse STT REST API

Usage:
    python basic_transcription.py path/to/audio.wav
    python basic_transcription.py path/to/audio.mp3 --language es

Requirements:
    pip install requests
"""

import os
import sys
import argparse
import requests
from typing import Optional


def transcribe_audio_file(
    file_path: str,
    language: str = "en",
    word_timestamps: bool = True
) -> dict:
    """
    Transcribe a local audio file using Smallest AI Pulse STT.
    Supports WAV, MP3, FLAC, and most common audio formats.
    """
    api_key = os.getenv("SMALLEST_API_KEY")
    if not api_key:
        raise ValueError("SMALLEST_API_KEY environment variable not set")

    url = "https://waves-api.smallest.ai/api/v1/pulse/get_text"

    # Determine content type based on file extension
    ext = file_path.lower().split('.')[-1]
    content_types = {
        'wav': 'audio/wav',
        'mp3': 'audio/mpeg',
        'flac': 'audio/flac',
        'm4a': 'audio/mp4',
        'webm': 'audio/webm',
        'ogg': 'audio/ogg'
    }

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": content_types.get(ext, "audio/wav")
    }

    params = {
        "model": "pulse",
        "language": language,
        "word_timestamps": str(word_timestamps).lower()
    }

    print(f"ðŸ“ Reading: {file_path}")
    print(f"ðŸŒ Language: {language}")
    print(f"â±ï¸  Timestamps: {word_timestamps}")
    print()

    with open(file_path, "rb") as audio_file:
        response = requests.post(
            url,
            headers=headers,
            params=params,
            data=audio_file.read(),
            timeout=120  # Longer timeout for large files
        )

    if not response.ok:
        print(f"âŒ Error: HTTP {response.status_code}")
        print(response.text)
        sys.exit(1)

    return response.json()


def main():
    parser = argparse.ArgumentParser(description="Transcribe audio with Pulse STT")
    parser.add_argument("file", help="Path to audio file")
    parser.add_argument("--language", "-l", default="en",
                        help="Language code (en, hi, es, multi for auto-detect)")
    parser.add_argument("--no-timestamps", action="store_true",
                        help="Disable word timestamps")

    args = parser.parse_args()

    if not os.path.exists(args.file):
        print(f"âŒ File not found: {args.file}")
        sys.exit(1)

    result = transcribe_audio_file(
        args.file,
        language=args.language,
        word_timestamps=not args.no_timestamps
    )

    print("=" * 60)
    print("ðŸ“ TRANSCRIPTION")
    print("=" * 60)
    print(result.get('transcription', '(no transcription)'))
    print()

    # Print word timestamps if available
    words = result.get('words', [])
    if words:
        print("=" * 60)
        print("â±ï¸  WORD TIMESTAMPS")
        print("=" * 60)
        for word in words:
            print(f"  [{word['start']:6.2f}s - {word['end']:6.2f}s] {word['word']}")

    # Print metadata if available
    metadata = result.get('metadata', {})
    if metadata:
        print()
        print(f"ðŸ“Š Duration: {metadata.get('duration', 'N/A')}s")
        print(f"ðŸ“Š File size: {metadata.get('fileSize', 'N/A')} bytes")


if __name__ == "__main__":
    main()



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/python/rest/url_transcription.py
================================================
#!/usr/bin/env python3
"""
Transcribe Audio from URL with Pulse STT REST API

Usage:
    python url_transcription.py "https://example.com/audio.mp3"

Requirements:
    pip install requests
"""

import os
import sys
import argparse
import requests


def transcribe_from_url(audio_url: str, language: str = "en") -> dict:
    """Transcribe audio from a remote URL."""
    api_key = os.getenv("SMALLEST_API_KEY")
    if not api_key:
        raise ValueError("SMALLEST_API_KEY environment variable not set")

    url = "https://waves-api.smallest.ai/api/v1/pulse/get_text"

    print(f"ðŸ”— URL: {audio_url}")
    print(f"ðŸŒ Language: {language}")
    print()
    print("â³ Transcribing...")

    response = requests.post(
        url,
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        },
        params={
            "model": "pulse",
            "language": language,
            "word_timestamps": "true"
        },
        json={"url": audio_url},
        timeout=120
    )

    if not response.ok:
        print(f"âŒ Error: HTTP {response.status_code}")
        print(response.text)
        sys.exit(1)

    return response.json()


def main():
    parser = argparse.ArgumentParser(description="Transcribe audio from URL")
    parser.add_argument("url", help="URL to audio file")
    parser.add_argument("--language", "-l", default="en",
                        help="Language code (en, hi, es, multi for auto-detect)")

    args = parser.parse_args()

    result = transcribe_from_url(args.url, language=args.language)

    print()
    print("=" * 60)
    print("ðŸ“ TRANSCRIPTION")
    print("=" * 60)
    print(result.get('transcription', '(no transcription)'))

    # Print utterances if available
    utterances = result.get('utterances', [])
    if utterances:
        print()
        print("=" * 60)
        print("ðŸ“œ UTTERANCES")
        print("=" * 60)
        for utt in utterances:
            print(f"  [{utt['start']:6.2f}s - {utt['end']:6.2f}s] {utt['text']}")


if __name__ == "__main__":
    main()



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/python/websocket/streaming_client.py
================================================
#!/usr/bin/env python3
"""
Real-time Speech-to-Text with Pulse STT WebSocket API

Streams audio files or microphone input and displays live transcription.

Usage:
    python streaming_client.py --file recording.wav
    python streaming_client.py --file recording.wav --language hi
    python streaming_client.py  # (microphone, requires pyaudio)

Requirements:
    pip install websockets
    
    For microphone support:
        macOS: brew install portaudio && pip install pyaudio
        Ubuntu: sudo apt-get install portaudio19-dev && pip install pyaudio
"""

import os
import sys
import asyncio
import argparse
import json
import wave
from typing import Callable, Optional

try:
    import websockets
except ImportError:
    print("Install websockets: pip install websockets")
    sys.exit(1)

try:
    import pyaudio
except ImportError:
    pyaudio = None


class PulseStreamingClient:
    """
    Real-time speech-to-text client using Smallest AI Pulse WebSocket API.
    """

    def __init__(
        self,
        api_key: str,
        language: str = "en",
        sample_rate: int = 16000,
        on_transcript: Optional[Callable[[str, bool], None]] = None
    ):
        self.api_key = api_key
        self.language = language
        self.sample_rate = sample_rate
        self.on_transcript = on_transcript or self._default_handler
        self.ws = None
        self.is_running = False

    def _default_handler(self, text: str, is_final: bool):
        if is_final:
            print(f"\nâœ… {text}")
        else:
            print(f"\râ³ {text}...", end="", flush=True)

    def _build_url(self) -> str:
        params = {
            "language": self.language,
            "encoding": "linear16",
            "sample_rate": str(self.sample_rate),
            "word_timestamps": "true",
            "full_transcript": "true"
        }
        query = "&".join(f"{k}={v}" for k, v in params.items())
        return f"wss://waves-api.smallest.ai/api/v1/pulse/get_text?{query}"

    async def _receive_transcripts(self):
        """Handle incoming transcription messages."""
        try:
            async for message in self.ws:
                try:
                    response = json.loads(message)

                    if response.get("status") == "error":
                        msg = response.get("message", "Unknown error")
                        if "timed out" in str(msg).lower():
                            continue
                        print(f"\nâŒ API Error: {msg}")
                        continue

                    if response.get("transcript"):
                        is_final = response.get("is_final", False)
                        self.on_transcript(response["transcript"], is_final)

                except json.JSONDecodeError as e:
                    print(f"\nâŒ Parse error: {e}")

        except websockets.exceptions.ConnectionClosed:
            print("\nðŸ”Œ Connection closed")

    async def _stream_microphone(self):
        """Capture and stream microphone audio."""
        if pyaudio is None:
            print("âŒ PyAudio not installed. Use --file option instead.")
            self.is_running = False
            return

        p = pyaudio.PyAudio()
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=1024
        )

        print("ðŸŽ¤ Listening... (Ctrl+C to stop)\n")

        try:
            while self.is_running:
                audio_data = stream.read(1024, exception_on_overflow=False)

                if self.ws:
                    try:
                        await self.ws.send(audio_data)
                    except websockets.exceptions.ConnectionClosed:
                        break

                await asyncio.sleep(0.01)

        finally:
            stream.stop_stream()
            stream.close()
            p.terminate()

    async def _stream_file(self, file_path: str):
        """Stream a WAV file to the API."""
        print(f"ðŸ“ Streaming file: {file_path}\n")

        with wave.open(file_path, 'rb') as wf:
            # Read audio data (skip header)
            frames = wf.readframes(wf.getnframes())

        chunk_size = 3200  # ~100ms at 16kHz
        for i in range(0, len(frames), chunk_size):
            if not self.is_running:
                break

            chunk = frames[i:i + chunk_size]
            if self.ws:
                try:
                    await self.ws.send(chunk)
                except websockets.exceptions.ConnectionClosed:
                    break

            await asyncio.sleep(0.05)  # Pace the sending

        # Signal end of audio
        if self.ws:
            try:
                await self.ws.send(json.dumps({"type": "end"}))
            except websockets.exceptions.ConnectionClosed:
                pass

        # Wait for final transcripts
        await asyncio.sleep(2)

    async def start(self, file_path: Optional[str] = None):
        """Start the streaming transcription session."""
        self.is_running = True
        headers = {"Authorization": f"Bearer {self.api_key}"}

        print("=" * 60)
        print("ðŸŽ™ï¸  Pulse STT Real-time Streaming")
        print("=" * 60)
        print(f"ðŸŒ Language: {self.language}")
        print(f"ðŸ”Š Sample rate: {self.sample_rate} Hz")
        print()

        try:
            async with websockets.connect(
                self._build_url(),
                additional_headers=headers,
                open_timeout=30
            ) as ws:
                self.ws = ws
                print("âœ… Connected to Pulse STT\n")

                if file_path:
                    # Stream file
                    await asyncio.gather(
                        self._receive_transcripts(),
                        self._stream_file(file_path)
                    )
                else:
                    # Stream microphone
                    await asyncio.gather(
                        self._receive_transcripts(),
                        self._stream_microphone()
                    )

        except websockets.exceptions.InvalidHandshake as e:
            error_msg = str(e)
            if "403" in error_msg:
                print("âŒ HTTP 403 - WebSocket STT requires Enterprise subscription")
            else:
                print(f"âŒ Connection failed: {error_msg}")
        except Exception as e:
            print(f"âŒ Error: {e}")

    def stop(self):
        """Stop the streaming session."""
        self.is_running = False


async def main():
    parser = argparse.ArgumentParser(description="Real-time speech-to-text")
    parser.add_argument("--language", "-l", default="en",
                        help="Language code (en, hi, es, multi)")
    parser.add_argument("--file", "-f", help="Stream a WAV file instead of microphone")

    args = parser.parse_args()

    api_key = os.getenv("SMALLEST_API_KEY")
    if not api_key:
        print("âŒ SMALLEST_API_KEY environment variable not set")
        sys.exit(1)

    transcripts = []

    def handle_transcript(text: str, is_final: bool):
        if is_final:
            transcripts.append(text)
            print(f"\nâœ… {text}")
        else:
            print(f"\râ³ {text}...", end="", flush=True)

    client = PulseStreamingClient(
        api_key=api_key,
        language=args.language,
        on_transcript=handle_transcript
    )

    try:
        await client.start(file_path=args.file)
    except KeyboardInterrupt:
        client.stop()

    print()
    print("=" * 60)
    print("ðŸ“ FULL TRANSCRIPT")
    print("=" * 60)
    print(" ".join(transcripts) if transcripts else "(no speech detected)")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/utils/audio_preprocessing.py
================================================
#!/usr/bin/env python3
"""
Audio Preprocessing Utilities for Pulse STT

Converts audio files to optimal format for STT:
- Mono channel
- 16kHz sample rate
- 16-bit PCM WAV

Usage:
    python audio_preprocessing.py input.mp3 output.wav
    python audio_preprocessing.py --chunk input.wav --duration 300

Requirements:
    pip install pydub

Note: pydub requires ffmpeg installed on your system.
"""

import argparse
import io
import sys
from pathlib import Path

try:
    from pydub import AudioSegment
except ImportError:
    print("Install pydub: pip install pydub")
    print("Also ensure ffmpeg is installed on your system")
    sys.exit(1)


def preprocess_audio(input_path: str, output_path: str = None) -> bytes:
    """
    Convert any audio format to STT-optimal WAV.
    - Mono channel
    - 16kHz sample rate
    - 16-bit PCM

    Args:
        input_path: Path to input audio file
        output_path: Optional path to save output WAV

    Returns:
        WAV audio bytes
    """
    print(f"ðŸ“ Loading: {input_path}")
    audio = AudioSegment.from_file(input_path)

    print(f"   Original: {audio.channels}ch, {audio.frame_rate}Hz, {audio.sample_width * 8}-bit")

    # Convert to mono
    audio = audio.set_channels(1)

    # Resample to 16kHz
    audio = audio.set_frame_rate(16000)

    # Ensure 16-bit
    audio = audio.set_sample_width(2)

    print(f"   Converted: 1ch, 16000Hz, 16-bit")

    # Export as WAV
    buffer = io.BytesIO()
    audio.export(buffer, format="wav")
    buffer.seek(0)
    wav_data = buffer.read()

    if output_path:
        with open(output_path, "wb") as f:
            f.write(wav_data)
        print(f"âœ… Saved: {output_path}")

    return wav_data


def chunk_long_audio(
    input_path: str,
    output_dir: str,
    chunk_duration_ms: int = 300000
) -> list[str]:
    """
    Split long audio files into chunks for batch processing.

    Args:
        input_path: Path to input audio file
        output_dir: Directory to save chunks
        chunk_duration_ms: Duration of each chunk in ms (default: 5 minutes)

    Returns:
        List of output file paths
    """
    print(f"ðŸ“ Loading: {input_path}")
    audio = AudioSegment.from_file(input_path)

    # Convert to STT format
    audio = audio.set_channels(1).set_frame_rate(16000).set_sample_width(2)

    total_duration = len(audio)
    num_chunks = (total_duration + chunk_duration_ms - 1) // chunk_duration_ms

    print(f"   Duration: {total_duration / 1000:.1f}s")
    print(f"   Splitting into {num_chunks} chunks of {chunk_duration_ms / 1000}s each")

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    output_files = []
    base_name = Path(input_path).stem

    for i, start in enumerate(range(0, total_duration, chunk_duration_ms)):
        chunk = audio[start:start + chunk_duration_ms]
        output_path = output_dir / f"{base_name}_chunk_{i:03d}.wav"

        chunk.export(output_path, format="wav")
        output_files.append(str(output_path))
        print(f"   âœ… Chunk {i + 1}/{num_chunks}: {output_path.name}")

    return output_files


def get_audio_info(input_path: str) -> dict:
    """Get information about an audio file."""
    audio = AudioSegment.from_file(input_path)

    return {
        "channels": audio.channels,
        "sample_rate": audio.frame_rate,
        "sample_width": audio.sample_width,
        "duration_seconds": len(audio) / 1000,
        "file_size_bytes": Path(input_path).stat().st_size
    }


def main():
    parser = argparse.ArgumentParser(description="Audio preprocessing for STT")
    parser.add_argument("input", help="Input audio file")
    parser.add_argument("output", nargs="?", help="Output WAV file (optional)")
    parser.add_argument("--chunk", action="store_true",
                        help="Split into chunks instead of converting")
    parser.add_argument("--duration", type=int, default=300,
                        help="Chunk duration in seconds (default: 300)")
    parser.add_argument("--info", action="store_true",
                        help="Just show audio info, don't convert")

    args = parser.parse_args()

    if not Path(args.input).exists():
        print(f"âŒ File not found: {args.input}")
        sys.exit(1)

    if args.info:
        info = get_audio_info(args.input)
        print("=" * 40)
        print("ðŸ“Š AUDIO INFO")
        print("=" * 40)
        print(f"  Channels:    {info['channels']}")
        print(f"  Sample rate: {info['sample_rate']} Hz")
        print(f"  Bit depth:   {info['sample_width'] * 8}-bit")
        print(f"  Duration:    {info['duration_seconds']:.2f}s")
        print(f"  File size:   {info['file_size_bytes']:,} bytes")
        return

    if args.chunk:
        output_dir = args.output or "./chunks"
        chunk_long_audio(
            args.input,
            output_dir,
            chunk_duration_ms=args.duration * 1000
        )
    else:
        output = args.output or args.input.rsplit(".", 1)[0] + "_16k.wav"
        preprocess_audio(args.input, output)


if __name__ == "__main__":
    main()



================================================
FILE: blog-code-samples/pulse-stt-developer-guide/utils/ffmpeg_commands.sh
================================================
#!/bin/bash
#
# FFmpeg Commands for Audio Preprocessing
#
# These commands convert audio files to the optimal format for Pulse STT:
# - Mono channel (-ac 1)
# - 16kHz sample rate (-ar 16000)
# - 16-bit PCM WAV (-acodec pcm_s16le)
#

# ============================================
# BASIC CONVERSIONS
# ============================================

# Convert MP3 to optimal WAV
ffmpeg -i input.mp3 -ar 16000 -ac 1 -acodec pcm_s16le output.wav

# Convert M4A/AAC to WAV
ffmpeg -i input.m4a -ar 16000 -ac 1 -acodec pcm_s16le output.wav

# Convert OGG to WAV
ffmpeg -i input.ogg -ar 16000 -ac 1 -acodec pcm_s16le output.wav

# Convert FLAC to WAV
ffmpeg -i input.flac -ar 16000 -ac 1 -acodec pcm_s16le output.wav


# ============================================
# EXTRACT AUDIO FROM VIDEO
# ============================================

# Extract audio from MP4 video
ffmpeg -i video.mp4 -vn -ar 16000 -ac 1 -acodec pcm_s16le audio.wav

# Extract audio from MKV video
ffmpeg -i video.mkv -vn -ar 16000 -ac 1 -acodec pcm_s16le audio.wav

# Extract audio from WebM video
ffmpeg -i video.webm -vn -ar 16000 -ac 1 -acodec pcm_s16le audio.wav


# ============================================
# SPLIT LONG AUDIO INTO CHUNKS
# ============================================

# Split into 5-minute chunks (300 seconds)
ffmpeg -i long_audio.wav -f segment -segment_time 300 -ar 16000 -ac 1 chunk_%03d.wav

# Split into 10-minute chunks
ffmpeg -i long_audio.wav -f segment -segment_time 600 -ar 16000 -ac 1 chunk_%03d.wav


# ============================================
# TRIM/CUT AUDIO
# ============================================

# Extract 30 seconds starting at 1:00
ffmpeg -i input.wav -ss 00:01:00 -t 30 -ar 16000 -ac 1 output.wav

# Extract from 1:00 to 2:30
ffmpeg -i input.wav -ss 00:01:00 -to 00:02:30 -ar 16000 -ac 1 output.wav


# ============================================
# AUDIO ENHANCEMENT
# ============================================

# Normalize audio volume
ffmpeg -i input.wav -filter:a "loudnorm" -ar 16000 -ac 1 output.wav

# Remove silence from beginning and end
ffmpeg -i input.wav -af "silenceremove=start_periods=1:start_silence=0.5:start_threshold=-50dB,areverse,silenceremove=start_periods=1:start_silence=0.5:start_threshold=-50dB,areverse" -ar 16000 -ac 1 output.wav

# Basic noise reduction (high frequency noise)
ffmpeg -i input.wav -af "highpass=f=200,lowpass=f=3000" -ar 16000 -ac 1 output.wav


# ============================================
# BATCH PROCESSING
# ============================================

# Convert all MP3 files in directory to WAV
for f in *.mp3; do
  ffmpeg -i "$f" -ar 16000 -ac 1 -acodec pcm_s16le "${f%.mp3}.wav"
done

# Convert all M4A files
for f in *.m4a; do
  ffmpeg -i "$f" -ar 16000 -ac 1 -acodec pcm_s16le "${f%.m4a}.wav"
done


# ============================================
# GET AUDIO INFO
# ============================================

# Show audio file information
ffprobe -v quiet -show_format -show_streams input.wav

# Get duration only
ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 input.wav



================================================
FILE: speech-to-text/README.md
================================================
# Speech-to-Text

> **Powered by [Pulse STT](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/overview)**

Convert audio to text using Smallest AI's Pulse Speech-to-Text API. Supports 30+ languages with industry-leading accuracy and speed.

## Examples

| Example | Description |
|---------|-------------|
| [Getting Started](./getting-started/) | Basic transcription - the simplest way to get started |
| [Word-Level Outputs](./word-level-outputs/) | Word timestamps and speaker diarization |
| [Subtitle Generation](./subtitle-generation/) | Generate SRT/VTT subtitles from audio or video |
| [Meeting Notes](./meeting-notes/) | Join meetings via Recall.ai, auto-identify speakers by name |
| [Podcast Summarizer](./podcast-summarizer/) | Transcribe and summarize podcasts with GPT-4o |
| [File Transcription](./file-transcription/) | Transcribe files with all advanced features |
| [Emotion Analyzer](./emotion-analyzer/) | Visualize speaker emotions across a conversation with interactive charts |

### WebSocket Examples

| Example | Description |
|---------|-------------|
| [Streaming Transcription](./websocket/streaming-text-output-transcription/) | Stream audio files via WebSocket |
| [Realtime Microphone](./websocket/realtime-microphone-transcription/) | Gradio web UI with live microphone transcription |
| [Jarvis Voice Assistant](./websocket/jarvis/) | Always-on assistant with wake word, LLM, and TTS |

## Quick Start

> **Prerequisites:** Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root. See the [main README](../README.md#usage) for full setup.

```bash
export SMALLEST_API_KEY="your-api-key-here"
uv run speech-to-text/getting-started/python/transcribe.py recording.wav
```

Get your API key at [smallest.ai/console](https://smallest.ai/console).

## Supported Languages

`en` English Â· `es` Spanish Â· `fr` French Â· `de` German Â· `hi` Hindi Â· `zh` Chinese Â· `ja` Japanese Â· `ko` Korean Â· `pt` Portuguese Â· `ar` Arabic Â· and 20+ more

Use `multi` for automatic language detection.

## Documentation

- [Pulse STT Overview](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/overview)
- [Pre-recorded Audio](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/pre-recorded/quickstart)
- [Streaming Audio](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/streaming/quickstart)
- [Response Format](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/realtime/response-format)
- [API Reference](https://waves-docs.smallest.ai/v4.0.0/content/api-references/pulse-asr)



================================================
FILE: speech-to-text/emotion-analyzer/README.md
================================================
# Emotion Analyzer

Visualize speaker emotions across a conversation. Upload any recording and see who said what â€” and how they felt saying it â€” powered by Smallest AI Pulse STT emotion detection.

## Demo

![Demo](demo/demo.gif)

## Features

- Upload audio and get a diarized transcript with per-segment emotion scores
- Interactive timeline chart with one line per speakerâ€“emotion combination
- Color-coded emotion filters (happiness, sadness, anger, fear, disgust)
- Speaker filters with dash-style line indicators
- Concurrent emotion detection for fast results on long recordings

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../README.md#usage) for setup. Add `SMALLEST_API_KEY` to your `.env`.

Extra dependencies:

```bash
uv pip install -r requirements.txt
```

## Usage

```bash
uv run backend/app.py
```

Open [http://localhost:5000](http://localhost:5000) in your browser.

1. Drop or select an audio file
2. Click **Analyze Emotions**
3. Explore the results â€” emotion timeline, speaker filters, and full transcript table

## Recommended Usage

- Analyzing tone and sentiment shifts across a multi-speaker conversation
- Comparing emotional patterns between speakers over time
- For basic emotion scores without visualization, [File Transcription](../file-transcription/) supports `emotion_detection=true` directly

## Structure

```
emotion-analyzer/
â”œâ”€â”€ .env.sample          # Environment variable template
â”œâ”€â”€ requirements.txt     # Extra Python dependencies (flask, pydub)
â”œâ”€â”€ README.md
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ app.py           # Flask backend â€” upload, diarize, merge, split, emotion detect
â””â”€â”€ frontend/
    â”œâ”€â”€ index.html       # Minimal HTML shell
    â”œâ”€â”€ app.js           # UI logic
    â””â”€â”€ style.css        # Styles and theming
```

## How It Works

1. **Upload** â€” the Flask backend receives an audio file via `/api/analyze`
2. **Diarize** â€” sends the full audio to Pulse STT with `diarize=true` and `word_timestamps=true`, getting speaker-labeled utterances
3. **Merge** â€” utterances sharing the same timestamp and speaker are merged into single segments
4. **Split** â€” pydub slices the original audio at utterance boundaries into per-segment WAV clips
5. **Detect emotions** â€” each segment is sent to Pulse STT with `emotion_detection=true` concurrently (up to 10 workers)
6. **Visualize** â€” the frontend renders an interactive Chart.js timeline with emotion/speaker filter chips and a transcript table

## Supported Formats

Audio: WAV, MP3, FLAC, OGG, M4A, AAC, WMA

## API Reference

- [Pulse STT Overview](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/overview)
- [Emotion Detection](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/features/emotion-detection)
- [Pre-recorded API](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/pre-recorded/quickstart)

## Next Steps

- [Realtime Microphone](../websocket/realtime-microphone-transcription/) â€” Live microphone transcription via WebSocket



================================================
FILE: speech-to-text/emotion-analyzer/requirements.txt
================================================
flask
pydub



================================================
FILE: speech-to-text/emotion-analyzer/.env.sample
================================================
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here



================================================
FILE: speech-to-text/emotion-analyzer/backend/app.py
================================================
#!/usr/bin/env python3
"""
Emotion Analyzer â€” Smallest AI Pulse STT

Upload audio â†’ diarize speakers â†’ detect emotions per segment â†’ visualize.

Usage:
    uv run backend/app.py

Opens a web UI at http://localhost:5000.
"""

import os
import io
import logging
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import Flask, request, jsonify, send_from_directory
from pydub import AudioSegment
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

app = Flask(__name__, static_folder="../frontend", static_url_path="")

API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text"
API_KEY = os.environ.get("SMALLEST_API_KEY", "")
MAX_WORKERS = 10


def transcribe_with_diarization(audio_bytes: bytes) -> dict:
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/octet-stream"}
    params = {"language": "en", "diarize": "true", "word_timestamps": "true"}
    logger.info("Transcribing audio (%d bytes)...", len(audio_bytes))
    resp = requests.post(API_URL, headers=headers, params=params, data=audio_bytes, timeout=600)
    resp.raise_for_status()
    result = resp.json()
    logger.info("Transcription done â€” %d utterances", len(result.get("utterances", [])))
    return result


def detect_emotions_for_segment(segment_bytes: bytes) -> dict:
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/octet-stream"}
    params = {"language": "en", "emotion_detection": "true"}
    resp = requests.post(API_URL, headers=headers, params=params, data=segment_bytes, timeout=300)
    resp.raise_for_status()
    return resp.json()


def split_audio_segment(audio: AudioSegment, start_sec: float, end_sec: float) -> bytes:
    segment = audio[int(start_sec * 1000):int(end_sec * 1000)]
    buf = io.BytesIO()
    segment.export(buf, format="wav")
    return buf.getvalue()


def process_utterance(idx: int, total: int, utt: dict, audio: AudioSegment) -> dict:
    speaker = utt.get("speaker", "unknown")
    start = utt.get("start", 0)
    end = utt.get("end", 0)
    text = utt.get("text", "")

    if (end - start) < 0.5:
        logger.info("[%d/%d] %s %.1fsâ€“%.1fs skipped (too short)", idx + 1, total, speaker, start, end)
        return {"index": idx, "speaker": speaker, "start": start, "end": end, "text": text, "emotions": None, "skipped": True}

    try:
        seg_bytes = split_audio_segment(audio, start, end)
        emotions = detect_emotions_for_segment(seg_bytes).get("emotions", {})
        top = max(emotions, key=emotions.get) if emotions else "none"
        logger.info("[%d/%d] %s %.1fsâ€“%.1fs â†’ %s (%.2f)", idx + 1, total, speaker, start, end, top, emotions.get(top, 0))
    except Exception as e:
        logger.warning("[%d/%d] %s %.1fsâ€“%.1fs failed: %s", idx + 1, total, speaker, start, end, e)
        emotions = None

    return {"index": idx, "speaker": speaker, "start": start, "end": end, "text": text, "emotions": emotions, "skipped": False}


@app.route("/")
def index():
    return send_from_directory(app.static_folder, "index.html")


@app.route("/api/analyze", methods=["POST"])
def analyze():
    if not API_KEY:
        return jsonify({"error": "SMALLEST_API_KEY not set"}), 500

    file = request.files.get("audio")
    if not file:
        return jsonify({"error": "No audio file provided"}), 400

    audio_bytes = file.read()
    logger.info("Received: %s (%.2f MB)", file.filename, len(audio_bytes) / 1024 / 1024)

    # Step 1: Diarized transcription
    try:
        diarized = transcribe_with_diarization(audio_bytes)
    except requests.RequestException as e:
        logger.error("Transcription failed: %s", e)
        return jsonify({"error": f"Transcription failed: {e}"}), 502

    utterances = diarized.get("utterances", [])
    if not utterances:
        return jsonify({"error": "No utterances found â€” audio may be too short or silent."}), 400

    # Merge utterances sharing the same (start, end, speaker)
    merged, prev_key = [], None
    for utt in utterances:
        key = (utt.get("start"), utt.get("end"), utt.get("speaker"))
        if merged and key == prev_key:
            merged[-1]["text"] += utt.get("text", "")
        else:
            merged.append({
                "start": utt.get("start", 0),
                "end": utt.get("end", 0),
                "speaker": f"Speaker {utt.get('speaker', '?')}",
                "text": utt.get("text", ""),
            })
            prev_key = key

    logger.info("Merged %d utterances â†’ %d segments", len(utterances), len(merged))
    utterances = merged

    # Load audio for splitting
    try:
        audio = AudioSegment.from_file(io.BytesIO(audio_bytes))
        logger.info("Audio: %.1fs, %d channels, %d Hz", len(audio) / 1000, audio.channels, audio.frame_rate)
    except Exception as e:
        logger.error("Audio decode failed: %s", e)
        return jsonify({"error": f"Could not decode audio: {e}"}), 400

    # Step 2: Concurrent emotion detection
    total = len(utterances)
    logger.info("Running emotion detection on %d segments (%d workers)...", total, MAX_WORKERS)

    results = [None] * total
    speakers_seen = set()

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {}
        for i, utt in enumerate(utterances):
            speakers_seen.add(utt.get("speaker", "unknown"))
            futures[executor.submit(process_utterance, i, total, utt, audio)] = i

        done = 0
        for future in as_completed(futures):
            results[future.result()["index"]] = future.result()
            done += 1
            if done % 5 == 0 or done == total:
                logger.info("Progress: %d/%d", done, total)

    for r in results:
        r.pop("index", None)

    logger.info("Done â€” %d segments, %d speakers", len(results), len(speakers_seen))

    return jsonify({
        "transcription": diarized.get("transcription", ""),
        "speakers": sorted(speakers_seen),
        "segments": results,
    })


if __name__ == "__main__":
    if not API_KEY:
        logger.warning("SMALLEST_API_KEY not set â€” export SMALLEST_API_KEY=your-key")
    else:
        logger.info("API key loaded (...%s)", API_KEY[-4:])
    logger.info("Starting on http://localhost:5000")
    app.run(host="0.0.0.0", port=5000, debug=True)



================================================
FILE: speech-to-text/emotion-analyzer/frontend/app.js
================================================
/**
 * Emotion Analyzer â€” Smallest AI Pulse STT
 * Builds DOM, loads Chart.js, handles upload/analysis/visualization.
 */
(function () {
  'use strict';

  // Load CSS + set page meta
  document.title = 'Emotion Analyzer \u2014 Smallest AI';
  var link = document.createElement('link');
  link.rel = 'stylesheet';
  link.href = 'style.css';
  document.head.appendChild(link);

  if (!document.querySelector('meta[name="viewport"]')) {
    var meta = document.createElement('meta');
    meta.name = 'viewport';
    meta.content = 'width=device-width, initial-scale=1.0';
    document.head.appendChild(meta);
  }

  // Build DOM
  var root = document.createElement('div');
  root.className = 'container';
  root.innerHTML =
    '<div class="header">' +
      '<span class="badge">Pulse STT</span>' +
      '<h1>Emotion Analyzer</h1>' +
      '<p>Upload a recording. See who said what, and how they felt saying it. Powered by Smallest AI emotion detection.</p>' +
    '</div>' +
    '<div class="upload-area" id="uploadArea">' +
      '<span class="icon">\uD83C\uDFA4</span>' +
      '<h3>Drop your audio file here</h3>' +
      '<p>WAV, MP3, FLAC, OGG, M4A \u2014 up to 100 MB</p>' +
      '<input type="file" id="fileInput" accept="audio/*">' +
      '<div class="file-info" id="fileInfo">' +
        '<span class="name" id="fileName"></span>' +
        '<button class="remove" id="removeFile">Remove</button>' +
      '</div>' +
    '</div>' +
    '<button class="analyze-btn" id="analyzeBtn" disabled>Analyze Emotions</button>' +
    '<div class="error-msg" id="errorMsg"></div>' +
    '<div class="progress-section" id="progressSection">' +
      '<div class="step" id="progressStep">Uploading audio...</div>' +
      '<div class="progress-bar-bg"><div class="progress-bar-fill" id="progressBar"></div></div>' +
      '<div class="detail" id="progressDetail">This may take a minute for longer recordings</div>' +
    '</div>' +
    '<div class="results-section" id="resultsSection">' +
      '<div class="filters">' +
        '<div class="filter-group"><label>Emotions</label><div class="filter-chips" id="emotionFilters"></div></div>' +
        '<div class="filter-group"><label>Speakers</label><div class="filter-chips" id="speakerFilters"></div></div>' +
      '</div>' +
      '<div class="chart-card"><h3>Emotion Timeline</h3><div class="chart-wrapper"><canvas id="emotionChart"></canvas></div></div>' +
      '<div class="transcript-card"><h3>Transcript with Emotions</h3>' +
        '<table class="transcript-table"><thead><tr><th>Speaker</th><th>Time</th><th>Text</th><th>Emotions</th></tr></thead><tbody id="transcriptBody"></tbody></table>' +
      '</div>' +
    '</div>';
  document.body.appendChild(root);

  // Load Chart.js then boot
  var script = document.createElement('script');
  script.src = 'https://cdn.jsdelivr.net/npm/chart.js@4.4.8/dist/chart.umd.min.js';
  script.onload = function () { boot(true); };
  script.onerror = function () { console.error('Chart.js failed to load'); boot(false); };
  document.head.appendChild(script);

  function boot(chartAvailable) {

    // Config
    var EMOTION_KEYS = ['happiness', 'sadness', 'anger', 'fear', 'disgust'];
    var EMOTION_COLORS = { happiness: '#4ade80', sadness: '#60a5fa', anger: '#f87171', fear: '#c084fc', disgust: '#facc15' };
    var MAX_DEFAULT_SPEAKERS = 5;
    var SPEAKER_DASHES = [[], [6,3], [2,2], [8,4,2,4], [4,4], [12,4], [2,6], [6,3,2,3]];
    var EMOTION_ALIASES = {
      happy: 'happiness', joy: 'happiness', sad: 'sadness', sorrow: 'sadness',
      angry: 'anger', mad: 'anger', afraid: 'fear', scared: 'fear', disgusted: 'disgust', neutral: 'neutral',
    };

    // State
    var selectedFile = null;
    var analysisData = null;
    var chart = null;
    var activeEmotions = new Set(EMOTION_KEYS);
    var activeSpeakers = new Set();

    // DOM refs
    var $upload     = document.getElementById('uploadArea');
    var $fileInput  = document.getElementById('fileInput');
    var $fileInfo   = document.getElementById('fileInfo');
    var $fileName   = document.getElementById('fileName');
    var $removeBtn  = document.getElementById('removeFile');
    var $analyzeBtn = document.getElementById('analyzeBtn');
    var $error      = document.getElementById('errorMsg');
    var $progress   = document.getElementById('progressSection');
    var $progBar    = document.getElementById('progressBar');
    var $progStep   = document.getElementById('progressStep');
    var $progDetail = document.getElementById('progressDetail');
    var $results    = document.getElementById('resultsSection');

    // Helpers
    function fmtSpeaker(s) {
      if (s == null || s === '') return 'Unknown';
      if (typeof s === 'string' && s.startsWith('Speaker ')) return s;
      if (typeof s === 'string' && s.startsWith('speaker_')) return s.replace('speaker_', 'Speaker ');
      if (typeof s === 'number') return 'Speaker ' + s;
      return String(s);
    }

    function fmtTime(sec) {
      var m = Math.floor(sec / 60);
      var s = Math.floor(sec % 60);
      var ms = Math.round((sec % 1) * 10);
      return m + ':' + String(s).padStart(2, '0') + '.' + ms;
    }

    function showProgress(step, pct, detail) {
      $progress.classList.add('visible');
      $progStep.textContent = step;
      $progDetail.textContent = detail;
      $progBar.style.width = pct + '%';
    }
    function hideProgress() { $progress.classList.remove('visible'); }
    function setProgress(pct) { $progBar.style.width = pct + '%'; }
    function showError(msg) { $error.textContent = msg; $error.classList.add('visible'); }
    function hideError() { $error.classList.remove('visible'); }

    // Normalize emotion keys from API response
    function normalizeEmotionKeys(segments) {
      segments.forEach(function (seg) {
        if (!seg.emotions) return;
        var normalized = {};
        Object.keys(seg.emotions).forEach(function (key) {
          normalized[EMOTION_ALIASES[key] || key] = seg.emotions[key];
        });
        seg.emotions = normalized;
        Object.keys(normalized).forEach(function (k) {
          if (EMOTION_KEYS.indexOf(k) === -1 && k !== 'neutral') {
            EMOTION_KEYS.push(k);
            EMOTION_COLORS[k] = '#a3a3a3';
          }
        });
      });
    }

    // Chart
    function chartOptions() {
      return {
        responsive: true, maintainAspectRatio: false,
        animation: { duration: 600 },
        interaction: { mode: 'nearest', intersect: false },
        plugins: {
          legend: { display: false },
          tooltip: {
            backgroundColor: '#0e2e33', titleColor: '#F8FAF5', bodyColor: '#9cbcbc',
            borderColor: '#1a4448', borderWidth: 1, padding: 12,
            callbacks: {
              title: function (items) { return 'Time: ' + fmtTime(items[0].parsed.x); },
              label: function (item) { return ' ' + item.dataset.label + ': ' + (item.parsed.y * 100).toFixed(0) + '%'; }
            }
          }
        },
        scales: {
          x: {
            type: 'linear',
            title: { display: true, text: 'Time (s)', color: '#6a9999', font: { size: 11 } },
            ticks: { color: '#6a9999', font: { size: 10 }, callback: function (v) { return fmtTime(v); } },
            grid: { color: 'rgba(26,68,72,0.4)' }
          },
          y: {
            min: 0, max: 1,
            title: { display: true, text: 'Score', color: '#6a9999', font: { size: 11 } },
            ticks: { color: '#6a9999', font: { size: 10 }, callback: function (v) { return (v * 100) + '%'; } },
            grid: { color: 'rgba(26,68,72,0.4)' }
          }
        }
      };
    }

    function initEmptyChart() {
      if (!chartAvailable) return;
      $results.classList.add('visible');
      renderEmotionChips();
      document.getElementById('speakerFilters').innerHTML =
        '<span style="color:var(--text-muted);font-size:0.8rem">Upload audio to see speakers</span>';
      chart = new Chart(document.getElementById('emotionChart').getContext('2d'), {
        type: 'line', data: { datasets: [] }, options: chartOptions()
      });
    }

    function buildChartData() {
      if (!analysisData || !analysisData.segments) return { datasets: [] };
      var allSpeakers = analysisData.speakers || [];
      var segments = analysisData.segments.filter(function (s) { return s.emotions && !s.skipped; });
      var datasets = [];

      EMOTION_KEYS.forEach(function (em) {
        if (!activeEmotions.has(em)) return;
        allSpeakers.forEach(function (sp) {
          if (!activeSpeakers.has(sp)) return;
          var spIdx = allSpeakers.indexOf(sp);
          var points = segments
            .filter(function (s) { return s.speaker === sp; })
            .map(function (s) { return { x: s.start, y: (s.emotions[em] != null) ? s.emotions[em] : 0 }; })
            .sort(function (a, b) { return a.x - b.x; });
          if (!points.length) return;
          datasets.push({
            label: fmtSpeaker(sp) + ' \u2014 ' + em.charAt(0).toUpperCase() + em.slice(1),
            data: points,
            borderColor: EMOTION_COLORS[em] || '#a3a3a3',
            backgroundColor: (EMOTION_COLORS[em] || '#a3a3a3') + '22',
            borderWidth: 2, borderDash: SPEAKER_DASHES[spIdx % SPEAKER_DASHES.length],
            pointRadius: 4, pointHoverRadius: 7, tension: 0.3, fill: false,
          });
        });
      });
      return { datasets: datasets };
    }

    function clearChartData() { if (chart) { chart.data = { datasets: [] }; chart.update(); } }
    function updateChartData() { if (!chart) return; chart.data = buildChartData(); chart.update(); }

    // Filter chips
    function renderEmotionChips() {
      var container = document.getElementById('emotionFilters');
      container.innerHTML = '';
      EMOTION_KEYS.forEach(function (em) {
        var btn = document.createElement('button');
        btn.className = 'chip emotion-' + em + (activeEmotions.has(em) ? ' active' : '');
        btn.textContent = em.charAt(0).toUpperCase() + em.slice(1);
        btn.onclick = function () {
          if (activeEmotions.has(em)) activeEmotions.delete(em); else activeEmotions.add(em);
          btn.classList.toggle('active');
          updateChartData();
        };
        container.appendChild(btn);
      });
    }

    function speakerDashSVG(spIdx) {
      var dash = SPEAKER_DASHES[spIdx % SPEAKER_DASHES.length];
      var attr = dash.length ? ' stroke-dasharray="' + dash.join(',') + '"' : '';
      return '<svg class="dash-icon" width="22" height="10" viewBox="0 0 22 10">' +
             '<line x1="0" y1="5" x2="22" y2="5" stroke="currentColor" stroke-width="2.5"' + attr + '/></svg>';
    }

    function renderSpeakerChips(speakers) {
      var container = document.getElementById('speakerFilters');
      container.innerHTML = '';
      speakers.forEach(function (sp, idx) {
        var btn = document.createElement('button');
        btn.className = 'chip speaker-chip' + (activeSpeakers.has(sp) ? ' active' : '');
        btn.innerHTML = speakerDashSVG(idx) + ' ' + fmtSpeaker(sp);
        btn.onclick = function () {
          if (activeSpeakers.has(sp)) activeSpeakers.delete(sp); else activeSpeakers.add(sp);
          btn.classList.toggle('active');
          updateChartData();
        };
        container.appendChild(btn);
      });
    }

    // Transcript
    function clearTranscript() { document.getElementById('transcriptBody').innerHTML = ''; }

    function renderTranscript(segments) {
      var tbody = document.getElementById('transcriptBody');
      tbody.innerHTML = '';
      segments.forEach(function (seg) {
        var tr = document.createElement('tr');

        var tdSp = document.createElement('td');
        tdSp.innerHTML = '<span class="speaker-tag">' + fmtSpeaker(seg.speaker) + '</span>';

        var tdTime = document.createElement('td');
        tdTime.className = 'time';
        tdTime.textContent = fmtTime(seg.start) + ' \u2013 ' + fmtTime(seg.end);

        var tdText = document.createElement('td');
        tdText.className = 'text';
        tdText.textContent = seg.text;

        var tdEm = document.createElement('td');
        if (seg.emotions && !seg.skipped) {
          var group = document.createElement('div');
          group.className = 'emotion-bar-group';
          Object.entries(seg.emotions)
            .sort(function (a, b) { return b[1] - a[1]; })
            .slice(0, 3)
            .forEach(function (pair) {
              var pill = document.createElement('span');
              pill.className = 'emotion-mini';
              pill.innerHTML = '<span class="dot" style="background:' + (EMOTION_COLORS[pair[0]] || '#999') + '"></span>' +
                               pair[0] + ' ' + (pair[1] * 100).toFixed(0) + '%';
              group.appendChild(pill);
            });
          tdEm.appendChild(group);
        } else {
          tdEm.innerHTML = '<span style="color:var(--text-muted);font-size:0.75rem">\u2014</span>';
        }

        tr.appendChild(tdSp); tr.appendChild(tdTime); tr.appendChild(tdText); tr.appendChild(tdEm);
        tbody.appendChild(tr);
      });
    }

    // Populate results
    function populateResults(data) {
      activeSpeakers = new Set((data.speakers || []).slice(0, MAX_DEFAULT_SPEAKERS));
      activeEmotions = new Set(EMOTION_KEYS);
      renderEmotionChips();
      renderSpeakerChips(data.speakers || []);
      updateChartData();
      renderTranscript(data.segments || []);
      $analyzeBtn.disabled = false;
    }

    // Upload handling
    $upload.addEventListener('click', function () { $fileInput.click(); });
    $upload.addEventListener('dragover', function (e) { e.preventDefault(); $upload.classList.add('dragover'); });
    $upload.addEventListener('dragleave', function () { $upload.classList.remove('dragover'); });
    $upload.addEventListener('drop', function (e) {
      e.preventDefault(); $upload.classList.remove('dragover');
      if (e.dataTransfer.files.length) pickFile(e.dataTransfer.files[0]);
    });
    $fileInput.addEventListener('change', function () { if ($fileInput.files.length) pickFile($fileInput.files[0]); });
    $removeBtn.addEventListener('click', function (e) { e.stopPropagation(); clearFile(); });

    function pickFile(file) {
      selectedFile = file;
      $fileName.textContent = file.name + ' (' + (file.size / 1024 / 1024).toFixed(1) + ' MB)';
      $fileInfo.classList.add('visible');
      $analyzeBtn.disabled = false;
      hideError();
    }

    function clearFile() {
      selectedFile = null;
      $fileInput.value = '';
      $fileInfo.classList.remove('visible');
      $analyzeBtn.disabled = true;
    }

    // Analyze
    $analyzeBtn.addEventListener('click', runAnalysis);

    async function runAnalysis() {
      if (!selectedFile) return;
      hideError();
      $analyzeBtn.disabled = true;
      analysisData = null;
      clearChartData();
      clearTranscript();

      showProgress('Uploading & transcribing audio...', 10, 'Diarizing speakers and extracting timestamps');

      var formData = new FormData();
      formData.append('audio', selectedFile);

      var progress = 10;
      var tick = setInterval(function () {
        progress = Math.min(progress + 2, 85);
        setProgress(progress);
        if (progress > 40) {
          $progStep.textContent = 'Analyzing emotions per speaker segment...';
          $progDetail.textContent = 'Running emotion detection on each utterance';
        }
      }, 1500);

      try {
        var resp = await fetch('/api/analyze', { method: 'POST', body: formData });
        clearInterval(tick);
        var data = await resp.json();
        if (!resp.ok) throw new Error(data.error || 'Server error (' + resp.status + ')');

        setProgress(95);
        $progStep.textContent = 'Rendering results...';
        $progDetail.textContent = '';

        analysisData = data;
        normalizeEmotionKeys(analysisData.segments);
        setProgress(100);

        setTimeout(function () { hideProgress(); populateResults(analysisData); }, 300);
      } catch (err) {
        clearInterval(tick);
        hideProgress();
        showError(err.message);
        $analyzeBtn.disabled = false;
        console.error('Analysis failed:', err);
      }
    }

    initEmptyChart();
  }

})();



================================================
FILE: speech-to-text/emotion-analyzer/frontend/index.html
================================================
<!DOCTYPE html>
<html lang="en">
<head><meta charset="UTF-8"></head>
<body><script src="app.js"></script></body>
</html>



================================================
FILE: speech-to-text/emotion-analyzer/frontend/style.css
================================================
:root {
  --bg-dark: #092023;
  --bg-card: #0e2e33;
  --teal-deep: #1D4E52;
  --teal-main: #43B6B6;
  --teal-light: #5ccfcf;
  --teal-muted: #2a7a7a;
  --surface: #F8FAF5;
  --text-primary: #F8FAF5;
  --text-secondary: #9cbcbc;
  --text-muted: #6a9999;
  --border: #1a4448;
  --emotion-happiness: #4ade80;
  --emotion-sadness: #60a5fa;
  --emotion-anger: #f87171;
  --emotion-fear: #c084fc;
  --emotion-disgust: #facc15;
}
* { margin:0; padding:0; box-sizing:border-box; }
body {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
  background: var(--bg-dark);
  color: var(--text-primary);
  min-height: 100vh;
}
.container { max-width:1400px; margin:0 auto; padding:20px 24px; }

/* Header */
.header { text-align:center; margin-bottom:12px; }
.header h1 { font-size:1.8rem; font-weight:700; color:var(--teal-main); margin-bottom:4px; }
.header p { color:var(--text-secondary); font-size:0.82rem; max-width:600px; margin:0 auto; line-height:1.4; }
.header .badge {
  display:inline-block; background:var(--teal-deep); color:var(--teal-main);
  padding:3px 10px; border-radius:100px; font-size:0.65rem; font-weight:600;
  letter-spacing:0.05em; text-transform:uppercase; margin-bottom:8px;
}

/* Upload */
.upload-area {
  border:2px dashed var(--teal-deep); border-radius:10px; padding:14px 20px;
  text-align:center; cursor:pointer; transition:all 0.2s;
  background:var(--bg-card); margin:0 auto 12px; max-width:440px;
}
.upload-area:hover, .upload-area.dragover { border-color:var(--teal-main); background:rgba(67,182,182,0.05); }
.upload-area .icon { font-size:22px; margin-bottom:2px; display:inline-block; vertical-align:middle; margin-right:8px; }
.upload-area h3 { color:var(--text-primary); font-size:0.85rem; margin-bottom:2px; display:inline; }
.upload-area p { color:var(--text-muted); font-size:0.72rem; margin-top:4px; }
.upload-area input[type="file"] { display:none; }

.file-info { display:none; align-items:center; justify-content:center; gap:10px; margin-top:10px; }
.file-info.visible { display:flex; }
.file-info .name { color:var(--teal-light); font-weight:500; font-size:0.82rem; }
.file-info .remove { background:none; border:none; color:var(--text-muted); cursor:pointer; font-size:0.75rem; text-decoration:underline; }

/* Analyze Button */
.analyze-btn {
  display:block; width:100%; max-width:260px; margin:0 auto 16px;
  padding:10px 20px; background:var(--teal-main); color:var(--bg-dark);
  border:none; border-radius:8px; font-size:0.9rem; font-weight:600;
  cursor:pointer; transition:all 0.2s;
}
.analyze-btn:hover:not(:disabled) { background:var(--teal-light); transform:translateY(-1px); }
.analyze-btn:disabled { opacity:0.4; cursor:not-allowed; }

/* Progress */
.progress-section { display:none; text-align:center; margin-bottom:16px; }
.progress-section.visible { display:block; }
.progress-bar-bg { width:100%; max-width:480px; height:6px; background:var(--teal-deep); border-radius:3px; margin:16px auto; overflow:hidden; }
.progress-bar-fill { height:100%; background:var(--teal-main); border-radius:3px; transition:width 0.4s ease; width:0%; }
.progress-section .step { color:var(--teal-main); font-size:0.9rem; font-weight:500; }
.progress-section .detail { color:var(--text-muted); font-size:0.8rem; margin-top:4px; }

/* Results */
.results-section { display:none; }
.results-section.visible { display:grid; grid-template-columns:1fr 1fr; gap:24px; }
.results-section .filters { grid-column:1/-1; }

/* Filters */
.filters { display:flex; gap:16px; margin-bottom:16px; flex-wrap:wrap; }
.filter-group { flex:1; min-width:200px; }
.filter-group label { display:block; font-size:0.8rem; color:var(--text-muted); text-transform:uppercase; letter-spacing:0.05em; font-weight:600; margin-bottom:10px; }
.filter-chips { display:flex; flex-wrap:wrap; gap:8px; }
.chip {
  padding:6px 14px; border-radius:100px; font-size:0.8rem; font-weight:500;
  cursor:pointer; border:1.5px solid var(--border); background:transparent;
  color:var(--text-secondary); transition:all 0.15s; user-select:none;
}
.chip:hover { border-color:var(--teal-muted); }
.chip.active { border-color:var(--teal-main); background:rgba(67,182,182,0.12); color:var(--teal-light); }
.chip.emotion-happiness.active { border-color:var(--emotion-happiness); color:var(--emotion-happiness); background:rgba(74,222,128,0.1); }
.chip.emotion-sadness.active   { border-color:var(--emotion-sadness);  color:var(--emotion-sadness);  background:rgba(96,165,250,0.1); }
.chip.emotion-anger.active     { border-color:var(--emotion-anger);    color:var(--emotion-anger);    background:rgba(248,113,113,0.1); }
.chip.emotion-fear.active      { border-color:var(--emotion-fear);     color:var(--emotion-fear);     background:rgba(192,132,252,0.1); }
.chip.emotion-disgust.active   { border-color:var(--emotion-disgust);  color:var(--emotion-disgust);  background:rgba(250,204,21,0.1); }

.speaker-chip { display:inline-flex; align-items:center; gap:6px; }
.dash-icon { vertical-align:middle; flex-shrink:0; }

/* Chart */
.chart-card { background:var(--bg-card); border:1px solid var(--border); border-radius:16px; padding:24px; min-width:0; }
.chart-card h3 { font-size:1rem; color:var(--text-primary); margin-bottom:16px; font-weight:600; }
.chart-wrapper { position:relative; width:100%; height:380px; }

/* Transcript */
.transcript-card { background:var(--bg-card); border:1px solid var(--border); border-radius:16px; padding:24px; min-width:0; max-height:480px; overflow-y:auto; }
.transcript-card h3 { font-size:1rem; color:var(--text-primary); margin-bottom:16px; font-weight:600; }
.transcript-table { width:100%; border-collapse:collapse; }
.transcript-table th { text-align:left; padding:10px 12px; font-size:0.75rem; color:var(--text-muted); text-transform:uppercase; letter-spacing:0.04em; border-bottom:1px solid var(--border); font-weight:600; }
.transcript-table td { padding:12px; font-size:0.85rem; border-bottom:1px solid rgba(26,68,72,0.5); vertical-align:top; }
.transcript-table .speaker-tag { display:inline-block; padding:2px 10px; border-radius:100px; font-size:0.75rem; font-weight:600; background:var(--teal-deep); color:var(--teal-main); white-space:nowrap; }
.transcript-table .time { color:var(--text-muted); font-variant-numeric:tabular-nums; white-space:nowrap; }
.transcript-table .text { color:var(--text-secondary); }
.emotion-bar-group { display:flex; gap:4px; align-items:center; flex-wrap:wrap; }
.emotion-mini { display:inline-flex; align-items:center; gap:3px; font-size:0.7rem; color:var(--text-muted); padding:2px 6px; border-radius:4px; background:rgba(67,182,182,0.06); }
.emotion-mini .dot { width:6px; height:6px; border-radius:50%; }

/* Error */
.error-msg { display:none; background:rgba(248,113,113,0.1); border:1px solid rgba(248,113,113,0.3); color:#f87171; border-radius:10px; padding:14px 20px; text-align:center; font-size:0.9rem; margin-bottom:24px; }
.error-msg.visible { display:block; }

@media (max-width:960px) {
  .results-section.visible { grid-template-columns:1fr; }
  .transcript-card { max-height:none; }
}
@media (max-width:640px) {
  .container { padding:24px 16px; }
  .header h1 { font-size:1.5rem; }
  .filters { flex-direction:column; gap:16px; }
  .chart-wrapper { height:280px; }
}



================================================
FILE: speech-to-text/file-transcription/README.md
================================================
# File Transcription

Transcribe audio files with advanced features like word timestamps, speaker diarization, and emotion detection.

## Features

- Transcribe audio files with language selection
- Enable advanced features (timestamps, diarization, emotion detection)
- Age and gender prediction
- Save transcription as text and JSON

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../README.md#usage) for setup. Add `SMALLEST_API_KEY` to your `.env`.

## Usage

### Python

```bash
uv run python/transcribe.py recording.wav
```

### JavaScript

```bash
node javascript/transcribe.js recording.wav
```

## Recommended Usage

- Transcription with paralinguistic features â€” emotions, age/gender detection
- Batch processing of audio/video files with detailed metadata output
- Speaker diarization and word-level timestamps on pre-recorded files
- For streaming or PII/PCI redaction, the [WebSocket API](../websocket/streaming-text-output-transcription/) is recommended

## Configuration

| Parameter | Description | Default |
|-----------|-------------|---------|
| `LANGUAGE` | Language code (ISO 639-1) or `multi` for auto-detect | `en` |
| `WORD_TIMESTAMPS` | Include word-level timestamps | `false` |
| `DIARIZE` | Perform speaker diarization | `false` |
| `AGE_DETECTION` | Predict age group of speaker | `false` |
| `GENDER_DETECTION` | Predict gender of speaker | `false` |
| `EMOTION_DETECTION` | Predict speaker emotions | `false` |

## Example Output

- `{filename}_transcript.txt` â€” Plain text transcription
- `{filename}_result.json` â€” Full API response with metadata

## API Reference

- [Pre-recorded Quickstart](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/pre-recorded/quickstart)
- [Pulse STT API Reference](https://waves-docs.smallest.ai/v4.0.0/content/api-references/pulse-asr)

## Next Steps

- [Word-Level Outputs](../word-level-outputs/) â€” Focused example for timestamps and diarization
- [Subtitle Generation](../subtitle-generation/) â€” Generate SRT/VTT files from transcriptions



================================================
FILE: speech-to-text/file-transcription/.env.sample
================================================
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here



================================================
FILE: speech-to-text/file-transcription/javascript/transcribe.js
================================================
#!/usr/bin/env node
/**
 * Smallest AI Speech-to-Text - File Transcription
 *
 * Transcribe audio files with advanced features like word timestamps,
 * speaker diarization, and emotion detection.
 *
 * Usage: node transcribe.js <audio_file>
 *
 * Output:
 * - Command line response with feature outputs
 * - {filename}_transcript.txt - Plain text transcription
 * - {filename}_result.json - Full API response
 */

const fs = require("fs");
const path = require("path");

const API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text";
const OUTPUT_DIR = ".";

// The following are all the features supported by the POST endpoint (Pre-Recorded API)
const LANGUAGE = "en"; // Use ISO 639-1 codes or "multi" for auto-detect
const WORD_TIMESTAMPS = true;
const DIARIZE = true;
const AGE_DETECTION = true;
const GENDER_DETECTION = true;
const EMOTION_DETECTION = true;

async function transcribe(audioFile, apiKey) {
  const audioData = fs.readFileSync(audioFile);

  const params = new URLSearchParams({
    language: LANGUAGE,
    word_timestamps: WORD_TIMESTAMPS,
    diarize: DIARIZE,
    age_detection: AGE_DETECTION,
    gender_detection: GENDER_DETECTION,
    emotion_detection: EMOTION_DETECTION,
  });

  const response = await fetch(`${API_URL}?${params}`, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/octet-stream",
    },
    body: audioData,
  });

  if (!response.ok) {
    throw new Error(`API request failed with status ${response.status}: ${await response.text()}`);
  }

  return response.json();
}

// This function is designed to process feature outputs for all the features supported
// by the POST endpoint (Pre-Recorded API)
function processResponse(result, audioPath) {
  if (result.status !== "success") {
    console.error("Error: Transcription failed");
    console.error(result);
    process.exit(1);
  }

  console.log("\n" + "=".repeat(60));
  console.log("TRANSCRIPTION");
  console.log("=".repeat(60));
  console.log(result.transcription || "");

  // Speaker info
  if (result.age || result.gender || result.emotions) {
    console.log("\n" + "-".repeat(60));
    console.log("SPEAKER INFO");
    console.log("-".repeat(60));
    if (result.gender) {
      console.log(`Gender: ${result.gender}`);
    }
    if (result.age) {
      console.log(`Age: ${result.age}`);
    }
    if (result.emotions) {
      console.log("Emotions:");
      const sortedEmotions = Object.entries(result.emotions).sort((a, b) => b[1] - a[1]);
      for (const [emotion, score] of sortedEmotions) {
        console.log(`  ${emotion}: ${score.toFixed(4)}`);
      }
    }
  }

  // Utterances (speaker diarization)
  if (result.utterances && result.utterances.length > 0) {
    console.log("\n" + "-".repeat(60));
    console.log("UTTERANCES");
    console.log("-".repeat(60));
    for (const utt of result.utterances) {
      const speaker = utt.speaker || "unknown";
      const start = utt.start || 0;
      const end = utt.end || 0;
      const text = utt.text || "";
      console.log(`[${start.toFixed(2)}s - ${end.toFixed(2)}s] ${speaker}: ${text}`);
    }
  }

  // Word timestamps
  if (result.words && result.words.length > 0) {
    console.log("\n" + "-".repeat(60));
    console.log("WORD TIMESTAMPS");
    console.log("-".repeat(60));
    for (const word of result.words) {
      const start = word.start || 0;
      const end = word.end || 0;
      const text = word.word || "";
      const speaker = word.speaker || "";
      if (speaker) {
        console.log(`[${start.toFixed(2)}s - ${end.toFixed(2)}s] ${speaker}: ${text}`);
      } else {
        console.log(`[${start.toFixed(2)}s - ${end.toFixed(2)}s] ${text}`);
      }
    }
  }

  console.log("\n" + "=".repeat(60));

  if (!fs.existsSync(OUTPUT_DIR)) {
    fs.mkdirSync(OUTPUT_DIR, { recursive: true });
  }

  const textPath = path.join(OUTPUT_DIR, `${audioPath.name}_transcript.txt`);
  fs.writeFileSync(textPath, result.transcription || "", "utf-8");
  console.log(`Saved: ${textPath}`);

  const jsonPath = path.join(OUTPUT_DIR, `${audioPath.name}_result.json`);
  fs.writeFileSync(jsonPath, JSON.stringify(result, null, 2), "utf-8");
  console.log(`Saved: ${jsonPath}`);

  console.log("\nDone!");
}

async function main() {
  const audioFile = process.argv[2];

  if (!audioFile) {
    console.log("Usage: node transcribe.js <audio_file>");
    process.exit(1);
  }

  const apiKey = process.env.SMALLEST_API_KEY;

  if (!apiKey) {
    console.error("Error: SMALLEST_API_KEY environment variable not set");
    process.exit(1);
  }

  if (!fs.existsSync(audioFile)) {
    console.error(`Error: File not found: ${audioFile}`);
    process.exit(1);
  }

  const audioPath = path.parse(audioFile);
  console.log(`Reading: ${audioPath.base}`);
  console.log(`Transcribing with language: ${LANGUAGE}`);

  const result = await transcribe(audioFile, apiKey);
  processResponse(result, audioPath);
}

main();



================================================
FILE: speech-to-text/file-transcription/python/transcribe.py
================================================
#!/usr/bin/env python3
"""
Smallest AI Speech-to-Text - File Transcription

Transcribe audio files with advanced features like word timestamps,
speaker diarization, and emotion detection.

Usage: python transcribe.py <audio_file>

Output:
- Command line response with feature outputs
- {filename}_transcript.txt - Plain text transcription
- {filename}_result.json - Full API response
"""

import json
import os
import sys
from pathlib import Path

import requests
from dotenv import load_dotenv

load_dotenv()

API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text"
OUTPUT_DIR = "."

# The following are all the features supported by the POST endpoint (Pre-Recorded API)
LANGUAGE = "en"  # Use ISO 639-1 codes or "multi" for auto-detect
WORD_TIMESTAMPS = False
DIARIZE = False
AGE_DETECTION = False
GENDER_DETECTION = False
EMOTION_DETECTION = False


def transcribe(audio_file: str, api_key: str) -> dict:
    with open(audio_file, "rb") as f:
        audio_data = f.read()

    response = requests.post(
        API_URL,
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/octet-stream",
        },
        params={
            "language": LANGUAGE,
            "word_timestamps": str(WORD_TIMESTAMPS).lower(),
            "diarize": str(DIARIZE).lower(),
            "age_detection": str(AGE_DETECTION).lower(),
            "gender_detection": str(GENDER_DETECTION).lower(),
            "emotion_detection": str(EMOTION_DETECTION).lower(),
        },
        data=audio_data,
        timeout=300,
    )

    if response.status_code != 200:
        raise Exception(f"API request failed with status {response.status_code}: {response.text}")

    return response.json()

# This function is designed to process feature outputs for all the features supported
# by the POST endpoint (Pre-Recorded API)
def process_response(result: dict, audio_path: Path):
    if result.get("status") != "success":
        print(f"Error: Transcription failed")
        print(result)
        sys.exit(1)

    print("\n" + "=" * 60)
    print("TRANSCRIPTION")
    print("=" * 60)
    print(result.get("transcription", ""))

    # Speaker info
    if result.get("age") or result.get("gender") or result.get("emotions"):
        print("\n" + "-" * 60)
        print("SPEAKER INFO")
        print("-" * 60)
        if result.get("gender"):
            print(f"Gender: {result['gender']}")
        if result.get("age"):
            print(f"Age: {result['age']}")
        if result.get("emotions"):
            print("Emotions:")
            emotions = result["emotions"]
            sorted_emotions = sorted(emotions.items(), key=lambda x: x[1], reverse=True)
            for emotion, score in sorted_emotions:
                print(f"  {emotion}: {score:.4f}")

    # Utterances (speaker diarization)
    if result.get("utterances"):
        print("\n" + "-" * 60)
        print("UTTERANCES")
        print("-" * 60)
        for utt in result["utterances"]:
            speaker = utt.get("speaker", "")
            start = utt.get("start", 0)
            end = utt.get("end", 0)
            text = utt.get("text", "")
            if speaker:
                print(f"[{start:.2f}s - {end:.2f}s] {speaker}: {text}")
            else:
                print(f"[{start:.2f}s - {end:.2f}s] {text}")

    # Word timestamps
    if result.get("words"):
        print("\n" + "-" * 60)
        print("WORD TIMESTAMPS")
        print("-" * 60)
        for word in result["words"]:
            start = word.get("start", 0)
            end = word.get("end", 0)
            text = word.get("word", "")
            speaker = word.get("speaker", "")
            if speaker:
                print(f"[{start:.2f}s - {end:.2f}s] {speaker}: {text}")
            else:
                print(f"[{start:.2f}s - {end:.2f}s] {text}")

    print("\n" + "=" * 60)

    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)

    text_path = output_dir / f"{audio_path.stem}_transcript.txt"
    with open(text_path, "w", encoding="utf-8") as f:
        f.write(result.get("transcription", ""))
    print(f"Saved: {text_path}")

    json_path = output_dir / f"{audio_path.stem}_result.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"Saved: {json_path}")

    print("\nDone!")


def main():
    if len(sys.argv) < 2:
        print("Usage: python transcribe.py <audio_file>")
        sys.exit(1)

    audio_file = sys.argv[1]
    api_key = os.environ.get("SMALLEST_API_KEY")

    if not api_key:
        print("Error: SMALLEST_API_KEY environment variable not set")
        sys.exit(1)

    audio_path = Path(audio_file)
    if not audio_path.exists():
        print(f"Error: File not found: {audio_file}")
        sys.exit(1)

    print(f"Reading: {audio_path.name}")
    print(f"Transcribing with language: {LANGUAGE}")

    result = transcribe(audio_file, api_key)
    process_response(result, audio_path)


if __name__ == "__main__":
    main()



================================================
FILE: speech-to-text/getting-started/README.md
================================================
# Getting Started

The simplest way to transcribe audio using Smallest AI's Pulse STT API. This is the "hello world" of speech-to-text.

## Features

- Make a basic transcription request
- Handle the API response
- Print the transcription result
- Language selection with auto-detect support

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../README.md#usage) for setup. Add `SMALLEST_API_KEY` to your `.env`.

## Usage

### Python

```bash
uv run python/transcribe.py recording.wav
```

### JavaScript

```bash
node javascript/transcribe.js recording.wav
```

## Recommended Usage

- The simplest possible transcription from an audio file â€” start here
- Quick validation that your API key and setup are working
- For advanced features (timestamps, diarization, emotions), [File Transcription](../file-transcription/) is recommended

## Configuration

| Parameter | Description | Default |
|-----------|-------------|---------|
| `LANGUAGE` | Language code (ISO 639-1) or `multi` for auto-detect | `en` |

## API Reference

- [Pre-recorded Quickstart](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/pre-recorded/quickstart)
- [Pulse STT API Reference](https://waves-docs.smallest.ai/v4.0.0/content/api-references/pulse-asr)

## Next Steps

- [Word-Level Outputs](../word-level-outputs/) â€” Add word timestamps and speaker diarization
- [File Transcription](../file-transcription/) â€” Enable emotions, age, gender, PII redaction
- [Streaming Transcription](../websocket/streaming-text-output-transcription/) â€” Stream audio via WebSocket



================================================
FILE: speech-to-text/getting-started/.env.sample
================================================
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here



================================================
FILE: speech-to-text/getting-started/javascript/transcribe.js
================================================
#!/usr/bin/env node
/**
 * Smallest AI Speech-to-Text - Getting Started
 *
 * The simplest way to transcribe audio using Smallest AI's Pulse STT API.
 *
 * Usage: node transcribe.js <audio_file>
 *
 * Output:
 * - Command line response with transcription
 */

const fs = require("fs");

const API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text";

// Features
const LANGUAGE = "en"; // Use ISO 639-1 codes or "multi" for auto-detect

async function transcribe(audioFile, apiKey) {
  const audioData = fs.readFileSync(audioFile);

  const params = new URLSearchParams({
    language: LANGUAGE,
  });

  const response = await fetch(`${API_URL}?${params}`, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/octet-stream",
    },
    body: audioData,
  });

  if (!response.ok) {
    throw new Error(`API request failed with status ${response.status}: ${await response.text()}`);
  }

  return response.json();
}

function processResponse(result) {
  if (result.status !== "success") {
    console.error("Error: Transcription failed");
    console.error(result);
    process.exit(1);
  }

  console.log("");
  console.log("=".repeat(60));
  console.log("TRANSCRIPTION");
  console.log("=".repeat(60));
  console.log(result.transcription || "");
  console.log("=".repeat(60));
  console.log("");
}

async function main() {
  const audioFile = process.argv[2];

  if (!audioFile) {
    console.log("Usage: node transcribe.js <audio_file>");
    process.exit(1);
  }

  const apiKey = process.env.SMALLEST_API_KEY;

  if (!apiKey) {
    console.error("Error: SMALLEST_API_KEY environment variable not set");
    process.exit(1);
  }

  if (!fs.existsSync(audioFile)) {
    console.error(`Error: File not found: ${audioFile}`);
    process.exit(1);
  }

  const result = await transcribe(audioFile, apiKey);
  processResponse(result);
}

main();



================================================
FILE: speech-to-text/getting-started/python/transcribe.py
================================================
#!/usr/bin/env python3
"""
Smallest AI Speech-to-Text - Getting Started

The simplest way to transcribe audio using Smallest AI's Pulse STT API.

Usage: python transcribe.py <audio_file>

Output:
- Command line response with transcription
"""

import os
import sys
import requests
from dotenv import load_dotenv

load_dotenv()

API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text"

# Features
LANGUAGE = "en"  # Use ISO 639-1 codes or "multi" for auto-detect


def transcribe(audio_file: str, api_key: str) -> dict:
    with open(audio_file, "rb") as f:
        audio_data = f.read()

    response = requests.post(
        API_URL,
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/octet-stream",
        },
        params={
            "language": LANGUAGE,
        },
        data=audio_data,
    )

    if response.status_code != 200:
        raise Exception(f"API request failed with status {response.status_code}: {response.text}")

    return response.json()


def process_response(result: dict):
    if result.get("status") != "success":
        print(f"Error: Transcription failed")
        print(result)
        sys.exit(1)

    print("\n" + "=" * 60)
    print("TRANSCRIPTION")
    print("=" * 60)
    print(result.get("transcription", ""))
    print("=" * 60 + "\n")


def main():
    if len(sys.argv) < 2:
        print("Usage: python transcribe.py <audio_file>")
        sys.exit(1)

    audio_file = sys.argv[1]
    api_key = os.environ.get("SMALLEST_API_KEY")

    if not api_key:
        print("Error: SMALLEST_API_KEY environment variable not set")
        sys.exit(1)

    if not os.path.exists(audio_file):
        print(f"Error: File not found: {audio_file}")
        sys.exit(1)

    result = transcribe(audio_file, api_key)
    process_response(result)


if __name__ == "__main__":
    main()



================================================
FILE: speech-to-text/online-meeting-notetaking-bot/README.md
================================================
# Meeting Notes Generator

Automatically join **Google Meet, Zoom, or Microsoft Teams**, transcribe with Pulse STT, and generate structured meeting notes with intelligent speaker identification.

## Features

- Auto-join Google Meet, Zoom, or Microsoft Teams via Recall.ai
- Transcribe with speaker diarization via Pulse STT
- Intelligent name mapping â€” uses real names instead of "Speaker 1"
- Structured meeting notes with action items and quotes via GPT-4o
- Supports any meeting platform that Recall.ai supports

## Demo


![Admit Bot](admit-bot.png)

<details>
<summary><strong>Sample Run</strong> (click to expand)</summary>

```
$ python bot.py "https://meet.google.com/ivm-ojjk-htz" "Smallest Notetaker"

Creating bot to join: https://meet.google.com/ivm-ojjk-htz
Bot created: 4cf29b90-ddc8-4a29-957a-50f6e62ad7d3
Bot will appear as: Smallest Notetaker
--------------------------------------------------
Waiting for meeting to end...
Status: pending
Bot is in waiting room. Please admit the bot.
Bot is in waiting room. Please admit the bot.
Recording...
Recording...
Meeting ended. Processing recordings...

Fetching audio recording...
Waiting for audio to be processed...
Audio ready!
Downloading audio...
Saved: ivmojjkhtz_audio.mp3
Transcribing with Pulse STT...
Transcription complete
Saved: ivmojjkhtz_transcript.txt
Generating meeting notes with GPT-4o...
Saved: ivmojjkhtz_notes.md

==================================================
MEETING NOTES
==================================================
# Meeting Notes

**Date:** January 23, 2026

## Attendees
- Aditya, smallest.ai

## Summary
Aditya from smallest.ai conducted a demonstration of a meeting bot
designed to transcribe and summarize meetings...

## Action Items
| Owner  | Task                           | Deadline |
|--------|--------------------------------|----------|
| Aditya | Finalize the meeting transcription | N/A  |

## Notable Quotes
- "This is a demo of testing the meeting bot..." - Aditya

Done!
```

</details>

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../README.md#usage) for setup. Add `SMALLEST_API_KEY`, `OPENAI_API_KEY`, and `RECALL_API_KEY` to your `.env`.

## Usage

```bash
# Google Meet
uv run bot.py "https://meet.google.com/abc-defg-hij" "Notes Bot"

# Zoom
uv run bot.py "https://zoom.us/j/1234567890" "Notes Bot"

# Microsoft Teams
uv run bot.py "https://teams.microsoft.com/l/meetup-join/..." "Notes Bot"
```

**Arguments:**
- `<meet_url>` â€” Meeting URL (Google Meet, Zoom, or Microsoft Teams)
- `[bot_name]` â€” Name the bot appears as (default: "Notes Bot")

## Recommended Usage

- Automated meeting transcription with structured notes and action items
- Live meetings on Google Meet, Zoom, or Microsoft Teams with speaker identification
- Keeping track of to-do lists and updates if you cannot join in

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Recall.ai  â”‚ â†’  â”‚  Download  â”‚ â†’  â”‚ Pulse STT  â”‚ â†’  â”‚   GPT-4o   â”‚
â”‚ Join Meet  â”‚    â”‚   Audio    â”‚    â”‚ Transcribe â”‚    â”‚ Summarize  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Recall.ai** creates a bot that joins your meeting (Google Meet, Zoom, or Teams)
2. **Admit the bot** when prompted in the meeting
3. **Bot records** the entire meeting audio
4. **Meeting ends** â†’ Audio is downloaded
5. **Pulse STT** transcribes with speaker diarization
6. **GPT-4o** generates structured meeting notes with name mapping

### Intelligent Name Mapping

The bot automatically identifies speaker names from the conversation using GPT-4o:

| Transcript | Notes Output |
|------------|--------------|
| `Speaker 1: Hey everyone, this is Alex from engineering` | Uses **"Alex"** throughout the notes |
| `Speaker 2: Thanks Sarah, I'll handle that` | Maps Speaker 2 â†’ **"Sarah"** |
| `Speaker 3: [name never mentioned]` | Falls back to **"speaker_3"** |

This means your action items, quotes, and summaries use **real names** instead of generic "Speaker 1" labels.

## Example Output

| File | Description |
|------|-------------|
| `{id}_audio.mp3` | Meeting audio recording |
| `{id}_transcript.txt` | Full transcript with speaker labels |
| `{id}_notes.md` | Structured meeting notes with real names |

### Example Notes

```markdown
# Meeting Notes

**Date:** January 21, 2026

## Attendees
- Alex (Engineering)
- Sarah (Product)
- speaker_3 (unidentified)

## Summary
Alex presented the Q1 roadmap. Sarah raised concerns about timeline...

## Action Items
| Owner | Task | Deadline |
|-------|------|----------|
| Alex | Finalize API spec | Jan 28 |
| Sarah | Review designs | Jan 25 |
| speaker_3 | Send follow-up email | TBD |

## Notable Quotes
> "We need to ship this by end of month" â€” Alex
```

## API Reference

- [Recall.ai API Docs](https://docs.recall.ai)
- [Pulse STT API](https://waves-docs.smallest.ai/v4.0.0/content/api-references/pulse-asr)
- [OpenAI Chat API](https://platform.openai.com/docs/api-reference/chat)

## Next Steps

- [Podcast Summarizer](../podcast-summarizer/) â€” Similar transcribe-and-summarize pattern for podcast files
- [YouTube Summarizer](../youtube-summarizer/) â€” Transcribe and summarize YouTube videos



================================================
FILE: speech-to-text/online-meeting-notetaking-bot/bot.py
================================================
#!/usr/bin/env python3
"""
Meeting Note-Taking Bot

Joins Google Meet, Zoom, or Microsoft Teams meetings via Recall.ai,
transcribes with Smallest AI Pulse STT, and generates notes with GPT-4o.

Usage:
    python bot.py <meet_url> [bot_name]

Examples:
    python bot.py "https://meet.google.com/abc-defg-hij" "Notes Bot"
    python bot.py "https://zoom.us/j/1234567890" "Notes Bot"
    python bot.py "https://teams.microsoft.com/l/meetup-join/..." "Notes Bot"

Environment:
    RECALL_API_KEY    - Your Recall.ai API key
    SMALLEST_API_KEY  - Your Smallest AI API key
    OPENAI_API_KEY    - Your OpenAI API key

Output:
    - {meeting_id}_audio.mp3       : Meeting audio
    - {meeting_id}_transcript.txt  : Full transcript with speaker labels
    - {meeting_id}_notes.md        : Structured meeting notes
"""

import os
import sys
import time
import requests
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

RECALL_API_URL = "https://us-west-2.recall.ai/api/v1"
RECALL_API_KEY = os.environ.get("RECALL_API_KEY")

PULSE_API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text"
SMALLEST_API_KEY = os.environ.get("SMALLEST_API_KEY")

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

LANGUAGE = "en"
DIARIZE = True

# Intelligent name mapping requested in the prompt
MEETING_NOTES_PROMPT = """Analyze this meeting transcript and create structured meeting notes.

TRANSCRIPT:
{transcript}

---

IMPORTANT - Speaker Name Resolution:
The transcript uses generic labels like "Speaker 1", "Speaker 2", etc. Your task:
1. Look for moments where people introduce themselves or are addressed by name (e.g., "Hi John", "Thanks Sarah", "This is Mike speaking")
2. Create a mapping of speaker labels to real names when identifiable
3. Use the REAL NAMES throughout the notes (in summary, action items, quotes, etc.)
4. If a speaker's name cannot be determined, use "speaker_x" format (e.g., speaker_1, speaker_2)

Example: If Speaker 1 says "Hey everyone, this is Alex" â†’ use "Alex" instead of "Speaker 1" everywhere
Example: If Speaker 3's name is never mentioned â†’ use "speaker_3" as fallback

---

Generate notes in this Markdown format:

# Meeting Notes

**Date:** {date}

## Attendees
List the identified participants with their roles if mentioned.

## Summary
A 2-3 paragraph overview of the meeting's purpose and outcomes. Use real names.

## Key Discussion Points
- Point 1
- Point 2
- Point 3

## Decisions Made
- Decision 1 (include who proposed/approved if known)
- Decision 2

## Action Items
| Owner | Task | Deadline |
|-------|------|----------|
| [Real Name] | Task description | Date if mentioned |

## Notable Quotes
Include 1-2 impactful quotes with attribution to real names.

## Follow-up Topics
- Topics for the next meeting

Be concise. Focus on actionable information. Remove filler and banter. Always prefer real names over speaker labels."""


def create_bot(meeting_url: str, bot_name: str) -> dict:
    """
    Create a Recall.ai bot that joins a meeting and records audio.
    
    Args:
        meeting_url: The meeting URL (Google Meet, Zoom, Teams, etc.)
        bot_name: Display name for the bot in the meeting
    
    Returns:
        Bot details including the bot ID
    
    Raises:
        Exception: If the API request fails
    """

    # reference: https://docs.recall.ai/reference/bot_create
    response = requests.post(
        f"{RECALL_API_URL}/bot",
        headers={
            "Authorization": f"Token {RECALL_API_KEY}",
            "Content-Type": "application/json",
        },
        json={
            "meeting_url": meeting_url,
            "bot_name": bot_name,
            "recording_config": {
                "audio_mixed_mp3": {},
            },
        },
    )
    
    if response.status_code not in [200, 201]:
        raise Exception(f"Failed to create bot: {response.status_code} - {response.text}")
    
    return response.json()


def get_bot_status(bot_id: str) -> dict:
    """
    Retrieve the current status and details of a bot.
    
    Args:
        bot_id: The unique identifier of the bot
    
    Returns:
        Bot status including recordings, participants, and status_changes
    """

    # reference: https://docs.recall.ai/reference/bot_retrieve
    response = requests.get(
        f"{RECALL_API_URL}/bot/{bot_id}",
        headers={"Authorization": f"Token {RECALL_API_KEY}"},
    )
    
    if response.status_code != 200:
        raise Exception(f"Failed to get bot status: {response.text}")
    
    return response.json()


def wait_for_meeting_end(bot_id: str, poll_interval: int = 10) -> dict:
    """
    Poll the bot status until the meeting ends.
    
    Prints status updates to console while waiting. The bot must be admitted
    to the meeting by a participant for recording to start.
    
    Args:
        bot_id: The unique identifier of the bot
        poll_interval: Seconds between status checks (default: 10)
    
    Returns:
        Final bot status after meeting ends
    
    Raises:
        Exception: If the bot encounters a fatal error
    """
    print("Waiting for meeting to end...")
    
    while True:
        status = get_bot_status(bot_id)
        status_changes = status.get("status_changes", [])
        
        if status_changes:
            latest = status_changes[-1]
            state = latest.get("code", "unknown")
        else:
            state = "pending"
        
        if state == "done":
            print("Meeting ended. Processing recordings...")
            return status
        elif state == "fatal":
            message = status_changes[-1].get("message", "Unknown error") if status_changes else "Unknown"
            raise Exception(f"Bot encountered an error: {message}")
        elif state == "joining_call":
            print("Bot is joining the call... (admit the bot inside the meeting)")
        elif state == "in_waiting_room":
            print("Bot is in waiting room. Please admit the bot.")
        elif state == "in_call_not_recording":
            print("Bot joined. Waiting for recording to start...")
        elif state == "in_call_recording":
            print(f"Recording...")
        elif state == "call_ended":
            print("Call ended. Processing...")
            return status
        else:
            print(f"Status: {state}")
        
        time.sleep(poll_interval)


def get_audio_url(bot_id: str, max_retries: int = 60, retry_delay: int = 5) -> str:
    """
    Wait for audio processing and retrieve the download URL.
    
    After a meeting ends, Recall.ai processes the audio. This function
    polls until the audio is ready or timeout is reached.
    
    Args:
        bot_id: The unique identifier of the bot
        max_retries: Maximum number of retry attempts (default: 60)
        retry_delay: Seconds between retries (default: 5)
    
    Returns:
        Download URL for the MP3 audio file, or None if unavailable
    """
    print("Waiting for audio to be processed...")
    
    for attempt in range(max_retries):
        status = get_bot_status(bot_id)
        recordings = status.get("recordings", [])
        
        if recordings:
            recording = recordings[0]
            media_shortcuts = recording.get("media_shortcuts", {})
            audio_mixed = media_shortcuts.get("audio_mixed")
            
            if audio_mixed:
                audio_status = audio_mixed.get("status", {}).get("code", "unknown")
                download_url = audio_mixed.get("data", {}).get("download_url")
                
                if download_url:
                    print("Audio ready!")
                    return download_url
                else:
                    print(f"  Attempt {attempt + 1}/{max_retries}: Audio status: {audio_status}")
            else:
                rec_status = recording.get("status", {}).get("code", "unknown")
                print(f"  Attempt {attempt + 1}/{max_retries}: Recording status: {rec_status}, audio_mixed not available yet")
        else:
            print(f"  Attempt {attempt + 1}/{max_retries}: No recordings yet...")
        
        time.sleep(retry_delay)
    
    print("Timeout: Audio not available after maximum retries")
    return None


# Pulse API provides support for GET request URLs (eg. https://example.mp3)
# The URL given by recall cannot be used directly with Pulse API since it is not of the above format
# Hence the need to download the audio first
def download_audio(url: str, output_path: str) -> str:
    """
    Download audio file from a URL to local disk.
    
    Args:
        url: The download URL for the audio file
        output_path: Local file path to save the audio
    
    Returns:
        The output_path where the file was saved
    
    Raises:
        Exception: If the download fails
    """
    print(f"Downloading audio...")
    
    response = requests.get(url, stream=True)
    if response.status_code != 200:
        raise Exception(f"Failed to download audio: {response.status_code}")
    
    with open(output_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    print(f"Saved: {output_path}")
    return output_path


def transcribe_with_pulse(audio_path: str) -> dict:
    """
    Transcribe audio using Smallest AI Pulse STT with speaker diarization.
    
    Args:
        audio_path: Path to the audio file to transcribe
    
    Returns:
        Transcription result with utterances and speaker labels
    
    Raises:
        Exception: If transcription fails
    """
    print("Transcribing with Pulse STT...")
    
    with open(audio_path, "rb") as f:
        audio_data = f.read()
    
    response = requests.post(
        PULSE_API_URL,
        headers={
            "Authorization": f"Bearer {SMALLEST_API_KEY}",
            "Content-Type": "application/octet-stream",
        },
        params={
            "language": LANGUAGE,
            "diarize": str(DIARIZE).lower(),
        },
        data=audio_data,
        timeout=600,
    )
    
    if response.status_code != 200:
        raise Exception(f"Transcription failed: {response.status_code} - {response.text}")
    
    result = response.json()
    
    if result.get("status") != "success":
        raise Exception(f"Transcription failed: {result}")
    
    print("Transcription complete")
    return result


def format_transcript(result: dict) -> str:
    """
    Format the transcription result into readable text with speaker labels.
    
    Args:
        result: Pulse STT response containing utterances or transcription
    
    Returns:
        Formatted transcript string with "Speaker X: text" format
    """
    lines = []
    
    if result.get("utterances"):
        for utt in result["utterances"]:
            speaker = utt.get("speaker", "Unknown")
            text = utt.get("text", "")
            if text.strip():
                lines.append(f"Speaker {speaker}: {text}")
    else:
        lines.append(result.get("transcription", ""))
    
    return "\n\n".join(lines)


def generate_notes(transcript: str) -> str:
    """
    Generate structured meeting notes from transcript using GPT-4o.
    
    The LLM intelligently maps speaker labels to real names when identifiable
    from the conversation (e.g., introductions, addressing by name).
    
    Args:
        transcript: Formatted transcript with speaker labels
    
    Returns:
        Markdown-formatted meeting notes, or None if generation fails
    """
    if not OPENAI_API_KEY:
        print("Warning: OPENAI_API_KEY not set, skipping notes generation")
        return None
    
    print("Generating meeting notes with GPT-4o...")
    
    try:
        from openai import OpenAI
    except ImportError:
        print("Error: openai package not installed. Run: pip install openai")
        return None
    
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    prompt = MEETING_NOTES_PROMPT.format(
        transcript=transcript,
        date=datetime.now().strftime("%B %d, %Y")
    )
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a professional meeting note-taker."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=4000,
    )
    
    return response.choices[0].message.content


def main():
    """
    Main entry point. Orchestrates the full pipeline:
    1. Create bot and join meeting
    2. Wait for meeting to end
    3. Download audio from Recall.ai
    4. Transcribe with Pulse STT
    5. Generate notes with GPT-4o
    """
    if len(sys.argv) < 2:
        print("Usage: python bot.py <meet_url> [bot_name]")
        print('Example: python bot.py "https://meet.google.com/abc-defg-hij" "Notes Bot"')
        sys.exit(1)
    
    if not RECALL_API_KEY:
        print("Error: RECALL_API_KEY environment variable not set")
        print("Get your API key at: https://recall.ai")
        sys.exit(1)
    
    if not SMALLEST_API_KEY:
        print("Error: SMALLEST_API_KEY environment variable not set")
        print("Get your API key at: https://smallest.ai/console")
        sys.exit(1)
    
    meeting_url = sys.argv[1]
    bot_name = sys.argv[2] if len(sys.argv) > 2 else "Notes Bot"
    meeting_id = meeting_url.split("/")[-1].replace("-", "")[:10]
    
    try:
        print(f"Creating bot to join: {meeting_url}")
        bot = create_bot(meeting_url, bot_name)
        bot_id = bot["id"]
        print(f"Bot created: {bot_id}")
        print(f"Bot will appear as: {bot_name}")
        print("-" * 50)
        
        wait_for_meeting_end(bot_id)
        
        print("\nFetching audio recording...")
        audio_url = get_audio_url(bot_id)
        
        if not audio_url:
            print("Error: No audio recording available")
            print("The meeting may have been too short or recording failed.")
            sys.exit(1)
        
        audio_path = f"{meeting_id}_audio.mp3"
        download_audio(audio_url, audio_path)
        
        result = transcribe_with_pulse(audio_path)
        transcript = format_transcript(result)
        
        if not transcript:
            print("Error: No transcript generated")
            sys.exit(1)
        
        transcript_file = f"{meeting_id}_transcript.txt"
        with open(transcript_file, "w") as f:
            f.write(transcript)
        print(f"Saved: {transcript_file}")
        
        notes = generate_notes(transcript)
        
        if notes:
            notes_file = f"{meeting_id}_notes.md"
            with open(notes_file, "w") as f:
                f.write(notes)
            print(f"Saved: {notes_file}")
            
            print("\n" + "=" * 50)
            print("MEETING NOTES")
            print("=" * 50)
            print(notes)
        
        print("\nDone!")
        
    except KeyboardInterrupt:
        print("\n\nInterrupted. Bot may still be in the meeting.")
        print("Check status at: https://recall.ai/dashboard")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()



================================================
FILE: speech-to-text/online-meeting-notetaking-bot/.env.sample
================================================
# Recall.ai API Key (for meeting bot)
# Get yours at https://recall.ai
RECALL_API_KEY=your-recall-api-key-here

# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here

# OpenAI API Key (for GPT-4o notes generation)
# Get yours at https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here



================================================
FILE: speech-to-text/podcast-summarizer/README.md
================================================
# Podcast Summarizer

Transcribe podcasts and generate concise summaries using OpenAI GPT-5. Extracts key points and removes unnecessary banter. The API handles both audio and video files directly â€” no preprocessing required.

## Features

- Transcribe podcasts via Pulse STT
- Generate structured Markdown summaries with GPT-5
- Extracts key points, topics, and notable quotes
- Direct audio and video file support (no ffmpeg needed)
- Customizable summarization prompt

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../README.md#usage) for setup. Add `SMALLEST_API_KEY` and `OPENAI_API_KEY` to your `.env`.

## Usage

```bash
uv run summarize.py podcast.mp3
uv run summarize.py video_podcast.mp4
```

## Recommended Usage

- Quickly distilling long podcasts or audio recordings into structured summaries
- Extracting key points, topics, and notable quotes from conversations
- For a web UI with YouTube support, [YouTube Summarizer](../youtube-summarizer/) is recommended

## How It Works

1. **Transcription**: Uses Smallest AI Pulse STT to transcribe the audio/video
2. **Summarization**: Sends transcript to OpenAI GPT-5 with a specialized prompt
3. **Output**: Generates a structured Markdown summary

## Example Output

- `{filename}_transcript.txt` â€” Full transcript
- `{filename}_summary.md` â€” Structured summary with key points

### Summary Format

The generated summary includes:

- **Summary**: 2-3 sentence overview
- **Key Points**: Bullet points of main takeaways
- **Topics Discussed**: Brief description of each major topic
- **Notable Quotes**: Impactful quotes from the conversation

## Supported Formats

Audio: WAV, MP3, FLAC, OGG, M4A, AAC, WMA

Video: MP4, MKV, AVI, MOV, WebM, FLV, WMV, M4V

## Customization

Edit the `SUMMARIZE_PROMPT` variable in the script to customize how summaries are generated.

## API Reference

- [Pulse STT API Reference](https://waves-docs.smallest.ai/v4.0.0/content/api-references/pulse-asr)
- [OpenAI Chat API](https://platform.openai.com/docs/api-reference/chat)

## Next Steps

- [YouTube Summarizer](../youtube-summarizer/) â€” Streamlit app for YouTube videos
- [File Transcription](../file-transcription/) â€” Add emotions, age, gender to transcriptions



================================================
FILE: speech-to-text/podcast-summarizer/summarize.py
================================================
#!/usr/bin/env python3
"""
Smallest AI Speech-to-Text - Podcast Summarizer

Transcribe a podcast (audio or video) and generate a concise summary
using OpenAI GPT-5. Extracts key points and removes unnecessary banter.

Usage: python summarize.py <audio_or_video_file>

Environment Variables:
- SMALLEST_API_KEY: Smallest AI API key
- OPENAI_API_KEY: OpenAI API key

Requirements:
- openai Python package

Output:
- Console output with summary
- {filename}_summary.md - Markdown summary file
- {filename}_transcript.txt - Full transcript
"""

import os
import sys
from pathlib import Path

import requests
from dotenv import load_dotenv

load_dotenv()

API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text"

LANGUAGE = "en"  # Use ISO 639-1 codes or "multi" for auto-detect

SUMMARIZE_PROMPT = """You are an expert podcast summarizer. Analyze the following podcast transcript and create a concise, well-structured summary.

Your task:
1. Extract the KEY POINTS and main topics discussed
2. Remove all filler words, tangents, and unnecessary banter
3. Keep only the valuable, actionable insights
4. Organize the summary with clear sections
5. Include any notable quotes if they add value

Format your response as:

## Summary
A 2-3 sentence overview of what the podcast is about.

## Key Points
- Bullet points of the main takeaways
- Focus on actionable insights
- Remove redundant information

## Topics Discussed
Brief description of each major topic covered.

## Notable Quotes (if any)
Include 1-2 impactful quotes from the conversation.

---

TRANSCRIPT:
{transcript}
"""


def transcribe(input_file: str, api_key: str) -> str:
    print("Transcribing...")

    with open(input_file, "rb") as f:
        file_data = f.read()

    response = requests.post(
        API_URL,
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/octet-stream",
        },
        params={
            "language": LANGUAGE,
        },
        data=file_data,
        timeout=600,
    )

    if response.status_code != 200:
        raise Exception(f"API request failed with status {response.status_code}: {response.text}")

    result = response.json()

    if result.get("status") != "success":
        raise Exception(f"Transcription failed: {result}")

    transcript = result.get("transcription", "")
    print(f"Transcription complete: {len(transcript)} characters")
    return transcript


def summarize_with_openai(transcript: str, openai_api_key: str) -> str:
    print("Generating summary with GPT-5...")

    try:
        from openai import OpenAI
    except ImportError:
        print("Error: openai package not installed. Run: pip install openai")
        sys.exit(1)

    client = OpenAI(api_key=openai_api_key)

    response = client.chat.completions.create(
        model="gpt-5",
        messages=[
            {
                "role": "system",
                "content": "You are an expert podcast summarizer who extracts key insights and removes unnecessary content."
            },
            {
                "role": "user",
                "content": SUMMARIZE_PROMPT.format(transcript=transcript)
            }
        ],
        temperature=0.3,
        max_tokens=2000,
    )

    summary = response.choices[0].message.content
    print("Summary generated successfully")
    return summary


def main():
    if len(sys.argv) < 2:
        print("Usage: python summarize.py <audio_or_video_file>")
        sys.exit(1)

    input_file = sys.argv[1]
    smallest_api_key = os.environ.get("SMALLEST_API_KEY")
    openai_api_key = os.environ.get("OPENAI_API_KEY")

    if not smallest_api_key:
        print("Error: SMALLEST_API_KEY environment variable not set")
        sys.exit(1)

    if not openai_api_key:
        print("Error: OPENAI_API_KEY environment variable not set")
        sys.exit(1)

    input_path = Path(input_file)
    if not input_path.exists():
        print(f"Error: File not found: {input_file}")
        sys.exit(1)

    print(f"Processing: {input_path.name}")
    print("=" * 60)

    # Step 1: Transcribe
    transcript = transcribe(input_file, smallest_api_key)

    # Save transcript
    transcript_path = input_path.with_suffix(".txt")
    transcript_path = transcript_path.with_stem(f"{input_path.stem}_transcript")
    with open(transcript_path, "w", encoding="utf-8") as f:
        f.write(transcript)
    print(f"Saved transcript: {transcript_path}")

    # Step 2: Summarize
    summary = summarize_with_openai(transcript, openai_api_key)

    # Save summary
    summary_path = input_path.with_suffix(".md")
    summary_path = summary_path.with_stem(f"{input_path.stem}_summary")
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(f"# Podcast Summary: {input_path.stem}\n\n")
        f.write(summary)
    print(f"Saved summary: {summary_path}")

    # Print summary
    print("\n" + "=" * 60)
    print("PODCAST SUMMARY")
    print("=" * 60)
    print(summary)
    print("=" * 60)
    print("\nDone!")


if __name__ == "__main__":
    main()



================================================
FILE: speech-to-text/podcast-summarizer/.env.sample
================================================
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here

# OpenAI API Key (for GPT-4o summarization)
# Get yours at https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key-here



================================================
FILE: speech-to-text/subtitle-generation/README.md
================================================
# Subtitle Generation

Generate SRT and VTT subtitle files from audio or video files. The API handles both audio and video files directly â€” no preprocessing required.

## Features

- Generate SubRip (.srt) and WebVTT (.vtt) subtitle formats
- Configurable words per segment and max segment duration
- Direct audio and video file support (no ffmpeg needed)
- Wide format support: WAV, MP3, FLAC, MP4, MKV, MOV, and more

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../README.md#usage) for setup. Add `SMALLEST_API_KEY` to your `.env`.

## Usage

### Python

```bash
uv run python/transcribe.py video.mp4
```

### JavaScript

```bash
node javascript/transcribe.js video.mp4
```

## Recommended Usage

- Generating SRT/VTT subtitle files for video players, YouTube uploads, or accessibility
- Batch subtitling of audio and video files with configurable segment length
- For raw word timestamps without subtitle formatting, [Word-Level Outputs](../word-level-outputs/) is recommended

## How It Works

The script calls Pulse STT with word timestamps enabled, then groups words into timed subtitle segments based on configurable limits.

### SRT Format

```
1
00:00:00,000 --> 00:00:02,500
Hello, this is a sample subtitle.

2
00:00:02,500 --> 00:00:05,000
This is the second line.
```

### VTT Format

```
WEBVTT

1
00:00:00.000 --> 00:00:02.500
Hello, this is a sample subtitle.

2
00:00:02.500 --> 00:00:05.000
This is the second line.
```

## Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `WORDS_PER_SEGMENT` | 10 | Maximum words per subtitle line |
| `MAX_SEGMENT_DURATION` | 5.0 | Maximum seconds per subtitle |

## Example Output

- `{filename}.srt` â€” SubRip subtitle format (most compatible)
- `{filename}.vtt` â€” WebVTT format (web standard)

## Supported Formats

Audio: WAV, MP3, FLAC, OGG, M4A, AAC, WMA

Video: MP4, MKV, AVI, MOV, WebM, FLV, WMV, M4V

## API Reference

- [Pre-recorded Quickstart](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/pre-recorded/quickstart)
- [Pulse STT API Reference](https://waves-docs.smallest.ai/v4.0.0/content/api-references/pulse-asr)

## Next Steps

- [Word-Level Outputs](../word-level-outputs/) â€” Access raw word timestamps for custom formatting
- [File Transcription](../file-transcription/) â€” Add emotions, age, gender to transcriptions



================================================
FILE: speech-to-text/subtitle-generation/.env.sample
================================================
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here



================================================
FILE: speech-to-text/subtitle-generation/javascript/transcribe.js
================================================
#!/usr/bin/env node
/**
 * Smallest AI Speech-to-Text - Subtitle Generation
 *
 * Generate SRT and VTT subtitles from audio or video files.
 * Uses word timestamps to create properly timed subtitle segments.
 *
 * Usage: node transcribe.js <audio_or_video_file>
 *
 * Output:
 * - {filename}.srt - SubRip subtitle file
 * - {filename}.vtt - WebVTT subtitle file
 */

const fs = require("fs");
const path = require("path");

const API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text";

const LANGUAGE = "en"; // Use ISO 639-1 codes or "multi" for auto-detect
const WORDS_PER_SEGMENT = 10; // Maximum words per subtitle segment
const MAX_SEGMENT_DURATION = 5.0; // Maximum duration per segment in seconds

async function transcribe(inputFile, apiKey) {
  const fileData = fs.readFileSync(inputFile);

  const params = new URLSearchParams({
    language: LANGUAGE,
    word_timestamps: "true",
  });

  const response = await fetch(`${API_URL}?${params}`, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/octet-stream",
    },
    body: fileData,
  });

  if (!response.ok) {
    throw new Error(`API request failed with status ${response.status}: ${await response.text()}`);
  }

  return response.json();
}

function formatTimeSrt(seconds) {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const millis = Math.floor((seconds % 1) * 1000);
  return `${hours.toString().padStart(2, "0")}:${minutes.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")},${millis.toString().padStart(3, "0")}`;
}

function formatTimeVtt(seconds) {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  const millis = Math.floor((seconds % 1) * 1000);
  return `${hours.toString().padStart(2, "0")}:${minutes.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}.${millis.toString().padStart(3, "0")}`;
}

function createSegments(words) {
  const segments = [];
  let currentSegment = [];
  let segmentStart = null;

  for (const word of words) {
    if (segmentStart === null) {
      segmentStart = word.start || 0;
    }

    currentSegment.push(word);
    const segmentEnd = word.end || 0;
    const segmentDuration = segmentEnd - segmentStart;

    if (currentSegment.length >= WORDS_PER_SEGMENT || segmentDuration >= MAX_SEGMENT_DURATION) {
      segments.push({
        start: segmentStart,
        end: segmentEnd,
        text: currentSegment.map((w) => w.word || "").join(" "),
      });
      currentSegment = [];
      segmentStart = null;
    }
  }

  if (currentSegment.length > 0) {
    segments.push({
      start: segmentStart,
      end: currentSegment[currentSegment.length - 1].end || 0,
      text: currentSegment.map((w) => w.word || "").join(" "),
    });
  }

  return segments;
}

function generateSrt(segments) {
  const lines = [];
  segments.forEach((segment, i) => {
    const start = formatTimeSrt(segment.start);
    const end = formatTimeSrt(segment.end);
    lines.push(`${i + 1}`);
    lines.push(`${start} --> ${end}`);
    lines.push(segment.text);
    lines.push("");
  });
  return lines.join("\n");
}

function generateVtt(segments) {
  const lines = ["WEBVTT", ""];
  segments.forEach((segment, i) => {
    const start = formatTimeVtt(segment.start);
    const end = formatTimeVtt(segment.end);
    lines.push(`${i + 1}`);
    lines.push(`${start} --> ${end}`);
    lines.push(segment.text);
    lines.push("");
  });
  return lines.join("\n");
}

function processResponse(result, inputPath) {
  if (result.status !== "success") {
    console.error("Error: Transcription failed");
    console.error(result);
    process.exit(1);
  }

  const words = result.words || [];
  if (words.length === 0) {
    console.error("Error: No word timestamps returned. Cannot generate subtitles.");
    process.exit(1);
  }

  const transcription = result.transcription || "";
  console.log(`Transcription: ${transcription.substring(0, 100)}...`);
  console.log(`Words detected: ${words.length}`);

  const segments = createSegments(words);
  console.log(`Subtitle segments: ${segments.length}`);

  const baseName = path.basename(inputPath, path.extname(inputPath));
  const dirName = path.dirname(inputPath);

  const srtContent = generateSrt(segments);
  const srtPath = path.join(dirName, `${baseName}.srt`);
  fs.writeFileSync(srtPath, srtContent, "utf-8");
  console.log(`Saved: ${srtPath}`);

  const vttContent = generateVtt(segments);
  const vttPath = path.join(dirName, `${baseName}.vtt`);
  fs.writeFileSync(vttPath, vttContent, "utf-8");
  console.log(`Saved: ${vttPath}`);

  console.log("\nDone!");
}

async function main() {
  const inputFile = process.argv[2];

  if (!inputFile) {
    console.log("Usage: node transcribe.js <audio_or_video_file>");
    process.exit(1);
  }

  const apiKey = process.env.SMALLEST_API_KEY;

  if (!apiKey) {
    console.error("Error: SMALLEST_API_KEY environment variable not set");
    process.exit(1);
  }

  if (!fs.existsSync(inputFile)) {
    console.error(`Error: File not found: ${inputFile}`);
    process.exit(1);
  }

  console.log(`Processing: ${path.basename(inputFile)}`);

  const result = await transcribe(inputFile, apiKey);
  processResponse(result, inputFile);
}

main();



================================================
FILE: speech-to-text/subtitle-generation/python/transcribe.py
================================================
#!/usr/bin/env python3
"""
Smallest AI Speech-to-Text - Subtitle Generation

Generate SRT and VTT subtitles from audio or video files.
Uses word timestamps to create properly timed subtitle segments.

Usage: python transcribe.py <audio_or_video_file>

Output:
- {filename}.srt - SubRip subtitle file
- {filename}.vtt - WebVTT subtitle file
"""

import os
import sys
from pathlib import Path

import requests
from dotenv import load_dotenv

load_dotenv()

API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text"

LANGUAGE = "en"  # Use ISO 639-1 codes or "multi" for auto-detect
WORDS_PER_SEGMENT = 10  # Maximum words per subtitle segment
MAX_SEGMENT_DURATION = 5.0  # Maximum duration per segment in seconds


def transcribe(audio_file: str, api_key: str) -> dict:
    with open(audio_file, "rb") as f:
        audio_data = f.read()

    response = requests.post(
        API_URL,
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/octet-stream",
        },
        params={
            "language": LANGUAGE,
            "word_timestamps": "true",
        },
        data=audio_data,
        timeout=300,
    )

    if response.status_code != 200:
        raise Exception(f"API request failed with status {response.status_code}: {response.text}")

    return response.json()


def format_time_srt(seconds: float) -> str:
    """Format seconds to SRT timestamp: HH:MM:SS,mmm"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


def format_time_vtt(seconds: float) -> str:
    """Format seconds to VTT timestamp: HH:MM:SS.mmm"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millis = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d}.{millis:03d}"


def create_segments(words: list) -> list:
    """Group words into subtitle segments."""
    segments = []
    current_segment = []
    segment_start = None

    for word in words:
        if segment_start is None:
            segment_start = word.get("start", 0)

        current_segment.append(word)
        segment_end = word.get("end", 0)
        segment_duration = segment_end - segment_start

        if len(current_segment) >= WORDS_PER_SEGMENT or segment_duration >= MAX_SEGMENT_DURATION:
            segments.append({
                "start": segment_start,
                "end": segment_end,
                "text": " ".join(w.get("word", "") for w in current_segment)
            })
            current_segment = []
            segment_start = None

    if current_segment:
        segments.append({
            "start": segment_start,
            "end": current_segment[-1].get("end", 0),
            "text": " ".join(w.get("word", "") for w in current_segment)
        })

    return segments


def generate_srt(segments: list) -> str:
    """Generate SRT format subtitles."""
    lines = []
    for i, segment in enumerate(segments, 1):
        start = format_time_srt(segment["start"])
        end = format_time_srt(segment["end"])
        lines.append(f"{i}")
        lines.append(f"{start} --> {end}")
        lines.append(segment["text"])
        lines.append("")
    return "\n".join(lines)


def generate_vtt(segments: list) -> str:
    """Generate WebVTT format subtitles."""
    lines = ["WEBVTT", ""]
    for i, segment in enumerate(segments, 1):
        start = format_time_vtt(segment["start"])
        end = format_time_vtt(segment["end"])
        lines.append(f"{i}")
        lines.append(f"{start} --> {end}")
        lines.append(segment["text"])
        lines.append("")
    return "\n".join(lines)


def process_response(result: dict, audio_path: Path):
    if result.get("status") != "success":
        print("Error: Transcription failed")
        print(result)
        sys.exit(1)

    words = result.get("words", [])
    if not words:
        print("Error: No word timestamps returned. Cannot generate subtitles.")
        sys.exit(1)

    print(f"Transcription: {result.get('transcription', '')[:100]}...")
    print(f"Words detected: {len(words)}")

    segments = create_segments(words)
    print(f"Subtitle segments: {len(segments)}")

    # Generate SRT
    srt_content = generate_srt(segments)
    srt_path = audio_path.with_suffix(".srt")
    with open(srt_path, "w", encoding="utf-8") as f:
        f.write(srt_content)
    print(f"Saved: {srt_path}")

    # Generate VTT
    vtt_content = generate_vtt(segments)
    vtt_path = audio_path.with_suffix(".vtt")
    with open(vtt_path, "w", encoding="utf-8") as f:
        f.write(vtt_content)
    print(f"Saved: {vtt_path}")

    print("\nDone!")


def main():
    if len(sys.argv) < 2:
        print("Usage: python transcribe.py <audio_or_video_file>")
        sys.exit(1)

    audio_file = sys.argv[1]
    api_key = os.environ.get("SMALLEST_API_KEY")

    if not api_key:
        print("Error: SMALLEST_API_KEY environment variable not set")
        sys.exit(1)

    audio_path = Path(audio_file)
    if not audio_path.exists():
        print(f"Error: File not found: {audio_file}")
        sys.exit(1)

    print(f"Processing: {audio_path.name}")

    result = transcribe(audio_file, api_key)
    process_response(result, audio_path)


if __name__ == "__main__":
    main()




================================================
FILE: speech-to-text/websocket/jarvis/README.md
================================================
# Jarvis Voice Assistant

A background AI assistant powered by Smallest AI that can understand what's on your screen through screenshots.

Say "Jarvis" followed by your question to get a spoken response. Mention "screenshot" in your query, and Jarvis will prompt you to capture part of your screen for visual context.

## Demo

[Watch Demo](./demo.mp4)

## Features

- **Wake Word Activation**: Triggered by saying "Jarvis"
- **Screenshot Analysis**: Say "screenshot" to capture and analyze part of your screen
- **Conversation Memory**: Stores previous context for follow-up questions
- **Voice Response**: Speaks responses aloud via TTS

## Pipeline

```
Microphone â†’ Pulse STT (WebSocket) â†’ Wake Word â†’ [Screenshot?] â†’ Vision + LLM â†’ TTS (HTTP) â†’ Speaker
```

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../../README.md#usage) for setup.

Add `SMALLEST_API_KEY` and `GROQ_API_KEY` to your `.env`.

Extra dependencies:

```bash
uv pip install -r requirements.txt
```

This installs `pyaudio` and `httpx` (groq is already in the root install).

## Usage

```bash
uv run jarvis.py
```

## Recommended Usage

- Starting point for building always-on voice assistants with wake word detection, vision, and speech output
- Local desktop assistant combining STT, LLM, and TTS
- For deployed telephony agents, the [Voice Agents](../../../voice-agents/) examples are recommended

## How It Works

1. **Continuous Listening**: Microphone audio streams to Pulse STT via WebSocket
2. **Wake Word Detection**: Listens for "Jarvis" in transcriptions
3. **Query Capture**: Collects speech until 5 seconds of silence
4. **Screenshot (Optional)**: If query contains "screenshot":
   - Opens native screenshot tool (flameshot/scrot/etc.)
   - Select the region you want to capture
   - Press Enter or click to confirm
   - Vision model extracts text and describes the image
5. **LLM Response**: Query + image context sent to Groq (Llama 3.3 70B)
6. **TTS Playback**: Response spoken via Lightning TTS (HTTP POST)
7. **Follow-up**: Stays in conversation mode for 60 seconds

## Project Structure

```
speech-to-text/websocket/jarvis/
â”œâ”€â”€ .env.sample          # Environment variables template
â”œâ”€â”€ README.md
â”œâ”€â”€ jarvis.py        # Main assistant (wake word, state machine)
â”œâ”€â”€ stt.py           # Pulse STT WebSocket client + mic capture
â”œâ”€â”€ llm.py           # Groq LLM + vision with screenshot support
â”œâ”€â”€ tts.py           # Lightning TTS HTTP client
â””â”€â”€ requirements.txt
```

## Configuration

| File | Variable | Default | Description |
|------|----------|---------|-------------|
| `jarvis.py` | `WAKE_WORD` | `"jarvis"` | Trigger phrase (case-insensitive) |
| `jarvis.py` | `SILENCE_TIMEOUT` | `5.0` | Seconds of silence to end query |
| `jarvis.py` | `CONVERSATION_TIMEOUT` | `60.0` | Seconds before returning to wake word mode |
| `tts.py` | `TTS_VOICE` | `"sophia"` | Lightning TTS voice ID |
| `llm.py` | `LLM_MODEL` | `"llama-3.3-70b-versatile"` | Text model |
| `llm.py` | `LLM_VISION_MODEL` | `"meta-llama/llama-4-scout-17b-16e-instruct"` | Vision model for screenshots |

## Further Scope

This example is intentionally kept simple to serve as a starting point for building voice assistants. Here are some ideas for extending it:

### Tool Calling
- **Web Search**: Integrate search APIs to answer questions about current events
- **Calendar/Email**: Connect to productivity APIs for scheduling and messaging
- **Smart Home**: Control IoT devices with voice commands
- **Code Execution**: Run Python snippets for calculations or data analysis

### TTS Streaming
The TTS implementation uses HTTP POST intentionally, returning the complete audio before playback. For lower latency, you can upgrade to the [Lightning TTS WebSocket API](https://waves-docs.smallest.ai/content/api-references/lightning-v3.1-ws) which streams audio chunks as they're generated. This is left as an exercise to help developers understand the difference between batch and streaming approaches when building voice assistants.

### Other Improvements
- Add interrupt handling (stop speaking when user starts talking)
- Implement wake word detection locally for privacy
- Add multi-language support
- Build a GUI with waveform visualization

## API Reference

- [Pulse STT WebSocket](https://waves-docs.smallest.ai/content/api-references/pulse-stt-ws)
- [Lightning TTS HTTP](https://waves-docs.smallest.ai/content/api-references/lightning-v3.1)
- [Lightning TTS WebSocket](https://waves-docs.smallest.ai/content/api-references/lightning-v3.1-ws)
- [Groq API](https://console.groq.com/docs/api-reference)

## Next Steps

- [Voice Agents](../../../voice-agents/) â€” Build deployed telephony agents with the Atoms SDK
- [Realtime Microphone](../realtime-microphone-transcription/) â€” Simpler Gradio UI for live transcription



================================================
FILE: speech-to-text/websocket/jarvis/jarvis.py
================================================
#!/usr/bin/env python3
"""
Jarvis Voice Assistant

A wake word activated voice assistant demonstrating how to build voice assistants
with Smallest AI's speech APIs.

Architecture:
- STT: WebSocket streaming for low-latency transcription (stt.py)
- LLM: Groq API with vision support for screenshots (llm.py)
- TTS: HTTP POST for simplicity - upgrade to WebSocket for lower latency (tts.py)

State Machine:
- LISTENING: Waiting for wake word "Jarvis"
- CAPTURING: Recording user's query until silence
- PROCESSING: Getting LLM response and speaking it

Usage: python jarvis.py
"""

import asyncio
import json
import re
import time

from dotenv import load_dotenv

from llm import LLMClient
from stt import STTClient
from tts import TTSClient

load_dotenv()

# Configuration - customize these for your use case
WAKE_WORD = "jarvis"
SILENCE_TIMEOUT = 5.0       # Seconds of silence to end query capture
CONVERSATION_TIMEOUT = 60.0  # Seconds before resetting conversation context


class JarvisAssistant:
    """
    Wake word activated voice assistant.
    
    States:
        LISTENING: Waiting for wake word
        CAPTURING: Recording user query after wake word
        PROCESSING: Getting LLM response and speaking
    """

    def __init__(self):
        self.llm = LLMClient()
        self.stt = STTClient()

        self.state = "LISTENING"
        self.query_buffer = []
        self.conversation_history = []
        self.last_transcript_time = 0
        self.last_conversation_time = 0

        self.running = False
        self.jarvis_speaking = False
        self.silence_task = None
        self.timer_version = 0

    async def run(self):
        """Start the voice assistant main loop."""
        self.running = True

        print("=" * 50)
        print("JARVIS VOICE ASSISTANT")
        print("=" * 50)
        print(f"Wake word: \"{WAKE_WORD}\"")
        print("Press Ctrl+C to exit.")
        print("-" * 50)
        print("[LISTENING] Waiting for wake word...")

        while self.running:
            try:
                await self._stt_session()
            except Exception as e:
                print(f"[STT] Reconnecting... ({e})")
                await asyncio.sleep(1)

    async def _stt_session(self):
        """Run STT session with send/receive loop."""
        await self.stt.connect()

        async def send_audio():
            while self.running:
                if self.jarvis_speaking:
                    self.stt.clear_queue()
                    await asyncio.sleep(0.05)
                    continue

                while not self.stt.audio_queue.empty():
                    await self.stt.ws.send(self.stt.audio_queue.get_nowait())
                await asyncio.sleep(0.05)

        async def receive_transcripts():
            async for message in self.stt.ws:
                if not self.running:
                    break

                result = json.loads(message)
                transcript = result.get("transcript", "").strip()

                if not result.get("is_final") or not transcript:
                    continue

                print(f"[STT] {transcript}")

                if self.state == "LISTENING":
                    self._handle_wake_word(transcript)
                elif self.state == "CAPTURING":
                    self._handle_capture(transcript)

        try:
            await asyncio.gather(send_audio(), receive_transcripts())
        finally:
            await self.stt.close()

    def _handle_wake_word(self, transcript: str):
        """Check for wake word and transition to CAPTURING state."""
        if WAKE_WORD not in transcript.lower():
            return

        print("\n[WAKE] Detected! Listening...")
        self.state = "CAPTURING"
        self.query_buffer = []
        self.conversation_history = []
        self.last_transcript_time = time.time()
        self.last_conversation_time = time.time()

        after = transcript.lower().split(WAKE_WORD, 1)[-1].strip()
        if after:
            self.query_buffer.append(after)
            print(f"[CAPTURE] {after}")

        self._restart_timer()

    def _handle_capture(self, transcript: str):
        """Add transcript to query buffer and reset silence timer."""
        clean = re.sub(rf"^.*{WAKE_WORD}", "", transcript.lower(), flags=re.IGNORECASE).strip()
        if clean:
            self.query_buffer.append(clean)
            print(f"[CAPTURE] {clean}")

        self.last_transcript_time = time.time()
        self._restart_timer()

    def _restart_timer(self):
        """Cancel existing silence timer and start a new one."""
        self.timer_version += 1
        my_version = self.timer_version

        if self.silence_task and not self.silence_task.done():
            self.silence_task.cancel()
        self.silence_task = asyncio.create_task(self._silence_check(my_version))

    async def _silence_check(self, my_version: int):
        """Wait for silence timeout, then process query if no new speech detected."""
        try:
            await asyncio.sleep(SILENCE_TIMEOUT)

            if self.timer_version != my_version:
                return

            if self.state != "CAPTURING":
                return

            if time.time() - self.last_transcript_time >= SILENCE_TIMEOUT - 0.1:
                if self.query_buffer:
                    await self._process_query()
                elif time.time() - self.last_conversation_time > CONVERSATION_TIMEOUT:
                    print("\n[TIMEOUT] Erasing context and returning to wake word detection...")
                    self.state = "LISTENING"
                    self.conversation_history = []
                    print("[LISTENING] Waiting for wake word...")
                else:
                    self._restart_timer()
        except asyncio.CancelledError:
            pass

    async def _process_query(self):
        """Send query to LLM and speak response via TTS."""
        query = " ".join(self.query_buffer).strip()
        self.query_buffer = []

        if not query:
            self.state = "CAPTURING"
            self._restart_timer()
            return

        if time.time() - self.last_conversation_time > CONVERSATION_TIMEOUT:
            self.conversation_history = []

        self.state = "PROCESSING"
        print(f"\n[QUERY] {query}")
        self.jarvis_speaking = True

        response, is_stop = self.llm.get_response(query, self.conversation_history)

        if is_stop:
            print("[STOP] Ignoring unrelated speech")
            self.jarvis_speaking = False
            self.state = "CAPTURING"
            self.last_transcript_time = time.time()
            self._restart_timer()
            return

        print(f"[RESPONSE] {response}")

        tts = TTSClient()
        try:
            tts.start()
            await tts.speak(response)
        finally:
            tts.stop()
            self.stt.clear_queue()
            self.jarvis_speaking = False

        self.conversation_history.append({"role": "user", "content": query})
        self.conversation_history.append({"role": "assistant", "content": response})

        self.last_conversation_time = time.time()
        self.state = "CAPTURING"
        self.last_transcript_time = time.time()
        self._restart_timer()
        print("[LISTENING] Ready for follow-up...")

    def stop(self):
        """Stop the assistant."""
        self.running = False


def main():
    try:
        assistant = JarvisAssistant()
        asyncio.run(assistant.run())
    except ValueError as e:
        print(f"Error: {e}")
    except KeyboardInterrupt:
        print("\n[EXIT] Goodbye!")


if __name__ == "__main__":
    main()



================================================
FILE: speech-to-text/websocket/jarvis/llm.py
================================================
"""
Groq LLM Client for Jarvis

This module handles LLM interactions including:
- Text-only queries using Llama 3.3 70B
- Screenshot analysis using a two-step approach:
  1. Vision model extracts text and describes the image
  2. Text model generates the final response

The two-step approach for vision ensures high-quality responses by leveraging
the stronger text model for reasoning while using the vision model for perception.

Further scope: Add tool calling for web search, calendar, smart home, etc.
"""

import base64
import os
import platform
import shutil
import subprocess
import tempfile

from groq import Groq

LLM_MODEL = "llama-3.3-70b-versatile"
LLM_VISION_MODEL = "meta-llama/llama-4-scout-17b-16e-instruct"
LLM_SYSTEM_PROMPT = """You are Jarvis, a helpful AI assistant.

Start EVERY response with exactly one of these prefixes:
- "SPEAK:" if the user's message is directed at you
- "STOP:" if it's unrelated background speech or not meant for you

Keep responses concise (1-3 sentences)."""

VISION_EXTRACTION_PROMPT = """Analyze this image and provide:
1. ALL TEXT visible in the image (extract exactly as written)
2. A brief description of what the image shows

Format:
TEXT IN IMAGE:
<extracted text or "No text visible">

IMAGE DESCRIPTION:
<brief description>"""


def take_screenshot() -> str | None:
    """Capture screen region using native tools. Returns base64 image or None."""
    system = platform.system()
    filepath = tempfile.mktemp(suffix=".png")

    try:
        if system == "Windows":
            subprocess.run(["snippingtool", "/clip"], check=True)
            ps_cmd = f'''
            Add-Type -AssemblyName System.Windows.Forms
            $img = [System.Windows.Forms.Clipboard]::GetImage()
            if ($img) {{ $img.Save("{filepath}") }}
            '''
            subprocess.run(["powershell", "-Command", ps_cmd], check=True)

        elif system == "Linux":
            if shutil.which("flameshot"):
                result = subprocess.run(["flameshot", "gui", "--raw"], capture_output=True)
                if result.returncode == 0 and result.stdout:
                    with open(filepath, "wb") as f:
                        f.write(result.stdout)
            elif shutil.which("spectacle"):
                subprocess.run(["spectacle", "-r", "-b", "-o", filepath], check=True)
            elif shutil.which("gnome-screenshot"):
                subprocess.run(["gnome-screenshot", "-a", "-f", filepath], check=True)
            elif shutil.which("scrot"):
                subprocess.run(["scrot", "-s", filepath], check=True)
            elif shutil.which("import"):
                subprocess.run(["import", filepath], check=True)
            else:
                print("[Screenshot] No tool found")
                return None

        elif system == "Darwin":
            subprocess.run(["screencapture", "-i", filepath], check=True)

        if os.path.exists(filepath) and os.path.getsize(filepath) > 0:
            with open(filepath, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
        return None

    except Exception as e:
        print(f"[Screenshot] Error: {e}")
        return None
    finally:
        if os.path.exists(filepath):
            os.unlink(filepath)


def get_context_history(history: list) -> list:
    """Get first query-response pair + last 3 pairs from history."""
    n = len(history)
    if n <= 2:
        return history

    first_pair = history[:2]
    last_pairs_start = max(2, n - 6)
    return first_pair + history[last_pairs_start:]


class LLMClient:
    """Groq LLM client with vision support."""

    def __init__(self):
        api_key = os.environ.get("GROQ_API_KEY")
        if not api_key:
            raise ValueError("GROQ_API_KEY environment variable not set")
        self.client = Groq(api_key=api_key)

    def get_response(self, query: str, history: list) -> tuple[str, bool]:
        """Get LLM response. Captures screenshot if 'screenshot' in query."""
        context = get_context_history(history)
        if context:
            history_text = "\n".join(
                f"{'User' if m['role'] == 'user' else 'Jarvis'}: {m['content']}"
                for m in context
            )
            text_content = f"CONVERSATION HISTORY:\n{history_text}\n\nCURRENT QUERY: {query}"
        else:
            text_content = f"CURRENT QUERY: {query}"

        if "screenshot" in query.lower():
            print("[LLM] Screenshot requested...")
            image_b64 = take_screenshot()
            if image_b64:
                return self._vision_request(query, image_b64)
            print("[LLM] Screenshot failed, using text model")

        return self._text_request(text_content)

    def _text_request(self, content: str) -> tuple[str, bool]:
        """Text-only LLM request."""
        response = self.client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": LLM_SYSTEM_PROMPT},
                {"role": "user", "content": content},
            ],
            max_tokens=256,
            temperature=0.7,
        )
        text = response.choices[0].message.content.strip()
        return self._parse_response(text)

    def _vision_request(self, query: str, image_b64: str) -> tuple[str, bool]:
        """Two-step: vision model extracts content, text model responds."""
        try:
            vision_response = self.client.chat.completions.create(
                model=LLM_VISION_MODEL,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": VISION_EXTRACTION_PROMPT},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_b64}"}},
                    ],
                }],
                max_tokens=1024,
                temperature=0.3,
            )
            extraction = vision_response.choices[0].message.content.strip()

            combined = f"Screenshot content:\n\n{extraction}\n\nUser's question: {query}"
            text_response = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "system", "content": LLM_SYSTEM_PROMPT},
                    {"role": "user", "content": combined},
                ],
                max_tokens=512,
                temperature=0.7,
            )
            text = text_response.choices[0].message.content.strip()
            return self._parse_response(text)

        except Exception as e:
            print(f"[Vision] Error: {e}")
            return f"Sorry, I couldn't analyze the screenshot: {e}", False

    def _parse_response(self, text: str) -> tuple[str, bool]:
        """Parse SPEAK:/STOP: prefix from response."""
        if text.upper().startswith("STOP:"):
            return None, True
        if text.upper().startswith("SPEAK:"):
            text = text[6:].strip()
        return text, False



================================================
FILE: speech-to-text/websocket/jarvis/requirements.txt
================================================
# Extra dependencies for Jarvis (beyond root requirements.txt)
pyaudio==0.2.14
httpx==0.28.1

# System dependencies (install first):
# On Ubuntu/Debian: sudo apt install portaudio19-dev flameshot
# On macOS: brew install portaudio
# On Windows: pip install pipwin && pipwin install pyaudio



================================================
FILE: speech-to-text/websocket/jarvis/stt.py
================================================
"""
Pulse STT WebSocket Client

This module handles real-time speech-to-text using WebSocket streaming.
Audio is captured from the microphone in a background thread and sent
to the Pulse API, which returns transcriptions as they're recognized.

The WebSocket approach is ideal for STT because:
- Low latency: Results arrive as you speak
- Continuous: No need to detect speech boundaries client-side
- Efficient: Single connection for the entire session
"""

import asyncio
import os
import queue
import threading
from urllib.parse import urlencode

import pyaudio
import websockets

STT_WS_URL = "wss://waves-api.smallest.ai/api/v1/pulse/get_text"
STT_SAMPLE_RATE = 16000
STT_LANGUAGE = "en"

# Customize according to your needs
CHUNK_SIZE = 1600
CHANNELS = 1
FORMAT = pyaudio.paInt16


class STTClient:
    """
    STT WebSocket client with microphone capture.
    
    Uses a separate thread for audio capture to avoid blocking the async event loop.
    Audio chunks are queued and sent to the WebSocket as fast as the network allows.
    """

    def __init__(self, language: str = STT_LANGUAGE, sample_rate: int = STT_SAMPLE_RATE):
        api_key = os.environ.get("SMALLEST_API_KEY")
        if not api_key:
            raise ValueError("SMALLEST_API_KEY environment variable not set")
        self.api_key = api_key
        self.language = language
        self.sample_rate = sample_rate
        self.audio_queue = queue.Queue()
        self.running = False
        self.paused = False
        self.capture_thread = None
        self.ws = None

    def _capture_audio(self):
        """Capture microphone audio in a background thread."""
        p = pyaudio.PyAudio()
        stream = p.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=CHUNK_SIZE,
        )

        while self.running:
            try:
                self.audio_queue.put(stream.read(CHUNK_SIZE, exception_on_overflow=False))
            except Exception as e:
                print(f"[Audio Error] {e}")
                break

        stream.stop_stream()
        stream.close()
        p.terminate()

    async def connect(self):
        """Connect to STT WebSocket and start audio capture."""
        params = {
            "language": self.language,
            "encoding": "linear16",
            "sample_rate": self.sample_rate,
        }
        url = f"{STT_WS_URL}?{urlencode(params)}"
        headers = {"Authorization": f"Bearer {self.api_key}"}

        self.ws = await websockets.connect(url, additional_headers=headers, open_timeout=30)
        self.running = True

        self.capture_thread = threading.Thread(target=self._capture_audio, daemon=True)
        self.capture_thread.start()

    async def send_audio(self):
        """Send audio chunks to STT WebSocket. Skips while paused."""
        while self.running:
            if self.paused:
                while not self.audio_queue.empty():
                    self.audio_queue.get_nowait()
                await asyncio.sleep(0.05)
                continue

            while not self.audio_queue.empty():
                await self.ws.send(self.audio_queue.get_nowait())
            await asyncio.sleep(0.05)

    def pause(self):
        """Pause audio capture (clears queue while paused)."""
        self.paused = True

    def resume(self):
        """Resume audio capture."""
        self.paused = False

    def clear_queue(self):
        """Clear any buffered audio."""
        while not self.audio_queue.empty():
            self.audio_queue.get_nowait()

    async def close(self):
        """Close WebSocket connection."""
        self.running = False
        if self.ws:
            await self.ws.close()



================================================
FILE: speech-to-text/websocket/jarvis/tts.py
================================================
"""
Lightning TTS HTTP Client

This module uses the HTTP POST endpoint for text-to-speech. The entire audio
is generated server-side and returned in one response, then played back locally.

For lower latency, you can upgrade to the WebSocket streaming API which delivers
audio chunks as they're generated. See:
https://waves-docs.smallest.ai/content/api-references/lightning-v3.1-ws
"""

import asyncio
import os
import queue
import threading

import httpx
import pyaudio

TTS_URL = "https://waves-api.smallest.ai/api/v1/lightning-v3.1/get_speech"
TTS_VOICE = "sophia"
TTS_SAMPLE_RATE = 24000
PLAYBACK_CHUNK_SIZE = 4800


class TTSClient:
    """
    TTS HTTP client with background audio playback.
    
    This implementation waits for the complete audio before playback starts.
    For streaming playback, you would use WebSocket and start playing chunks
    as they arrive from the server.
    """

    def __init__(self, voice: str = TTS_VOICE, sample_rate: int = TTS_SAMPLE_RATE):
        api_key = os.environ.get("SMALLEST_API_KEY")
        if not api_key:
            raise ValueError("SMALLEST_API_KEY environment variable not set")
        self.api_key = api_key
        self.voice = voice
        self.sample_rate = sample_rate
        self.audio_queue = queue.Queue()
        self.playback_thread = None
        self.pyaudio_instance = None
        self.audio_stream = None
        self.stop_playback = False

    def _play_audio(self):
        """Background thread that plays audio from the queue."""
        while not self.stop_playback:
            try:
                data = self.audio_queue.get(timeout=0.1)
                if data is None:
                    break
                self.audio_stream.write(data)
            except queue.Empty:
                continue

    def start(self):
        """Initialize audio playback."""
        self.pyaudio_instance = pyaudio.PyAudio()
        self.audio_stream = self.pyaudio_instance.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=self.sample_rate,
            output=True,
        )

        self.stop_playback = False
        self.playback_thread = threading.Thread(target=self._play_audio, daemon=True)
        self.playback_thread.start()

    async def speak(self, text: str):
        """
        Convert text to speech and play it.
        
        With HTTP POST, we wait for the full audio before playback begins.
        With WebSocket streaming, you could start playback as soon as the
        first audio chunk arrives, reducing perceived latency.
        """
        if not text.strip():
            return

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "text": text,
            "voice_id": self.voice,
            "sample_rate": self.sample_rate,
            "speed": 1.0,
            "language": "en",
            "output_format": "pcm",
        }

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(TTS_URL, headers=headers, json=payload)

            if response.status_code != 200:
                print(f"[TTS ERROR] {response.status_code}: {response.text}")
                return

            audio_bytes = response.content

            # Feed audio to playback thread in chunks to avoid buffer issues
            for i in range(0, len(audio_bytes), PLAYBACK_CHUNK_SIZE):
                self.audio_queue.put(audio_bytes[i:i + PLAYBACK_CHUNK_SIZE])

        while not self.audio_queue.empty():
            await asyncio.sleep(0.1)
        await asyncio.sleep(0.2)

    def stop(self):
        """Stop playback and clean up resources."""
        self.stop_playback = True
        self.audio_queue.put(None)

        if self.playback_thread:
            self.playback_thread.join(timeout=2.0)
        if self.audio_stream:
            self.audio_stream.stop_stream()
            self.audio_stream.close()
        if self.pyaudio_instance:
            self.pyaudio_instance.terminate()



================================================
FILE: speech-to-text/websocket/jarvis/.env.sample
================================================
# Smallest AI API Key (for Pulse STT and Lightning TTS)
# Get yours at https://console.smallest.ai/apikeys
SMALLEST_API_KEY=your-smallest-api-key-here

# Groq API Key (for LLM responses)
# Get yours at https://console.groq.com
GROQ_API_KEY=your-groq-api-key-here



================================================
FILE: speech-to-text/websocket/realtime-microphone-transcription/README.md
================================================
# Realtime Microphone Transcription

Gradio web interface for real-time speech-to-text transcription from your microphone.

## Demo

![Demo](demo.gif)

## Features

- Web UI for live transcription powered by Gradio
- Stream microphone audio through WebSocket in real-time
- Display live transcription results as you speak

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../../README.md#usage) for setup. Add `SMALLEST_API_KEY` to your `.env`.

Extra dependencies:

```bash
uv pip install -r requirements.txt
```

This installs `numpy` (gradio is already in the root install).

## Usage

```bash
uv run app.py
```

Open http://localhost:7860 in your browser.

1. Click the microphone button to start recording
2. Speak â€” transcription appears in real-time
3. Click stop to end the session

## Recommended Usage

- Visual web interface for live microphone transcription â€” great for demos and prototyping
- Quick testing of Pulse STT with real-time audio
- For a full voice assistant with LLM and TTS, [Jarvis](../jarvis/) is recommended

## How It Works

The Gradio app captures microphone audio, streams it via WebSocket to Pulse STT, and displays interim and final transcripts in real-time. The WebSocket connection is managed using `websockets.sync.client` for thread compatibility with Gradio.

## API Reference

- [Streaming Quickstart](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/streaming/quickstart)
- [Pulse STT WebSocket API](https://waves-docs.smallest.ai/content/api-references/pulse-stt-ws)

## Next Steps

- [Jarvis Voice Assistant](../jarvis/) â€” Full assistant with wake word detection, LLM, and TTS
- [Streaming Transcription](../streaming-text-output-transcription/) â€” Stream audio files via WebSocket



================================================
FILE: speech-to-text/websocket/realtime-microphone-transcription/app.py
================================================
#!/usr/bin/env python3
"""
Smallest AI Speech-to-Text - Realtime Transcription

Gradio web interface for real-time speech-to-text transcription.
Speak into your microphone and see live transcription.

Usage: python app.py

Output:
- Web interface at http://localhost:7860
"""

import json
import os
import threading
import queue
from urllib.parse import urlencode

import gradio as gr
import numpy as np
from websockets.sync.client import connect
from dotenv import load_dotenv

load_dotenv()

WS_URL = "wss://waves-api.smallest.ai/api/v1/pulse/get_text"
SAMPLE_RATE = 16000
API_KEY = os.environ.get("SMALLEST_API_KEY")
if API_KEY is None:
    print("Error: SMALLEST_API_KEY environment variable not set")
    exit(1)


class TranscriptionSession:
    def __init__(self):
        self.ws = None
        self.response_queue = queue.Queue()
        self.receiver_thread = None
        self.prev = ""
        self.is_active = False

    def start(self):
        if self.is_active:
            return

        params = {
            "language": "en",
            "encoding": "linear16",
            "sample_rate": SAMPLE_RATE,
        }
        url = f"{WS_URL}?{urlencode(params)}"
        headers = {"Authorization": f"Bearer {API_KEY}"}

        try:
            self.ws = connect(url, additional_headers=headers, open_timeout=30)
        except TimeoutError:
            self.prev = "Error: Connection timed out. Please try again."
            return
        except Exception as e:
            self.prev = f"Error: {str(e)}"
            return

        self.is_active = True
        self.prev = ""

        self.receiver_thread = threading.Thread(target=self._receive_responses, daemon=True)
        self.receiver_thread.start()

    def _receive_responses(self):
        try:
            for message in self.ws:
                result = json.loads(message)
                if result.get("is_final"):
                    self.response_queue.put(result)
                if result.get("is_last"):
                    self.is_active = False
                    break
        except Exception as e:
            self.response_queue.put({"error": str(e)})

    def send_audio(self, audio_data: bytes):
        if self.ws and self.is_active:
            try:
                self.ws.send(audio_data)
            except Exception:
                pass

    def end_session(self):
        if self.ws and self.is_active:
            try:
                self.ws.send(json.dumps({"type": "end"}))
            except Exception:
                pass

    def get_transcript(self) -> str:
        while not self.response_queue.empty():
            try:
                result = self.response_queue.get_nowait()
                if result.get("error"):
                    return f"Error: {result['error']}"
                self.prev = result.get("transcript", "")
            except queue.Empty:
                break
        return self.prev

    def close(self):
        self.is_active = False
        if self.ws:
            try:
                self.ws.close()
            except Exception:
                pass
            self.ws = None


session = TranscriptionSession()


def process_audio(audio, history, is_recording):
    if audio is None:
        if is_recording:
            session.end_session()
            session.close()
        return history or "", False

    sr, audio_data = audio

    if len(audio_data) == 0:
        return history or "", is_recording

    if not is_recording:
        session.start()
        is_recording = True

    if len(audio_data.shape) > 1:
        audio_data = audio_data.mean(axis=1)

    if audio_data.dtype != np.int16:
        if audio_data.dtype in [np.float32, np.float64]:
            audio_data = (audio_data * 32768).clip(-32768, 32767).astype(np.int16)
        else:
            audio_data = audio_data.astype(np.int16)

    if sr != SAMPLE_RATE:
        import librosa
        audio_float = audio_data.astype(np.float32) / 32768.0
        audio_resampled = librosa.resample(audio_float, orig_sr=sr, target_sr=SAMPLE_RATE)
        audio_data = (audio_resampled * 32768).clip(-32768, 32767).astype(np.int16)

    session.send_audio(audio_data.tobytes())

    current = session.get_transcript()
    return current, is_recording


def clear_history():
    session.close()
    session.prev = ""
    return "", False


with gr.Blocks(
    title="Realtime Transcription",
    theme=gr.themes.Soft(primary_hue="emerald"),
) as app:
    gr.Markdown("# Realtime Transcription")
    gr.Markdown("Speak into your microphone for live transcription")

    is_recording_state = gr.State(False)

    audio_input = gr.Audio(
        sources=["microphone"],
        streaming=True,
        label="Microphone",
    )

    transcript_output = gr.Textbox(
        label="Transcript",
        lines=8,
        interactive=False,
    )

    clear_btn = gr.Button("Clear")

    audio_input.stream(
        fn=process_audio,
        inputs=[audio_input, transcript_output, is_recording_state],
        outputs=[transcript_output, is_recording_state],
    )

    clear_btn.click(
        fn=clear_history,
        outputs=[transcript_output, is_recording_state],
    )


if __name__ == "__main__":
    app.launch()



================================================
FILE: speech-to-text/websocket/realtime-microphone-transcription/requirements.txt
================================================
# Extra dependencies for Realtime Microphone Transcription (beyond root requirements.txt)
numpy



================================================
FILE: speech-to-text/websocket/realtime-microphone-transcription/.env.sample
================================================
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here



================================================
FILE: speech-to-text/websocket/streaming-text-output-transcription/README.md
================================================
# Streaming Transcription

Stream an audio file through the WebSocket API and capture all transcription responses â€” both interim (partial) and final.

## Features

- Stream audio files through the WebSocket API
- Handle streaming responses (interim and final)
- Configurable language, diarization, PII/PCI redaction, and more
- Save all responses to a text file

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../../README.md#usage) for setup. Add `SMALLEST_API_KEY` to your `.env`.

Extra dependencies:

```bash
uv pip install -r requirements.txt
```

This installs `librosa` and `numpy` for audio processing.

## Usage

### Python

```bash
uv run python/transcribe.py audio.wav
```

### JavaScript

```bash
cd javascript && npm install
node javascript/transcribe.js audio.wav
```

## Recommended Usage

- Streaming a pre-recorded audio file through the WebSocket API with interim + final transcripts
- PII/PCI redaction, keyword boosting, and other streaming-only features
- For live microphone input, [Realtime Microphone](../realtime-microphone-transcription/) is recommended

## Key Snippets

The script reads an audio file, resamples it to 16kHz mono PCM, and streams it in chunks over a WebSocket connection. The API returns interim transcripts (fast, lower accuracy) and final transcripts (accurate) as the audio streams in.

```python
async with websockets.connect(ws_url) as ws:
    # Send audio in chunks
    for i in range(0, len(audio_bytes), chunk_size):
        await ws.send(audio_bytes[i:i + chunk_size])
    
    # Signal end of audio
    await ws.send(json.dumps({"eof": True}))
    
    # Receive transcripts
    async for message in ws:
        data = json.loads(message)
        if data.get("is_final"):
            print(data["transcript"])
```

## Configuration

| Parameter | Description | Default |
|-----------|-------------|---------|
| `LANGUAGE` | Language code (ISO 639-1) or `multi` for auto-detect | `en` |
| `FULL_TRANSCRIPT` | Return cumulative transcript | `true` |
| `WORD_TIMESTAMPS` | Include word-level timestamps | `false` |
| `SENTENCE_TIMESTAMPS` | Include sentence-level timestamps | `false` |
| `DIARIZE` | Perform speaker diarization | `false` |
| `REDACT_PII` | Redact names, addresses | `false` |
| `REDACT_PCI` | Redact credit cards, CVV | `false` |
| `NUMERALS` | Convert spoken numbers to digits | `auto` |
| `KEYWORDS` | Keyword boosting list | `[]` |

## Example Output

- **Console**: Shows only final transcripts (`is_final=true`)
- **File**: `{filename}_responses.txt` â€” all transcripts as plain text

### Response Format

Based on [Response Format documentation](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/realtime/response-format):

- `is_final=false`: Interim transcript (quick, lower accuracy)
- `is_final=true`: Final transcript for segment (accurate)
- `is_last=true`: Last response in session

## API Reference

- [Streaming Quickstart](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/streaming/quickstart)
- [Response Format](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/realtime/response-format)
- [Pulse STT WebSocket API](https://waves-docs.smallest.ai/content/api-references/pulse-stt-ws)

## Next Steps

- [Realtime Microphone](../realtime-microphone-transcription/) â€” Live microphone transcription with a Gradio web UI
- [Jarvis Voice Assistant](../jarvis/) â€” Full voice assistant with wake word, LLM, and TTS



================================================
FILE: speech-to-text/websocket/streaming-text-output-transcription/requirements.txt
================================================
# Extra dependencies for Streaming Transcription (beyond root requirements.txt)
librosa>=0.10.0
numpy>=1.24.0



================================================
FILE: speech-to-text/websocket/streaming-text-output-transcription/.env.sample
================================================
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here



================================================
FILE: speech-to-text/websocket/streaming-text-output-transcription/javascript/package.json
================================================
{
  "name": "stt-streaming-transcription",
  "version": "1.0.0",
  "description": "Smallest AI Speech-to-Text - Streaming Transcription",
  "main": "transcribe.js",
  "scripts": {
    "start": "node transcribe.js"
  },
  "dependencies": {
    "ws": "^8.18.0",
    "wav": "^1.0.2"
  },
  "keywords": ["smallest-ai", "stt", "speech-to-text", "streaming", "websocket"],
  "license": "Apache-2.0",
  "engines": {
    "node": ">=18.0.0"
  }
}




================================================
FILE: speech-to-text/websocket/streaming-text-output-transcription/javascript/transcribe.js
================================================
#!/usr/bin/env node
/**
 * Smallest AI Speech-to-Text - Streaming Transcription
 *
 * Stream an audio file through the WebSocket API and get transcription responses.
 * All transcripts are appended to a text file. Console shows only final transcripts.
 *
 * Usage: node transcribe.js <audio_file>
 *
 * Output:
 * - Console shows final transcripts (is_final=true)
 * - {filename}_responses.txt - All transcripts as plain text
 */

const fs = require("fs");
const path = require("path");
const WebSocket = require("ws");
const wav = require("wav");

const WS_URL = "wss://waves-api.smallest.ai/api/v1/pulse/get_text";
const OUTPUT_DIR = ".";

// The following are all the features supported by the WebSocket endpoint (Streaming API)
const LANGUAGE = "en";
const ENCODING = "linear16";
const SAMPLE_RATE = 16000;
const WORD_TIMESTAMPS = false;
const FULL_TRANSCRIPT = true;
const SENTENCE_TIMESTAMPS = false;
const DIARIZE = false;
const REDACT_PII = false;
const REDACT_PCI = false;
const NUMERALS = "auto";
const KEYWORDS = [];


async function loadAudio(audioFile) {
  return new Promise((resolve, reject) => {
    const reader = new wav.Reader();
    const chunks = [];

    reader.on("format", (format) => {
      reader.on("data", (chunk) => chunks.push(chunk));
      reader.on("end", () => {
        const buffer = Buffer.concat(chunks);
        const samples = new Int16Array(buffer.buffer, buffer.byteOffset, buffer.length / 2);

        // Resample if needed
        if (format.sampleRate !== SAMPLE_RATE) {
          const ratio = format.sampleRate / SAMPLE_RATE;
          const newLength = Math.floor(samples.length / ratio);
          const resampled = new Int16Array(newLength);
          for (let i = 0; i < newLength; i++) {
            resampled[i] = samples[Math.floor(i * ratio)];
          }
          resolve(resampled);
        } else {
          resolve(samples);
        }
      });
    });

    reader.on("error", reject);
    fs.createReadStream(audioFile).pipe(reader);
  });
}


async function transcribe(audioFile, apiKey, onResponse) {
  const params = new URLSearchParams({
    language: LANGUAGE,
    encoding: ENCODING,
    sample_rate: SAMPLE_RATE,
    word_timestamps: WORD_TIMESTAMPS,
    full_transcript: FULL_TRANSCRIPT,
    sentence_timestamps: SENTENCE_TIMESTAMPS,
    diarize: DIARIZE,
    redact_pii: REDACT_PII,
    redact_pci: REDACT_PCI,
    numerals: NUMERALS,
  });

  if (KEYWORDS.length > 0) {
    params.append("keywords", JSON.stringify(KEYWORDS));
  }

  const url = `${WS_URL}?${params}`;
  const audio = await loadAudio(audioFile);
  const chunkDuration = 0.1;
  const chunkSize = Math.floor(chunkDuration * SAMPLE_RATE);

  return new Promise((resolve, reject) => {
    const ws = new WebSocket(url, {
      headers: { Authorization: `Bearer ${apiKey}` },
    });

    ws.on("open", async () => {
      for (let i = 0; i < audio.length; i += chunkSize) {
        const chunk = audio.slice(i, i + chunkSize);
        ws.send(Buffer.from(chunk.buffer, chunk.byteOffset, chunk.byteLength));
        await new Promise((r) => setTimeout(r, chunkDuration * 1000));
      }
      ws.send(JSON.stringify({ type: "end" }));
    });

    ws.on("message", (data) => {
      const result = JSON.parse(data.toString());
      onResponse(result);
      if (result.is_last) {
        ws.close();
      }
    });

    ws.on("close", resolve);
    ws.on("error", reject);
  });
}


// This function is designed to process feature outputs for all the features supported
// by the WebSocket endpoint (Streaming API)
function processResponse(result, outputFile) {
  const transcript = result.transcript || "";
  if (transcript) {
    fs.appendFileSync(outputFile, transcript + "\n", "utf-8");
  }

  if (result.is_final) {
    if (result.is_last) {
      console.log(`[FINAL] ${transcript}`);

      if (result.full_transcript) {
        console.log("\n" + "=".repeat(60));
        console.log("FULL TRANSCRIPT");
        console.log("=".repeat(60));
        console.log(result.full_transcript);
      }

      // Language
      if (result.language) {
        console.log("\n" + "-".repeat(60));
        console.log("LANGUAGE");
        console.log("-".repeat(60));
        console.log(`Detected: ${result.language}`);
        if (result.languages) {
          console.log(`All: ${JSON.stringify(result.languages)}`);
        }
      }

      // Utterances (speaker diarization)
      if (result.utterances && result.utterances.length > 0) {
        console.log("\n" + "-".repeat(60));
        console.log("UTTERANCES");
        console.log("-".repeat(60));
        for (const utt of result.utterances) {
          const speaker = utt.speaker !== undefined ? utt.speaker : "";
          const start = utt.start || 0;
          const end = utt.end || 0;
          const text = utt.text || "";
          if (speaker !== "") {
            console.log(`[${start.toFixed(2)}s - ${end.toFixed(2)}s] speaker_${speaker}: ${text}`);
          } else {
            console.log(`[${start.toFixed(2)}s - ${end.toFixed(2)}s] ${text}`);
          }
        }
      }

      // Word timestamps
      if (result.words && result.words.length > 0) {
        console.log("\n" + "-".repeat(60));
        console.log("WORD TIMESTAMPS");
        console.log("-".repeat(60));
        for (const word of result.words) {
          const start = word.start || 0;
          const end = word.end || 0;
          const text = word.word || "";
          const confidence = word.confidence || 0;
          const speaker = word.speaker !== undefined ? word.speaker : "";
          if (speaker !== "") {
            console.log(`[${start.toFixed(2)}s - ${end.toFixed(2)}s] speaker_${speaker}: ${text} (${confidence.toFixed(2)})`);
          } else {
            console.log(`[${start.toFixed(2)}s - ${end.toFixed(2)}s] ${text} (${confidence.toFixed(2)})`);
          }
        }
      }

      // Redacted entities
      if (result.redacted_entities && result.redacted_entities.length > 0) {
        console.log("\n" + "-".repeat(60));
        console.log("REDACTED ENTITIES");
        console.log("-".repeat(60));
        for (const entity of result.redacted_entities) {
          console.log(`  ${entity}`);
        }
      }

      console.log("\n" + "=".repeat(60));
    } else {
      process.stdout.write(transcript);
    }
  }
}


async function main() {
  if (process.argv.length < 3) {
    console.log("Usage: node transcribe.js <audio_file>");
    process.exit(1);
  }

  const audioFile = process.argv[2];
  const apiKey = process.env.SMALLEST_API_KEY;

  if (!apiKey) {
    console.error("Error: SMALLEST_API_KEY environment variable not set");
    process.exit(1);
  }

  if (!fs.existsSync(audioFile)) {
    console.error(`Error: File not found: ${audioFile}`);
    process.exit(1);
  }

  const audioPath = path.parse(audioFile);
  const outputFile = path.join(OUTPUT_DIR, `${audioPath.name}_responses.txt`);

  if (fs.existsSync(outputFile)) {
    fs.unlinkSync(outputFile);
  }

  console.log(`Streaming: ${audioPath.base}`);
  console.log(`Responses saved to: ${outputFile}`);
  console.log("-".repeat(60));

  try {
    await transcribe(audioFile, apiKey, (result) => processResponse(result, outputFile));
    console.log("-".repeat(60));
    console.log("Done!");
  } catch (error) {
    console.error(`Error: ${error.message}`);
    process.exit(1);
  }
}

main();



================================================
FILE: speech-to-text/websocket/streaming-text-output-transcription/python/transcribe.py
================================================
#!/usr/bin/env python3
"""
Smallest AI Speech-to-Text - Streaming Transcription

Stream an audio file through the WebSocket API and get transcription responses.
All transcripts are appended to a text file. Console shows only final transcripts.

Usage: python transcribe.py <audio_file>

Output:
- Console shows final transcripts (is_final=true)
- {filename}_responses.txt - All transcripts as plain text
"""

import asyncio
import json
import os
import sys
from pathlib import Path
from urllib.parse import urlencode

import librosa
import numpy as np
import websockets
from dotenv import load_dotenv

load_dotenv()

WS_URL = "wss://waves-api.smallest.ai/api/v1/pulse/get_text"
OUTPUT_DIR = "."

# The following are all the features supported by the WebSocket endpoint (Streaming API)
LANGUAGE = "hi"
ENCODING = "linear16"
SAMPLE_RATE = 16000
WORD_TIMESTAMPS = False
FULL_TRANSCRIPT = True
SENTENCE_TIMESTAMPS = False
DIARIZE = False
REDACT_PII = False
REDACT_PCI = False
NUMERALS = "auto"
KEYWORDS = []


async def transcribe(audio_file: str, api_key: str, on_response):
    params = {
        "language": LANGUAGE,
        "encoding": ENCODING,
        "sample_rate": SAMPLE_RATE,
        "word_timestamps": str(WORD_TIMESTAMPS).lower(),
        "full_transcript": str(FULL_TRANSCRIPT).lower(),
        "sentence_timestamps": str(SENTENCE_TIMESTAMPS).lower(),
        "diarize": str(DIARIZE).lower(),
        "redact_pii": str(REDACT_PII).lower(),
        "redact_pci": str(REDACT_PCI).lower(),
        "numerals": str(NUMERALS).lower() if isinstance(NUMERALS, bool) else NUMERALS,
    }
    if KEYWORDS:
        params["keywords"] = json.dumps(KEYWORDS)

    url = f"{WS_URL}?{urlencode(params)}"
    headers = {"Authorization": f"Bearer {api_key}"}

    audio, _ = librosa.load(audio_file, sr=SAMPLE_RATE, mono=True)
    chunk_duration = 0.1
    chunk_size = int(chunk_duration * SAMPLE_RATE)

    async with websockets.connect(url, additional_headers=headers) as ws:
        async def send_audio():
            for i in range(0, len(audio), chunk_size):
                chunk = audio[i:i + chunk_size]
                pcm16 = (chunk * 32768.0).astype(np.int16).tobytes()
                await ws.send(pcm16)
                await asyncio.sleep(chunk_duration)
            await ws.send(json.dumps({"type": "end"}))

        async def receive_responses():
            async for message in ws:
                result = json.loads(message)
                on_response(result)
                if result.get("is_last"):
                    break

        await asyncio.gather(send_audio(), receive_responses())


# This function is designed to process feature outputs for all the features supported
# by the WebSocket endpoint (Streaming API)
def process_response(result: dict, output_file: Path):
    transcript = result.get("transcript", "")
    if transcript:
        with open(output_file, "a", encoding="utf-8") as f:
            f.write(transcript + "\n")

    if result.get("is_final"):
        transcript = result.get("transcript", "")
        if result.get("is_last"):
            print(f"[FINAL] {transcript}")

            if result.get("full_transcript"):
                print("\n" + "=" * 60)
                print("FULL TRANSCRIPT")
                print("=" * 60)
                print(result.get("full_transcript"))

            # Language
            if result.get("language"):
                print("\n" + "-" * 60)
                print("LANGUAGE")
                print("-" * 60)
                print(f"Detected: {result.get('language')}")
                if result.get("languages"):
                    print(f"All: {result.get('languages')}")

            # Utterances (speaker diarization)
            if result.get("utterances"):
                print("\n" + "-" * 60)
                print("UTTERANCES")
                print("-" * 60)
                for utt in result["utterances"]:
                    speaker = utt.get("speaker", "")
                    start = utt.get("start", 0)
                    end = utt.get("end", 0)
                    text = utt.get("text", "")
                    if speaker != "":
                        print(f"[{start:.2f}s - {end:.2f}s] speaker_{speaker}: {text}")
                    else:
                        print(f"[{start:.2f}s - {end:.2f}s] {text}")

            # Word timestamps
            if result.get("words"):
                print("\n" + "-" * 60)
                print("WORD TIMESTAMPS")
                print("-" * 60)
                for word in result["words"]:
                    start = word.get("start", 0)
                    end = word.get("end", 0)
                    text = word.get("word", "")
                    confidence = word.get("confidence", 0)
                    speaker = word.get("speaker", "")
                    if speaker != "":
                        print(f"[{start:.2f}s - {end:.2f}s] speaker_{speaker}: {text} ({confidence:.2f})")
                    else:
                        print(f"[{start:.2f}s - {end:.2f}s] {text} ({confidence:.2f})")

            # Redacted entities
            if result.get("redacted_entities"):
                print("\n" + "-" * 60)
                print("REDACTED ENTITIES")
                print("-" * 60)
                for entity in result["redacted_entities"]:
                    print(f"  {entity}")

            print("\n" + "=" * 60)
        else:
            print(f"{transcript}", end="", flush=True)


def main():
    if len(sys.argv) < 2:
        print("Usage: python transcribe.py <audio_file>")
        sys.exit(1)

    audio_file = sys.argv[1]
    api_key = os.environ.get("SMALLEST_API_KEY")

    if not api_key:
        print("Error: SMALLEST_API_KEY environment variable not set")
        sys.exit(1)

    audio_path = Path(audio_file)
    if not audio_path.exists():
        print(f"Error: File not found: {audio_file}")
        sys.exit(1)

    output_dir = Path(OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / f"{audio_path.stem}_responses.txt"

    if output_file.exists():
        output_file.unlink()

    print(f"Streaming: {audio_path.name}")
    print(f"Responses saved to: {output_file}")
    print("-" * 60)

    asyncio.run(transcribe(
        audio_file,
        api_key,
        lambda result: process_response(result, output_file)
    ))

    print("-" * 60)
    print(f"Done!")


if __name__ == "__main__":
    main()



================================================
FILE: speech-to-text/word-level-outputs/README.md
================================================
# Word-Level Outputs

Get word-level timestamps and speaker diarization for detailed analysis of your transcription.

## Features

- Word-level start/end timestamps for every word
- Speaker diarization â€” know who said what
- Utterance grouping by speaker
- JSON output with full metadata

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../README.md#usage) for setup. Add `SMALLEST_API_KEY` to your `.env`.

## Usage

### Python

```bash
uv run python/transcribe.py audio.wav
```

### JavaScript

```bash
node javascript/transcribe.js audio.wav
```

## Recommended Usage

- When you need to know exactly when each word was spoken or who said what
- Meeting transcription, closed captions, audio analytics with speaker diarization
- For subtitle file generation, [Subtitle Generation](../subtitle-generation/) is recommended

## How It Works

The API is called with `word_timestamps=true` and `diarize=true`. The response includes:

### Utterances (Speaker Diarization)

Each utterance groups words by speaker:

```json
{
  "speaker": "speaker_0",
  "start": 0.0,
  "end": 2.5,
  "text": "Hello, how are you?"
}
```

### Word Timestamps

Each word includes timing and optional speaker info:

```json
{
  "word": "hello",
  "start": 0.0,
  "end": 0.45,
  "speaker": "speaker_0"
}
```

## Configuration

| Feature | Value | Description |
|---------|-------|-------------|
| `language` | `en` | Language code (ISO 639-1) |
| `word_timestamps` | `true` | Enable word-level timestamps |
| `diarize` | `true` | Enable speaker diarization |

## Example Output

- Console output with transcription, utterances, and word timestamps
- `{filename}_result.json` â€” Full API response

## API Reference

- [Pre-recorded Quickstart](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/pre-recorded/quickstart)
- [Response Format](https://waves-docs.smallest.ai/v4.0.0/content/speech-to-text-new/realtime/response-format)
- [Pulse STT API Reference](https://waves-docs.smallest.ai/v4.0.0/content/api-references/pulse-asr)

## Next Steps

- [Subtitle Generation](../subtitle-generation/) â€” Turn word timestamps into SRT/VTT subtitle files
- [File Transcription](../file-transcription/) â€” Add emotions, age, gender detection



================================================
FILE: speech-to-text/word-level-outputs/.env.sample
================================================
# Smallest AI API Key
# Get yours at https://smallest.ai/console
SMALLEST_API_KEY=your-smallest-api-key-here



================================================
FILE: speech-to-text/word-level-outputs/javascript/transcribe.js
================================================
#!/usr/bin/env node
/**
 * Smallest AI Speech-to-Text - Word-Level Outputs
 *
 * Transcribe audio with word-level timestamps and speaker diarization,
 * showing timing and speaker information for each word and utterance.
 *
 * Usage: node transcribe.js <audio_file>
 *
 * Output:
 * - Command line response with word timestamps and utterances
 * - {filename}_result.json - Full result with words and utterances
 */

const fs = require("fs");
const path = require("path");

const API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text";

const LANGUAGE = "en"; // Use ISO 639-1 codes or "multi" for auto-detect
const WORD_TIMESTAMPS = true;
const DIARIZE = true;

async function transcribe(audioFile, apiKey) {
  const audioData = fs.readFileSync(audioFile);

  const params = new URLSearchParams({
    language: LANGUAGE,
    word_timestamps: String(WORD_TIMESTAMPS).toLowerCase(),
    diarize: String(DIARIZE).toLowerCase(),
  });

  const response = await fetch(`${API_URL}?${params}`, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/octet-stream",
    },
    body: audioData,
  });

  if (!response.ok) {
    throw new Error(`API request failed with status ${response.status}: ${await response.text()}`);
  }

  return response.json();
}

// This function is designed to process feature outputs ONLY for word-level timestamps and speaker diarization
function processResponse(result, audioPath) {
  if (result.status !== "success") {
    console.error("Error: Transcription failed");
    console.error(result);
    process.exit(1);
  }

  console.log("");
  console.log("=".repeat(60));
  console.log("TRANSCRIPTION");
  console.log("=".repeat(60));
  console.log(result.transcription || "");

  // Utterances (speaker diarization)
  const utterances = result.utterances || [];
  if (utterances.length > 0) {
    console.log("");
    console.log("-".repeat(60));
    console.log("UTTERANCES");
    console.log("-".repeat(60));
    for (const utt of utterances) {
      const speaker = utt.speaker || "";
      const start = (utt.start || 0).toFixed(2);
      const end = (utt.end || 0).toFixed(2);
      const text = utt.text || "";
      if (speaker) {
        console.log(`[${start}s - ${end}s] ${speaker}: ${text}`);
      } else {
        console.log(`[${start}s - ${end}s] ${text}`);
      }
    }
  }

  // Word timestamps
  const words = result.words || [];
  if (words.length > 0) {
    console.log("");
    console.log("-".repeat(60));
    console.log("WORD TIMESTAMPS");
    console.log("-".repeat(60));
    for (const word of words) {
      const start = (word.start || 0).toFixed(2);
      const end = (word.end || 0).toFixed(2);
      const text = word.word || "";
      const speaker = word.speaker || "";
      if (speaker) {
        console.log(`[${start}s - ${end}s] ${speaker}: ${text}`);
      } else {
        console.log(`[${start}s - ${end}s] ${text}`);
      }
    }
  }

  console.log("");
  console.log("=".repeat(60));

  const baseName = path.basename(audioPath, path.extname(audioPath));
  const jsonPath = `${baseName}_result.json`;
  fs.writeFileSync(jsonPath, JSON.stringify(result, null, 2));
  console.log(`Saved: ${jsonPath}`);
  console.log("Done!");
}

async function main() {
  const audioFile = process.argv[2];

  if (!audioFile) {
    console.log("Usage: node transcribe.js <audio_file>");
    process.exit(1);
  }

  const apiKey = process.env.SMALLEST_API_KEY;

  if (!apiKey) {
    console.error("Error: SMALLEST_API_KEY environment variable not set");
    process.exit(1);
  }

  if (!fs.existsSync(audioFile)) {
    console.error(`Error: File not found: ${audioFile}`);
    process.exit(1);
  }

  console.log(`Transcribing: ${path.basename(audioFile)}`);

  const result = await transcribe(audioFile, apiKey);
  processResponse(result, audioFile);
}

main();



================================================
FILE: speech-to-text/word-level-outputs/python/transcribe.py
================================================
#!/usr/bin/env python3
"""
Smallest AI Speech-to-Text - Word-Level Outputs

Transcribe audio with word-level timestamps and speaker diarization,
showing timing and speaker information for each word and utterance.

Usage: python transcribe.py <audio_file>

Output:
- Command line response with word timestamps and utterances
- {filename}_result.json - Full result with words and utterances
"""

import json
import os
import sys
from pathlib import Path

import requests
from dotenv import load_dotenv

load_dotenv()

API_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text"

LANGUAGE = "en"  # Use ISO 639-1 codes or "multi" for auto-detect
WORD_TIMESTAMPS = True
DIARIZE = True


def transcribe(audio_file: str, api_key: str) -> dict:
    with open(audio_file, "rb") as f:
        audio_data = f.read()

    response = requests.post(
        API_URL,
        headers={
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/octet-stream",
        },
        params={
            "language": LANGUAGE,
            "word_timestamps": str(WORD_TIMESTAMPS).lower(),
            "diarize": str(DIARIZE).lower(),
        },
        data=audio_data,
        timeout=300,
    )

    if response.status_code != 200:
        raise Exception(f"API request failed with status {response.status_code}: {response.text}")

    return response.json()

# This function is designed to process feature outputs ONLY for word-level timestamps and speaker diarization
def process_response(result: dict, audio_path: Path):
    if result.get("status") != "success":
        print("Error: Transcription failed")
        print(result)
        sys.exit(1)

    print("\n" + "=" * 60)
    print("TRANSCRIPTION")
    print("=" * 60)
    print(result.get("transcription", ""))

    # Utterances (speaker diarization)
    utterances = result.get("utterances", [])
    if utterances:
        print("\n" + "-" * 60)
        print("UTTERANCES")
        print("-" * 60)
        for utt in utterances:
            speaker = utt.get("speaker", "")
            start = utt.get("start", 0)
            end = utt.get("end", 0)
            text = utt.get("text", "")
            if speaker:
                print(f"[{start:.2f}s - {end:.2f}s] {speaker}: {text}")
            else:
                print(f"[{start:.2f}s - {end:.2f}s] {text}")

    # Word timestamps
    words = result.get("words", [])
    if words:
        print("\n" + "-" * 60)
        print("WORD TIMESTAMPS")
        print("-" * 60)
        for word in words:
            start = word.get("start", 0)
            end = word.get("end", 0)
            text = word.get("word", "")
            speaker = word.get("speaker", "")
            if speaker:
                print(f"[{start:.2f}s - {end:.2f}s] {speaker}: {text}")
            else:
                print(f"[{start:.2f}s - {end:.2f}s] {text}")

    print("\n" + "=" * 60)

    json_path = Path(".") / f"{audio_path.stem}_result.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"Saved: {json_path}")
    print("Done!")


def main():
    if len(sys.argv) < 2:
        print("Usage: python transcribe.py <audio_file>")
        sys.exit(1)

    audio_file = sys.argv[1]
    api_key = os.environ.get("SMALLEST_API_KEY")

    if not api_key:
        print("Error: SMALLEST_API_KEY environment variable not set")
        sys.exit(1)

    audio_path = Path(audio_file)
    if not audio_path.exists():
        print(f"Error: File not found: {audio_file}")
        sys.exit(1)

    print(f"Transcribing: {audio_path.name}")

    result = transcribe(audio_file, api_key)
    process_response(result, audio_path)


if __name__ == "__main__":
    main()



================================================
FILE: speech-to-text/youtube-summarizer/README.md
================================================
# YouTube Summarizer

![Demo](demo.png)

A lightning-fast tool to transcribe and summarize YouTube videos or uploaded audio files. Powered by **Smallest AI Pulse** for ultra-low latency transcription and **Groq** for instant summarization.

## Features

- **Pure Speed**: Leverages Pulse STT for sub-200ms transcription latency
- **Smart Summaries**: Uses Groq (Llama 3) to extract executive summaries and key takeaways
- **Flexible Input**: Supports both YouTube URLs and direct file uploads (MP3, MP4, WAV)
- **Latency Metrics**: Visualizes the precise time taken for STT (Network) vs LLM (Processing)

## Requirements

> Base dependencies are installed via the root `requirements.txt`. See the [main README](../../README.md#usage) for setup. Add `SMALLEST_API_KEY` and `GROQ_API_KEY` to your `.env`.

Extra dependencies:

```bash
uv pip install -r requirements.txt
```

## Usage

```bash
uv run streamlit run app.py
```

## Recommended Usage

- Visual web app for quickly transcribing and summarizing YouTube videos
- Comparing STT vs LLM latency with built-in speed metrics
- For CLI-based summarization of podcast files, [Podcast Summarizer](../podcast-summarizer/) is recommended

## How It Works

1. **Extraction**: `yt-dlp` extracts audio from the YouTube video (or reads uploaded bytes)
2. **Transcription**: **Pulse STT** receives the raw audio stream and returns text in milliseconds
3. **Analysis**: **Groq** processes the transcript to generate a structured summary and "Value Density" score
4. **Display**: Results are rendered instantly with a focus on speed metrics

## Supported Formats

- **YouTube Links**: Standard Video URLs
- **Uploads**: MP3, WAV, M4A, MP4, MOV

## Customization

You can modify `process_pulse_metadata` to re-enable paralinguistic features (Age, Gender, Emotion) if needed, which Pulse supports natively.

## API Reference

- [Pulse STT API Reference](https://waves-docs.smallest.ai/v4.0.0/content/api-references/pulse-asr)
- [Groq API Docs](https://console.groq.com/docs/api-reference)

## Next Steps

- [Podcast Summarizer](../podcast-summarizer/) â€” CLI-based transcribe-and-summarize with GPT-5
- [Streaming Transcription](../websocket/streaming-text-output-transcription/) â€” Stream audio via WebSocket for real-time results



================================================
FILE: speech-to-text/youtube-summarizer/analysis.py
================================================
import os
import json
from groq import Groq
from dotenv import load_dotenv

load_dotenv()

GROQ_API_KEY = os.getenv("GROQ_API_KEY")

def get_groq():
    if not GROQ_API_KEY: return None
    return Groq(api_key=GROQ_API_KEY)

def analyze_transcript(text):
    """Send transcript to Groq for summarization."""
    client = get_groq()
    if not client: return None
    
    prompt = f"""
    Summarize this YouTube video transcript.
    
    Transcript Snippet: "{text[:15000]}"
    
    Return JSON with:
    1. 'summary': A punchy 3-sentence summary.
    2. 'key_points': List of 5 actionable bullet points (strings).
    3. 'rating': A score 1-10 on "Value Density".
    """
    
    try:
        resp = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="openai/gpt-oss-120b",
            response_format={"type": "json_object"}
        )
        return json.loads(resp.choices[0].message.content)
    except Exception as e:
        return None



================================================
FILE: speech-to-text/youtube-summarizer/app.py
================================================
"""
YouTube Summarizer (YT Summarizer)
Powered by Smallest AI Pulse + Groq + yt-dlp
"""
import streamlit as st
import time
import os
from dotenv import load_dotenv

# Helpers
# Helpers
import requests
from youtube import get_video_info, download_audio
from transcription import transcribe_bytes
from analysis import analyze_transcript

@st.cache_resource
def get_api_session():
    return requests.Session()

load_dotenv()

st.set_page_config(page_title="ðŸ“º YT Summarizer", page_icon="âš¡", layout="wide")

# Custom Styles
st.markdown("""
<style>
    .stApp { background-color: #ffffff; color: #333333; }
    .summary-card {
        background: #f8f9fa;
        border-left: 4px solid #00ff9d;
        border-radius: 8px;
        padding: 20px;
        margin-top: 20px;
    }
    button[kind="primary"] {
        background: linear-gradient(90deg, #111 0%, #333 100%);
        color: white !important;
        border: none;
    }
    img { border-radius: 12px; }
</style>
""", unsafe_allow_html=True)

# Header
st.markdown("""
<div style="display: flex; align-items: center; margin-bottom: 20px;">
    <h1 style="margin: 0; font-size: 2.5rem; font-weight: 800;">âš¡ YT Summarizer</h1>
</div>
""", unsafe_allow_html=True)
st.markdown("<p style='color: #666; font-size: 1.2rem;'>Powered by <b style='color: #00ff9d'>Smallest Pulse</b></p>", unsafe_allow_html=True)

# Input Tabs
tab_yt, tab_file = st.tabs(["ðŸ”— YouTube Link", "PY Upload File"])
url = None
uploaded_file = None

with tab_yt:
    url = st.text_input("Paste URL:", placeholder="https://youtube.com/watch?v=...", label_visibility="collapsed")
with tab_file:
    uploaded_file = st.file_uploader("Upload Audio/Video", type=['mp3', 'wav', 'mp4', 'm4a', 'mov'])

analyze_btn = st.button("ðŸ”¥ Summon", type="primary", use_container_width=True)

if analyze_btn:
    audio_bytes = None
    info = {} # Dummy info
    
    # 1. Acquire Media
    if uploaded_file:
        with st.spinner("Reading file..."):
            audio_bytes = uploaded_file.read()
            info = {'title': uploaded_file.name, 'uploader': 'Local File', 'thumbnail': None}
    elif url:
        with st.spinner("Fetching..."):
            info = get_video_info(url)
            if not info:
                st.error("Invalid URL"); st.stop()
        
        with st.spinner("Extracting Audio..."):
            try:
                audio_file = download_audio(url)
                with open(audio_file, "rb") as f:
                    audio_bytes = f.read()
            except Exception as e:
                st.error(f"Download blocked: {e}"); st.stop()
    else:
        st.warning("Please provide a URL or Upload a file."); st.stop()

    # Layout: Media Info
    if info.get('thumbnail'):
        c1, c2 = st.columns([1, 2])
        c1.image(info['thumbnail'], use_container_width=True)
        c2.subheader(info.get('title'))
        c2.caption(f"Source: **{info.get('uploader')}**")
    else:
        st.subheader(info.get('title'))
        st.caption(f"Source: **{info.get('uploader')}**")

    st.divider()
    step_box = st.empty()
    
    # 2. Pipeline
    try:
        # A. Transcribe (Pulse)
        step_box.info("ðŸ‘‚ Pulse is listening...")
        session = get_api_session()
        t0 = time.time()
        pulse_data = transcribe_bytes(audio_bytes, session=session)
        stt_time = time.time() - t0
        
        if "error" in pulse_data:
            step_box.error(pulse_data["error"]); st.stop()
            
        transcript_text = pulse_data.get('transcript') or pulse_data.get('text') or pulse_data.get('transcription')
        if not transcript_text:
             step_box.error(f"No transcript found. Keys: {list(pulse_data.keys())}"); st.stop()

        # B. Analyze (Groq)
        step_box.info("ðŸ§  Groq is summarizing...")
        analysis = analyze_transcript(transcript_text)
        
        step_box.empty()
        st.metric("Model Latency (Pulse)", f"{stt_time:.3f}s")
        st.caption("Pure Network Request Time")
        
        # 3. Results
        if analysis:
            st.markdown("### ðŸ“ Executive Summary")
            st.markdown(f"<div class='summary-card'>{analysis.get('summary')}</div>", unsafe_allow_html=True)
            
            st.markdown("### ðŸ—ï¸ Key Takeaways")
            for point in analysis.get('key_points', []):
                st.markdown(f"- {point}")
            
            with st.expander("ðŸ“œ Full Transcript (Raw)"):
                st.code(transcript_text, language="text")
                
        # Cleanup
        if url and not uploaded_file:
             try: os.remove("temp_audio.mp3")
             except: pass
        
    except Exception as e:
        step_box.error(f"Pipeline failed: {e}")



================================================
FILE: speech-to-text/youtube-summarizer/requirements.txt
================================================
# Extra dependencies for YouTube Summarizer (beyond root requirements.txt)
yt-dlp

# System dependency: ffmpeg (required for audio extraction)
# On Ubuntu/Debian: sudo apt install ffmpeg
# On macOS: brew install ffmpeg
# On Windows: winget install ffmpeg



================================================
FILE: speech-to-text/youtube-summarizer/transcription.py
================================================
import os
import requests
from dotenv import load_dotenv

load_dotenv()

SMALLEST_API_KEY = os.getenv("SMALLEST_API_KEY")
SMALLEST_STT_URL = "https://waves-api.smallest.ai/api/v1/pulse/get_text"

# Global session to reuse TCP connections (Keep-Alive)
session = requests.Session()

def transcribe_bytes(audio_bytes, session=None):
    """Send audio bytes to Pulse STT API."""
    if not SMALLEST_API_KEY: return {"error": "Error: No API Key"}
    
    headers = {
        "Authorization": f"Bearer {SMALLEST_API_KEY}",
        "Content-Type": "audio/mpeg"
    }
    # Pure STT for maximum speed
    params = {
        "model": "pulse",
        "language": "en"
    }
    
    try:
        requester = session if session else requests
        response = requester.post(
            SMALLEST_STT_URL, 
            headers=headers, 
            params=params, 
            data=audio_bytes
        )
        
        # DEBUG PRINT TO TERMINAL (Optional, can be removed for production)
        # print(f"DEBUG: Pulse Status Code: {response.status_code}")

        if response.status_code == 200:
            return response.json()
        return {"error": f"Error {response.status_code}: {response.text}"}
    except Exception as e:
        return {"error": str(e)}



================================================
FILE: speech-to-text/youtube-summarizer/youtube.py
================================================
import os
import yt_dlp

def get_video_info(url):
    """Fetch video metadata without downloading."""
    ydl_opts = {'quiet': True, 'no_warnings': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            return info
        except Exception as e:
            return None

def download_audio(url):
    """Download video audio to a temporary MP3 file."""
    output_filename = "temp_audio"
    # Remove existing
    for ext in ['mp3', 'm4a', 'webm', 'wav']:
        try: os.remove(f"{output_filename}.{ext}")
        except: pass

    ydl_opts = {
        'format': 'bestaudio/best',
        'outtmpl': output_filename,
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192',
        }],
        'quiet': True,
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
    
    return f"{output_filename}.mp3"



================================================
FILE: voice-agents/README.md
================================================
# Voice Agents Cookbook

Build AI voice agents with the [Atoms SDK](https://atoms-docs.smallest.ai/dev).

## Basics

| Example | What You'll Learn |
|---------|-------------------|
| [getting_started](./getting_started/) | `OutputAgentNode`, `generate_response()`, `AtomsApp` |
| [agent_with_tools](./agent_with_tools/) | `@function_tool`, `ToolRegistry`, tool execution |
| [call_control](./call_control/) | `SDKAgentEndCallEvent`, cold/warm transfers |

## Multi-Node Patterns

| Example | What You'll Learn |
|---------|-------------------|
| [background_agent](./background_agent/) | `BackgroundAgentNode`, parallel nodes, cross-node state |
| [observability](./observability/) | Langfuse integration via `BackgroundAgentNode` â€” live traces, tool spans, transcript events |
| [language_switching](./language_switching/) | `add_edge()`, custom nodes, event pipelines |

## Call Handling

| Example | What You'll Learn |
|---------|-------------------|
| [inbound_ivr](./inbound_ivr/) | Intent routing, department transfers, mute/unmute |
| [interrupt_control](./interrupt_control/) | Mute/unmute events, blocking interruptions |

## Advanced

| Example | What You'll Learn |
|---------|-------------------|
| [bank_csr](./bank_csr/) | Multi-round tool chaining, real SQLite DB, deterministic computation, audit logging, banking actions |
| [atoms_sdk_web_agent](./atoms_sdk_web_agent/) | Multi-agent collaboration, tool calling, Next.js integration |

## Platform Features

| Example | What You'll Learn |
|---------|-------------------|
| [knowledge_base_rag](./knowledge_base_rag/) | KB creation, PDF upload, URL scraping |
| [campaigns](./campaigns/) | Audiences, outbound campaigns |
| [analytics](./analytics/) | Call logs, transcripts, post-call metrics |

## Quick Start

### Step 1: Create env + install base deps (once, from repo root)

```bash
uv venv
uv pip install -r requirements.txt
```

### Step 2: Run an example

```bash
uv pip install -r voice-agents/getting_started/requirements.txt
uv run voice-agents/getting_started/app.py
```

For another example:

```bash
uv pip install -r voice-agents/bank_csr/requirements.txt
uv run voice-agents/bank_csr/app.py
```

### Step 3: Connect via CLI

```bash
smallestai agent chat
```

### API Keys

```bash
export SMALLEST_API_KEY=your_key
export OPENAI_API_KEY=your_openai_key
```

## Requirements

- [uv](https://docs.astral.sh/uv/) (Python package manager)
- Python 3.10+
- `smallestai` SDK (`>=4.3.0`)
- OpenAI API key (for LLM)
- Smallest API key (for platform features)



================================================
FILE: voice-agents/agent_with_tools/README.md
================================================
# Agent with Tools

Build an agent with custom function tools that the LLM can call.

## Features

- **@function_tool decorator** for defining tools
- **ToolRegistry** for auto-discovering and managing tools
- **Tool execution** in `generate_response` with parallel support
- **Intermediate feedback pattern** â€” speak while tools execute to avoid silence

## Demo

**Weather Query**:
```
User: What's the weather in Tokyo?
Assistant: The weather in Tokyo is Clear, 68F.
```

**Book Appointment**:
```
User: Book a haircut for tomorrow at 2pm
Assistant: Booked haircut for 2024-01-16 at 14:00. Confirmation sent!
```

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage).

## Usage

```bash
uv pip install -r requirements.txt
uv run app.py
```

Connect with the CLI:

```bash
smallestai agent chat
```

## Recommended Usage

- When your agent needs to take actions â€” check weather, book appointments, look up data
- Learning the `@function_tool` decorator, `ToolRegistry`, and intermediate feedback pattern
- For call transfers and end-call handling, [Call Control](../call_control/) is recommended

## Key Snippets

### Define Tools with Decorator

```python
from smallestai.atoms.agent.tools import function_tool

@function_tool()
def get_weather(self, city: str) -> str:
    """Get the current weather for a city.
    
    Args:
        city: The city name to check weather for.
    """
    return f"The weather in {city} is sunny, 72Â°F"
```

### Register Tools in Agent

```python
from smallestai.atoms.agent.tools import ToolRegistry

class AssistantAgent(OutputAgentNode):
    def __init__(self):
        super().__init__(name="assistant-agent")
        
        # Initialize tool registry
        self.tool_registry = ToolRegistry()
        self.tool_registry.discover(self)  # Auto-discover @function_tool methods
        self.tool_schemas = self.tool_registry.get_schemas()
```

### Execute Tools in generate_response

```python
async def generate_response(self):
    response = await self.llm.chat(
        messages=self.context.messages,
        stream=True,
        tools=self.tool_schemas
    )
    
    tool_calls = []
    async for chunk in response:
        if chunk.content:
            yield chunk.content
        if chunk.tool_calls:
            tool_calls.extend(chunk.tool_calls)
    
    if tool_calls:
        # Provide immediate feedback (best practice!)
        yield "One moment while I check that for you. "
        
        results = await self.tool_registry.execute(tool_calls, parallel=True)
        # Add results to context and get final response
```

### Intermediate Feedback Pattern

Always provide feedback during tool execution to avoid awkward silence:

```python
if tool_calls:
    yield "One moment while I check that for you. "  # User hears this immediately
    results = await self.tool_registry.execute(tool_calls)  # May take time
    # Continue with response...
```

This is essential for voice agents where silence feels unnatural.

## Tools Included

| Tool | Description |
|------|-------------|
| `get_weather` | Get current weather for a city |
| `book_appointment` | Book an appointment |
| `list_appointments` | List scheduled appointments |
| `end_call` | End the call gracefully |

## Structure

```
agent_with_tools/
â”œâ”€â”€ app.py               # Server entry point
â””â”€â”€ assistant_agent.py   # Agent with tool definitions
```

## API Reference

- [Atoms SDK â€” Quick Start](https://atoms-docs.smallest.ai/dev/introduction/quickstart)
- [Core Concepts â€” Nodes](https://atoms-docs.smallest.ai/dev/introduction/core-concepts/nodes)

## Next Steps

- [Call Control](../call_control/) â€” End calls, cold/warm transfers
- [Getting Started](../getting_started/) â€” Review basic SDK usage



================================================
FILE: voice-agents/agent_with_tools/app.py
================================================
"""Agent with Tools Example - Custom function tools using decorator pattern."""

from assistant_agent import AssistantAgent
from loguru import logger

from smallestai.atoms.agent.events import SDKEvent, SDKSystemUserJoinedEvent
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession


async def setup_session(session: AgentSession):
    """Configure assistant agent with custom tools."""
    assistant = AssistantAgent()
    session.add_node(assistant)
    await session.start()

    @session.on_event("on_event_received")
    async def on_event_received(_, event: SDKEvent):
        logger.info(f"Event received: {event.type}")

        if isinstance(event, SDKSystemUserJoinedEvent):
            greeting = "Hello! I can check the weather, book appointments, and list your schedule. How can I help?"
            # Add to context so LLM knows conversation has started
            assistant.context.add_message({"role": "assistant", "content": greeting})
            await assistant.speak(greeting)

    await session.wait_until_complete()
    logger.success("Session complete")


if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()



================================================
FILE: voice-agents/agent_with_tools/assistant_agent.py
================================================
"""Assistant agent with tool calling capabilities."""

import os
from typing import List

from dotenv import load_dotenv

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.clients.types import ToolCall, ToolResult
from smallestai.atoms.agent.events import SDKAgentEndCallEvent
from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.tools import ToolRegistry, function_tool

load_dotenv()


class AssistantAgent(OutputAgentNode):
    """Assistant that can call tools to answer questions."""

    def __init__(self):
        super().__init__(name="assistant-agent")
        self.llm = OpenAIClient(
            model="gpt-4o-mini", 
            temperature=0.7, 
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # Appointments storage
        self.appointments = []

        # Initialize tool registry
        self.tool_registry = ToolRegistry()
        self.tool_registry.discover(self)
        self.tool_schemas = self.tool_registry.get_schemas()

        self.context.add_message({
            "role": "system",
            "content": """You are a helpful assistant that can:
1. Check the weather for any city
2. Book appointments for users
3. List upcoming appointments

Use the available tools when users ask about weather or appointments.
Be friendly and conversational.""",
        })

    async def generate_response(self):
        """Generate response with tool calling support."""

        response = await self.llm.chat(
            messages=self.context.messages, 
            stream=True, 
            tools=self.tool_schemas
        )

        tool_calls: List[ToolCall] = []
        full_response = ""

        async for chunk in response:
            if chunk.content:
                full_response += chunk.content
                yield chunk.content
            if chunk.tool_calls:
                tool_calls.extend(chunk.tool_calls)

        # Add assistant response to context (if no tool calls)
        if full_response and not tool_calls:
            self.context.add_message({"role": "assistant", "content": full_response})

        if tool_calls:
            # Provide immediate feedback while processing tools
            # This prevents awkward silence during tool execution
            yield "One moment while I check that for you. "
            
            results: List[ToolResult] = await self.tool_registry.execute(
                tool_calls=tool_calls, parallel=True
            )

            self.context.add_messages([
                {
                    "role": "assistant",
                    "content": "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments),
                            },
                        }
                        for tc in tool_calls
                    ],
                },
                *[
            {
                "role": "tool",
                "tool_call_id": tc.id,
                "content": "" if result.content is None else str(result.content),
            }
                    for tc, result in zip(tool_calls, results)
                ],
            ])

            final_response = await self.llm.chat(
                messages=self.context.messages, stream=True
            )

            async for chunk in final_response:
                if chunk.content:
                    yield chunk.content

    @function_tool()
    def get_weather(self, city: str) -> str:
        """Get the current weather for a city.
        
        Args:
            city: The city name to check weather for.
        """
        weather_data = {
            "new york": "Sunny, 72Â°F",
            "london": "Cloudy, 58Â°F",
            "tokyo": "Clear, 68Â°F",
            "paris": "Rainy, 55Â°F",
            "san francisco": "Foggy, 62Â°F",
        }
        city_lower = city.lower()
        if city_lower in weather_data:
            return f"The weather in {city} is {weather_data[city_lower]}"
        return f"The weather in {city} is partly cloudy, 65Â°F"

    @function_tool()
    def book_appointment(self, date: str, time: str, service: str) -> str:
        """Book an appointment for the user.
        
        Args:
            date: The date for the appointment (YYYY-MM-DD format)
            time: The time for the appointment (HH:MM format)
            service: The type of service to book
        """
        appointment = {"date": date, "time": time, "service": service}
        self.appointments.append(appointment)
        return f"Booked {service} for {date} at {time}. Confirmation sent!"

    @function_tool()
    def list_appointments(self) -> str:
        """List all scheduled appointments."""
        if not self.appointments:
            return "You have no upcoming appointments."
        
        result = "Your appointments:\n"
        for i, apt in enumerate(self.appointments, 1):
            result += f"{i}. {apt['service']} on {apt['date']} at {apt['time']}\n"
        return result

    @function_tool()
    async def end_call(self) -> None:
        """End the call when user says goodbye."""
        await self.send_event(SDKAgentEndCallEvent())
        return None



================================================
FILE: voice-agents/agent_with_tools/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/analytics/README.md
================================================
# Call Analytics

Scripts for retrieving and analyzing call data.

## Features

- **Call Logs** â€” Retrieve and filter call history
- **Call Details** â€” Get transcripts, recordings, and metadata
- **Post-Call Analytics** â€” Configure and retrieve AI-extracted metrics
- **Export** â€” Export transcripts for external analysis

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage). Add `SMALLEST_API_KEY` to your `.env`.

## Usage

```bash
uv pip install -r requirements.txt
```

### List Recent Calls

```bash
# Get last 10 calls
uv run get_calls.py

# Filter by agent
uv run get_calls.py --agent agent_123

# Filter by status
uv run get_calls.py --status completed

# Get more results
uv run get_calls.py --limit 50 --page 1
```

### Get Call Details

```bash
# Get full details including transcript
uv run get_call_details.py CALL-1768155029217-0bae45
```

### Configure Post-Call Analytics

```bash
# Show current configuration
uv run configure_post_call.py agent_123 --show

# Configure sample metrics
uv run configure_post_call.py agent_123
```

### Export Transcripts

```bash
# Export as text
uv run export_transcripts.py --output transcripts.txt

# Export as JSON
uv run export_transcripts.py --output transcripts.json --format json

# Filter and limit
uv run export_transcripts.py --agent agent_123 --limit 50 --output data.json
```

## Recommended Usage

- Analyzing call outcomes, exporting transcripts, and setting up post-call metrics for quality monitoring
- Configuring custom disposition metrics (ENUM, STRING, BOOLEAN, etc.)
- For real-time in-call analytics, [Background Agent](../background_agent/) is recommended

## Key Snippets

### Post-Call Analytics Configuration

Define custom metrics that are extracted from every call:

```python
from smallestai.atoms import Call

call = Call()

call.set_post_call_config(
    agent_id="agent_123",
    summary_prompt="Summarize the key points of this call.",
    disposition_metrics=[
        {
            "identifier": "outcome",
            "dispositionMetricPrompt": "What was the call outcome?",
            "dispositionValues": {"type": "ENUM"},
            "choices": ["Resolved", "Escalated", "Callback"]
        },
        {
            "identifier": "customer_name",
            "dispositionMetricPrompt": "What is the customer's name?",
            "dispositionValues": {"type": "STRING"}
        },
        {
            "identifier": "satisfied",
            "dispositionMetricPrompt": "Was the customer satisfied?",
            "dispositionValues": {"type": "BOOLEAN"}
        }
    ]
)
```

### Metric Types

| Type | Description | Example |
|------|-------------|---------|
| `STRING` | Free text | Customer name, product mentioned |
| `BOOLEAN` | True/False | Requires follow-up, issue resolved |
| `INTEGER` | Numeric | Callback count, satisfaction score |
| `ENUM` | Predefined choices | Outcome, sentiment category |
| `DATETIME` | Date/time | Callback scheduled time |

### Retrieving Analytics

After calls complete, analytics are included in call details:

```python
result = call.get_call("CALL-123")
analytics = result["data"]["postCallAnalytics"]

summary = analytics.get("summary")
disposition = analytics.get("disposition", {})

print(f"Outcome: {disposition.get('outcome')}")
print(f"Satisfied: {disposition.get('satisfied')}")
```

### Get Calls with Filters

```python
from smallestai.atoms import Call

call = Call()

# Get completed calls for an agent
result = call.get_calls(
    agent_id="agent_123",
    status="completed",
    limit=50,
    page=1
)

for c in result["data"]["calls"]:
    print(f"{c['callId']}: {c['duration']}s")
```

### Get Call Transcript

```python
result = call.get_call("CALL-123")
transcript = result["data"]["transcript"]

for entry in transcript:
    role = "Agent" if entry["role"] == "assistant" else "User"
    print(f"{role}: {entry['content']}")
```

### Search Calls by ID

```python
# Batch lookup
result = call.search_calls([
    "CALL-123",
    "CALL-456",
    "CALL-789"
])
```

## Call Data Fields

| Field | Description |
|-------|-------------|
| `callId` | Unique call identifier |
| `status` | completed, failed, in_progress, etc. |
| `type` | telephony_inbound, telephony_outbound, chat |
| `duration` | Call duration in seconds |
| `from` | Caller number |
| `to` | Called number |
| `transcript` | Array of {role, content} |
| `recordingUrl` | Mono recording URL |
| `recordingDualUrl` | Stereo recording URL |
| `callCost` | Cost breakdown |
| `postCallAnalytics` | AI-extracted metrics |

## Scripts Included

```
analytics/
â”œâ”€â”€ get_calls.py             # List calls with filtering options
â”œâ”€â”€ get_call_details.py      # Get detailed info for a specific call
â”œâ”€â”€ configure_post_call.py   # Set up post-call analytics
â””â”€â”€ export_transcripts.py    # Export transcripts to file
```

## Best Practices

1. **Filter Early** â€” Use query parameters to reduce data transfer
2. **Paginate** â€” Use page/limit for large datasets
3. **Configure Analytics** â€” Set up metrics before running calls
4. **Export Regularly** â€” Archive transcripts for long-term analysis
5. **Handle Missing Data** â€” Not all fields are always present

## API Reference

- [Analytics â€” Overview](https://atoms-docs.smallest.ai/dev/build/analytics/overview)

## Next Steps

- [Campaigns](../campaigns/) â€” Outbound call management
- [Background Agent](../background_agent/) â€” Real-time in-call analytics



================================================
FILE: voice-agents/analytics/configure_post_call.py
================================================
"""
Configure Post-Call Analytics Script

Set up custom summary prompts and disposition metrics for an agent.

Usage:
    python configure_post_call.py <agent_id>
    python configure_post_call.py <agent_id> --show
"""

import os
import sys
from dotenv import load_dotenv
from smallestai.atoms import Call

load_dotenv()


def show_config(agent_id: str):
    """Display current post-call configuration."""
    call = Call()
    
    print(f"Getting post-call config for agent: {agent_id}\n")
    
    result = call.get_post_call_config(agent_id)
    data = result.get("data", {})
    
    print("Current Configuration:")
    print("-" * 40)
    
    summary_prompt = data.get("summaryPrompt", "Default")
    print(f"Summary Prompt: {summary_prompt}")
    
    metrics = data.get("dispositionMetrics", [])
    if metrics:
        print(f"\nDisposition Metrics ({len(metrics)}):")
        for m in metrics:
            print(f"  - {m.get('identifier')}: {m.get('dispositionValues', {}).get('type')}")
            if m.get('choices'):
                print(f"    Choices: {', '.join(m['choices'])}")
    else:
        print("\nNo disposition metrics configured")


def configure_sample(agent_id: str):
    """Configure sample post-call analytics."""
    call = Call()
    
    print(f"Configuring post-call analytics for: {agent_id}\n")
    
    # Define custom summary prompt
    summary_prompt = """Summarize this call in 2-3 sentences, focusing on:
1. The main reason for the call
2. Whether the issue was resolved
3. Any follow-up actions needed"""
    
    # Define disposition metrics
    disposition_metrics = [
        {
            "identifier": "call_outcome",
            "dispositionMetricPrompt": "What was the outcome of this call?",
            "dispositionValues": {"type": "ENUM"},
            "choices": ["Issue Resolved", "Escalated", "Callback Requested", "No Resolution"]
        },
        {
            "identifier": "customer_sentiment",
            "dispositionMetricPrompt": "What was the customer's overall sentiment?",
            "dispositionValues": {"type": "ENUM"},
            "choices": ["Positive", "Neutral", "Negative"]
        },
        {
            "identifier": "product_mentioned",
            "dispositionMetricPrompt": "Which product was discussed?",
            "dispositionValues": {"type": "STRING"}
        },
        {
            "identifier": "requires_followup",
            "dispositionMetricPrompt": "Does this call require a follow-up?",
            "dispositionValues": {"type": "BOOLEAN"}
        }
    ]
    
    result = call.set_post_call_config(
        agent_id=agent_id,
        summary_prompt=summary_prompt,
        disposition_metrics=disposition_metrics
    )
    
    print("âœ“ Post-call analytics configured")
    print("\nMetrics configured:")
    for m in disposition_metrics:
        print(f"  - {m['identifier']} ({m['dispositionValues']['type']})")
    
    print("\nThese metrics will be extracted from every call transcript.")


def main():
    """Main entry point."""
    if len(sys.argv) < 2:
        print("Usage: python configure_post_call.py <agent_id> [--show]")
        print("\nOptions:")
        print("  --show    Show current configuration only")
        print("\nExample:")
        print("  python configure_post_call.py agent_123 --show")
        print("  python configure_post_call.py agent_123")
        sys.exit(1)
    
    agent_id = sys.argv[1]
    
    if "--show" in sys.argv:
        show_config(agent_id)
    else:
        configure_sample(agent_id)


if __name__ == "__main__":
    main()



================================================
FILE: voice-agents/analytics/export_transcripts.py
================================================
"""
Export Transcripts Script

Export call transcripts to a file for analysis.

Usage:
    python export_transcripts.py --output transcripts.txt
    python export_transcripts.py --agent <agent_id> --output transcripts.txt
    python export_transcripts.py --limit 50 --output transcripts.json
"""

import os
import sys
import json
import argparse
from datetime import datetime
from dotenv import load_dotenv
from smallestai.atoms import Call

load_dotenv()


def main():
    """Export transcripts to a file."""
    
    parser = argparse.ArgumentParser(description="Export call transcripts")
    parser.add_argument("--agent", help="Filter by agent ID")
    parser.add_argument("--campaign", help="Filter by campaign ID")
    parser.add_argument("--status", default="completed", help="Filter by status (default: completed)")
    parser.add_argument("--limit", type=int, default=20, help="Number of calls to export (default: 20)")
    parser.add_argument("--output", required=True, help="Output file path")
    parser.add_argument("--format", choices=["txt", "json"], default="txt", help="Output format")
    
    args = parser.parse_args()
    
    call = Call()
    
    print(f"Fetching up to {args.limit} calls...")
    
    # Get call list
    result = call.get_calls(
        agent_id=args.agent,
        campaign_id=args.campaign,
        status=args.status,
        limit=args.limit
    )
    
    calls = result.get("data", {}).get("logs", [])
    
    if not calls:
        print("No calls found.")
        return
    
    print(f"Found {len(calls)} calls. Fetching transcripts...")
    
    transcripts = []
    
    for i, c in enumerate(calls):
        call_id = c.get("callId")
        if not call_id:
            continue
        
        # Get full call details
        details = call.get_call(call_id)
        data = details.get("data", {})
        
        transcript = data.get("transcript", [])
        
        transcripts.append({
            "call_id": call_id,
            "date": data.get("createdAt", ""),
            "duration": data.get("duration", 0),
            "from": data.get("from", ""),
            "to": data.get("to", ""),
            "transcript": transcript,
            "summary": data.get("postCallAnalytics", {}).get("summary", "")
        })
        
        print(f"  [{i+1}/{len(calls)}] {call_id}")
    
    # Write output
    output_path = args.output
    
    if args.format == "json" or output_path.endswith(".json"):
        with open(output_path, "w") as f:
            json.dump(transcripts, f, indent=2)
    else:
        with open(output_path, "w") as f:
            for t in transcripts:
                f.write("=" * 60 + "\n")
                f.write(f"Call ID: {t['call_id']}\n")
                f.write(f"Date: {t['date']}\n")
                f.write(f"Duration: {t['duration']}s\n")
                f.write(f"From: {t['from']} -> To: {t['to']}\n")
                f.write("-" * 60 + "\n")
                
                for entry in t["transcript"]:
                    role = "Agent" if entry.get("role") == "agent" else "User"
                    content = entry.get("content", "")
                    f.write(f"{role}: {content}\n")
                
                if t["summary"]:
                    f.write("-" * 60 + "\n")
                    f.write(f"Summary: {t['summary']}\n")
                
                f.write("\n")
    
    print(f"\nâœ“ Exported {len(transcripts)} transcripts to {output_path}")


if __name__ == "__main__":
    main()



================================================
FILE: voice-agents/analytics/get_call_details.py
================================================
"""
Get Call Details Script

Retrieve detailed information about a specific call including transcript.

Usage:
    python get_call_details.py <call_id>
"""

import os
import sys
from dotenv import load_dotenv
from smallestai.atoms import Call

load_dotenv()


def main():
    """Get detailed information about a call."""
    
    if len(sys.argv) < 2:
        print("Usage: python get_call_details.py <call_id>")
        print("Example: python get_call_details.py CALL-1768155029217-0bae45")
        sys.exit(1)
    
    call_id = sys.argv[1]
    
    call = Call()
    
    print(f"Fetching details for: {call_id}\n")
    
    result = call.get_call(call_id)
    data = result.get("data", {})
    
    # Basic info
    print("=" * 60)
    print("CALL DETAILS")
    print("=" * 60)
    print(f"Call ID:      {data.get('callId', 'N/A')}")
    print(f"Status:       {data.get('status', 'N/A')}")
    print(f"Type:         {data.get('type', 'N/A')}")
    print(f"Duration:     {data.get('duration', 0)} seconds")
    print(f"From:         {data.get('from', 'N/A')}")
    print(f"To:           {data.get('to', 'N/A')}")
    print(f"Agent ID:     {data.get('agentId', 'N/A')}")
    print(f"Campaign ID:  {data.get('campaignId', 'N/A') or 'None'}")
    
    # Cost
    cost = data.get("callCost", 0)
    if cost:
        print(f"\nCost:         ${cost:.4f}")
    
    # Recording URLs
    print("\n" + "=" * 60)
    print("RECORDINGS")
    print("=" * 60)
    recording = data.get("recordingUrl")
    recording_dual = data.get("recordingDualUrl")
    
    if recording:
        print(f"Recording:      {recording}")
    if recording_dual:
        print(f"Stereo:         {recording_dual}")
    if not recording and not recording_dual:
        print("No recordings available")
    
    # Transcript
    print("\n" + "=" * 60)
    print("TRANSCRIPT")
    print("=" * 60)
    transcript = data.get("transcript", [])
    
    if transcript:
        for entry in transcript:
            role = entry.get("role", "unknown")
            content = entry.get("content", "")
            role_display = "Agent" if role == "agent" else "User"
            print(f"{role_display}: {content}")
    else:
        print("No transcript available")
    
    # Post-call analytics (if available)
    analytics = data.get("postCallAnalytics", {})
    if analytics:
        print("\n" + "=" * 60)
        print("POST-CALL ANALYTICS")
        print("=" * 60)
        
        summary = analytics.get("summary")
        if summary:
            print(f"Summary: {summary}")
        
        disposition = analytics.get("disposition", {})
        if disposition:
            print("\nDisposition Metrics:")
            for key, value in disposition.items():
                print(f"  {key}: {value}")


if __name__ == "__main__":
    main()



================================================
FILE: voice-agents/analytics/get_calls.py
================================================
"""
Get Call Logs Script

Retrieve and display call history with filtering.

Usage:
    python get_calls.py
    python get_calls.py --agent <agent_id>
    python get_calls.py --status completed
    python get_calls.py --limit 20
"""

import os
import sys
import argparse
from datetime import datetime
from dotenv import load_dotenv
from smallestai.atoms import Call

load_dotenv()


def format_duration(seconds):
    """Format duration in human-readable format."""
    if not seconds:
        return "N/A"
    minutes = int(seconds) // 60
    secs = int(seconds) % 60
    return f"{minutes}m {secs}s"


def format_timestamp(timestamp_str):
    """Format timestamp for display."""
    if not timestamp_str:
        return "N/A"
    try:
        dt = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
        return dt.strftime("%Y-%m-%d %H:%M")
    except:
        return timestamp_str[:16]


def main():
    """Get and display call logs."""
    
    parser = argparse.ArgumentParser(description="Get call logs")
    parser.add_argument("--agent", help="Filter by agent ID")
    parser.add_argument("--campaign", help="Filter by campaign ID")
    parser.add_argument("--status", help="Filter by status (completed, failed, in_progress)")
    parser.add_argument("--type", help="Filter by type (telephony_inbound, telephony_outbound, chat)")
    parser.add_argument("--search", help="Search by call ID, from, or to number")
    parser.add_argument("--limit", type=int, default=10, help="Number of results (default: 10)")
    parser.add_argument("--page", type=int, default=1, help="Page number (default: 1)")
    
    args = parser.parse_args()
    
    call = Call()
    
    print("Fetching call logs...")
    
    result = call.get_calls(
        agent_id=args.agent,
        campaign_id=args.campaign,
        status=args.status,
        call_type=args.type,
        search=args.search,
        limit=args.limit,
        page=args.page
    )
    
    calls = result.get("data", {}).get("logs", [])
    total = result.get("data", {}).get("pagination", {}).get("total", len(calls))
    
    if not calls:
        print("No calls found.")
        return
    
    print(f"\nShowing {len(calls)} of {total} calls (page {args.page}):\n")
    print("-" * 80)
    
    for c in calls:
        call_id = c.get("callId", "N/A")
        status = c.get("status", "N/A")
        duration = format_duration(c.get("duration"))
        call_type = c.get("type", "N/A")
        from_num = c.get("from", "N/A")
        to_num = c.get("to", "N/A")
        created = format_timestamp(c.get("createdAt"))
        
        print(f"Call ID:   {call_id}")
        print(f"Status:    {status}")
        print(f"Type:      {call_type}")
        print(f"Duration:  {duration}")
        print(f"From:      {from_num}")
        print(f"To:        {to_num}")
        print(f"Date:      {created}")
        print("-" * 80)


if __name__ == "__main__":
    main()



================================================
FILE: voice-agents/analytics/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/atoms_sdk_web_agent/README.md
================================================
# Multi-Agent Voice AI Dashboard

A real-time, multi-agent voice AI dashboard powered by **Smallest.ai Atoms SDK** and **Next.js**. It features high-performance voice interactions with specialized agents for gaming and utility.

[![Project Hero Banner](./public/project-hero-bannar.png)](https://agent-smallest-ai.vercel.app/)

## Getting Started

### 1. Create your Agents
First, you need to set up your voice agents on the Smallest.ai platform:
1. Visit [Smallest.ai Atoms](https://atoms.smallest.ai/).
2. Create three agents: one for Weather, one for Tic-Tac-Toe, and one for Hacker News.
3. Configure their voices, system prompts, and tools as detailed in the [`agents-config`](./agents-config) folder. Each agent requires specific tool definitions (API calls, parameters, and query mappings) to function correctly.

![Create Agent Guide](./public/create-agent.png)

### 2. Prerequisites
- Node.js (v20+)
- A Smallest.ai API Key

### 3. Installation
```bash
git clone https://github.com/smallest-ai/smallestai-cookbook.git
cd smallestai-cookbook/voice-agents/atoms_sdk_web_agent
npm install
```

### 4. Environment Setup
Create a `.env.local` file in the root directory:
```env
# Smallest.ai API Key
SMALLESTAI_API_KEY=your_api_key_here

# Agent IDs from Smallest.ai dashboard
TICTACTOE_AGENT_ID=your_tictactoe_agent_id
WEATHER_AGENT_ID=your_weather_agent_id
HACKERNEWS_AGENT_ID=your_hackernews_agent_id
```
> **Note:** The `TICTACTOE_AGENT_ID`, `WEATHER_AGENT_ID`, and `HACKERNEWS_AGENT_ID` are the unique IDs for the agents you created in Step 1.

### 5. Run Locally
```bash
npm run dev
```
Open [http://localhost:3000](http://localhost:3000).

## Smallest.ai Dashboard Configuration

### Tic-Tac-Toe Agent Tools
Configure these tools in your Smallest.ai dashboard for the Tic-Tac-Toe agent:

#### Tool: `new_game`
- **Method:** `POST`
- **URL:** [`/api/tictactoe/new-game`](./app/api/tictactoe/new-game/route.ts)

#### Tool: `make_move`
- **Method:** `POST`
- **URL:** [`/api/tictactoe/make-move`](./app/api/tictactoe/make-move/route.ts)
- **Parameters:** `gameId` (String), `position` (Number, 0-8)

#### Tool: `get_state`
- **Method:** `GET`
- **URL:** [`/api/tictactoe/get-state`](./app/api/tictactoe/get-state/route.ts)
- **Parameters:** `gameId` (String)

### Hacker News Agent Tools
Configure this tool in your Smallest.ai dashboard for the Hacker News agent:

#### Tool: `get_top_stories`
- **Method:** `GET`
- **URL:** [`/api/hackernews/top`](./app/api/hackernews/top/route.ts)

## Deployment

This project is optimized for deployment on **Vercel**:

1. Push your code to a GitHub repository.
2. Link the repository to a new project on Vercel.
3. Add your Environment Variables in the Vercel Dashboard.

## License
MIT License. Feel free to use and modify for your own voice-enabled applications!


================================================
FILE: voice-agents/atoms_sdk_web_agent/eslint.config.mjs
================================================
import { defineConfig, globalIgnores } from "eslint/config";
import nextVitals from "eslint-config-next/core-web-vitals";
import nextTs from "eslint-config-next/typescript";

const eslintConfig = defineConfig([
  ...nextVitals,
  ...nextTs,
  // Override default ignores of eslint-config-next.
  globalIgnores([
    // Default ignores of eslint-config-next:
    ".next/**",
    "out/**",
    "build/**",
    "next-env.d.ts",
  ]),
]);

export default eslintConfig;



================================================
FILE: voice-agents/atoms_sdk_web_agent/next.config.ts
================================================
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;



================================================
FILE: voice-agents/atoms_sdk_web_agent/package.json
================================================
{
  "name": "chess-smallest-ai",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "eslint"
  },
  "dependencies": {
    "@vercel/kv": "^3.0.0",
    "atoms-client-sdk": "^1.1.0",
    "eslint-plugin-import": "^2.32.0",
    "framer-motion": "^12.31.0",
    "lucide-react": "^0.563.0",
    "next": "16.1.6",
    "react": "19.2.3",
    "react-dom": "19.2.3",
    "uuid": "^13.0.0"
  },
  "devDependencies": {
    "@tailwindcss/postcss": "^4",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "16.1.6",
    "tailwindcss": "^4",
    "typescript": "^5"
  }
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/postcss.config.mjs
================================================
const config = {
  plugins: {
    "@tailwindcss/postcss": {},
  },
};

export default config;



================================================
FILE: voice-agents/atoms_sdk_web_agent/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    ".next/dev/types/**/*.ts",
    "**/*.mts"
  ],
  "exclude": ["node_modules"]
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/agents-config/hackernews-agent.md
================================================
# ðŸ“° Hacker News Agent Configuration

The Hacker News Agent keeps you updated with the latest tech trends, startups, and top discussions from the Hacker News community.

![Hacker News Agent Hero](../public/hackernews-agent.png)

## System Prompt
```text
You are a knowledgeable Tech News Curator. 
Your goal is to fetch and summarize the top stories from Hacker News.
When the user asks for "news", "top stories", or "what's trending", call the 'get_top_stories' tool.
Summarize the top 5 stories concisely and naturally.
```

## Tools & Functions

### Tool: `get_top_stories`
- **Description:** Get the top 5 trending stories from Hacker News.
- **Method:** `GET`
- **URL:** `${NEXT_APP_URL}/api/hackernews/top`
- **Response Variables:**
  - `stories`: An array containing story objects with `title`, `url`, `score`, and `by`.

---

## Technical Details
This agent interacts with the official Hacker News Firebase API via a serverless proxy route. It simplifies the multi-step process (fetching IDs then details) into a single call for the voice agent.



================================================
FILE: voice-agents/atoms_sdk_web_agent/agents-config/tic-tac-toe-agent.md
================================================
# Tic-Tac-Toe Agent Configuration

The Tic-Tac-Toe Agent is a highly interactive gaming AI that plays as 'O' while the user plays as 'X'.

![Tic Tac Toe Agent Hero](../public/tic-tac-toe-agent.png)

## System Prompt
```text
You are a Tic-Tac-Toe expert (playing as 'O'). The user is 'X'.
The board is a 3x3 grid numbered 0-8:
0 | 1 | 2
3 | 4 | 5
6 | 7 | 8

Instructions:
1. When the user says "Start", call 'new_game'. SAVE the 'gameId' from the result.
2. For all moves, call 'make_move' with the saved 'gameId' and the position (0-8).
3. The board updates are synced in real-time to the user's screen.
4. Announce the result naturally when the game ends!
```

## Tools & Functions

### Tool: `new_game`
- **Description:** Starts a new Tic Tac Toe game. Call this when the user says "Start", "Play", or "New Game". Returns the initial board and the gameId.
- **Method:** `POST`
- **URL:** `${NEXT_APP_URL}/api/tictactoe/new-game`
- **Request Body:** `{}`

### Tool: `make_move`
- **Description:** Places the user's move on the board (X) and calculates the AI's move (O). Takes a position number (0-8).
- **Method:** `POST`
- **URL:** `${NEXT_APP_URL}/api/tictactoe/make-move`
- **LLM Parameters:**
  - `gameId` (text, required): The ID of the current game returned by new_game.
  - `position` (number, required): The grid position to place X (0-8).
- **Request Body:**
  ```json
  {
    "gameId": "{{gameId}}",
    "position": "{{position}}"
  }
  ```

### Tool: `get_state`
- **Description:** Checks the current status of the board without making a move. Use this if the user asks "What does the board look like?" or "Whose turn is it?".
- **Method:** `GET`
- **URL:** `${NEXT_APP_URL}/api/tictactoe/get-state`
- **LLM Parameters:**
  - `gameId` (text, required): The ID of the current game.
- **Query Parameters:**
  - `gameId`: `{{gameId}}`

## Technical Details
This agent uses **Vercel KV (Redis)** to persist game data across serverless executions. The AI move logic is powered by a minimax algorithm integrated into the backend routes.



================================================
FILE: voice-agents/atoms_sdk_web_agent/agents-config/weather-agent.md
================================================
# Weather Agent Configuration

The Weather Agent provides real-time weather information and forecasts through a natural voice interface.

![Weather Agent Hero](../public/weather-agent.png)

## System Prompt
```text
You are a helpful and friendly Weather Assistant. 
Your goal is to provide accurate, real-time weather information to the user.
When a user asks about the weather in a specific city, use the 'get_current_weather' tool to fetch the data.
Always respond in a conversational and natural voice.
```

## Tools & Functions

### Tool: `get_current_weather`
- **Description:** Get current weather information for a specified city
- **Method:** `GET`
- **URL:** `https://api.openweathermap.org/data/2.5/weather`
- **LLM Parameters:**
  - `city` (text, required): The name of the city to get weather for
  - `units` (text, optional): Temperature units (metric, imperial, or standard)
- **Query Parameters:**
  - `q`: `{{city}}`
  - `appid`: `your_openweather_api_key`
  - `units`: `{{units}}`
- **Response Variables:**
  - `temperature`: `main.temp`
  - `description`: `weather[0].description`
  - `humidity`: `main.humidity`
  - `city_name`: `name`

## Technical Details
This agent handles tool calls by interacting with external weather APIs. The frontend provides a visual representation and ensures the voice interaction is smooth and low-latency.



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/globals.css
================================================
@import "tailwindcss";

:root {
  --background: #ffffff;
  --foreground: #171717;
}

@theme inline {
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --font-sans: var(--font-geist-sans);
  --font-mono: var(--font-geist-mono);
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: Arial, Helvetica, sans-serif;
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/layout.tsx
================================================
import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/page.tsx
================================================
'use client';

import { useState, useEffect, useCallback, useRef, useMemo } from "react";
import { AtomsClient } from "atoms-client-sdk";
import { WaveAvatar } from "./components/WaveAvatar"; // Assuming this is kept
import { motion, AnimatePresence } from "framer-motion";
import { Mic, MicOff, PhoneOff, CloudSun, Newspaper, Headset, Sparkles, ChevronRight, Gamepad2 } from "lucide-react";
import { TicTacToeBoard } from "./components/TicTacToeBoard";
// Mock Agents Data
const AGENTS = [
  {
    id: 'weather-agent',
    name: 'Weather Agent',
    role: 'Forecast & Updates',
    description: 'Get real-time weather updates and forecasts.',
    color: 'from-blue-500 to-cyan-400',
    icon: CloudSun
  },
  {
    id: 'tictactoe-agent',
    name: 'Tic Tac Toe',
    role: 'Game Opponent',
    description: 'Play Tic Tac Toe with voice commands.',
    color: 'from-purple-500 to-pink-400',
    icon: Gamepad2
  },
  {
    id: 'hackernews-agent',
    name: 'Hacker News',
    role: 'Tech News curator',
    description: 'Get the latest tech news and trends.',
    color: 'from-orange-500 to-yellow-400',
    icon: Newspaper
  }
];

interface AtomsVoiceChatProps {
  onError?: (error: string) => void;
  onTranscript?: (text: string, data: unknown) => void;
}

export default function Home({
  onError,
  onTranscript,
}: AtomsVoiceChatProps) {
  // Client Init
  const client = useMemo(() => {
    if (typeof window !== 'undefined') {
      return new AtomsClient();
    }
    return null as unknown as AtomsClient;
  }, []);

  // State
  const [selectedAgent, setSelectedAgent] = useState<typeof AGENTS[0] | null>(null);
  const [isConnected, setIsConnected] = useState(false);
  const [isConnecting, setIsConnecting] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isAgentTalking, setIsAgentTalking] = useState(false);
  const [messages, setMessages] = useState<any[]>([]);
  const [status, setStatus] = useState("");
  const [gameState, setGameState] = useState<{
    gameId: string | null;
    board: string[];
    status: 'in_progress' | 'x_wins' | 'o_wins' | 'draw';
    winner: 'X' | 'O' | null;
  } | null>(null);

  const mode = "webcall";
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const onErrorRef = useRef(onError);
  const onTranscriptRef = useRef(onTranscript);

  // Sync refs
  useEffect(() => { onErrorRef.current = onError; }, [onError]);
  useEffect(() => { onTranscriptRef.current = onTranscript; }, [onTranscript]);

  const resetAllStates = useCallback(() => {
    setIsConnected(false);
    setIsConnecting(false);
    setIsAgentTalking(false);
    setIsMuted(false);
    setStatus("");
    // Note: We don't nullify selectedAgent here to allow animation logic if needed,
    // but typically disconnecting goes back to home.
    setSelectedAgent(null);
  }, []);

  const forceReset = useCallback(() => {
    try {
      if (client) client.stopSession();
    } catch (e) { /* ignore */ }
    resetAllStates();
  }, [client, resetAllStates]);

  // Event Listeners
  const setupEventListeners = useCallback(() => {
    if (!client) return;
    client.removeAllListeners();

    client.on("session_started", () => {
      setIsConnected(true);
      setIsConnecting(false);
      setStatus("Session started");
    });

    client.on("session_ended", resetAllStates);

    client.on("agent_speaking_started", () => {
      setIsAgentTalking(true);
    });

    client.on("agent_speaking_stopped", () => {
      setIsAgentTalking(false);
    });

    client.on("transcript", (data: { text: string }) => {
      onTranscriptRef.current?.(data.text, data);
    });

    client.on("error", (error: string) => {
      console.error("Client error:", error);
      onErrorRef.current?.(error);
      forceReset();
    });

    // Listen for function call results (for Tic Tac Toe game updates)
    client.on("function_call_result", (data: any) => {
      console.log("Function call result:", data);

      // Check if this is a Tic Tac Toe function
      if (data.function_name === 'make_move' || data.function_name === 'new_game') {
        try {
          const result = typeof data.result === 'string' ? JSON.parse(data.result) : data.result;

          setGameState({
            gameId: result.gameId || null,
            board: result.board || ['', '', '', '', '', '', '', '', ''],
            status: result.status || 'in_progress',
            winner: result.winner || null,
          });
        } catch (error) {
          console.error("Error parsing function result:", error);
        }
      }
    });
  }, [client, resetAllStates, forceReset]);

  // Polling for talking state AND Game State
  useEffect(() => {
    if (!isConnected || !client) return;

    // Talk state polling
    const interval = setInterval(() => {
      if (client.isAgentTalking !== isAgentTalking) {
        setIsAgentTalking(client.isAgentTalking);
      }
    }, 50);

    // Game state polling (only if tictactoe agent)
    let gameInterval: any;
    if (selectedAgent?.id === 'tictactoe-agent') {
      gameInterval = setInterval(async () => {
        try {
          const res = await fetch('/api/tictactoe/get-state?latest=true');
          if (res.ok) {
            const data = await res.json();
            setGameState(data);
          }
        } catch (e) {
          // ignore errors during polling
        }
      }, 1000);
    }

    return () => {
      clearInterval(interval);
      if (gameInterval) clearInterval(gameInterval);
    };
  }, [isConnected, client, isAgentTalking, selectedAgent]);

  // Connect Logic
  const connect = async (agent: typeof AGENTS[0]) => {
    if (!client) return;
    setIsConnecting(true);

    // Setup before connecting
    setupEventListeners();

    try {
      // Fetch token - Using default agent ID from backend logic (works for all for demo)
      // or we could pass agent.id if backend handled it.
      const response = await fetch(`/api/invite-agent?mode=${mode}`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ agentId: agent.id }),
      });

      if (!response.ok) throw new Error("Failed to get token");
      const data = await response.json();

      await client.startSession({
        accessToken: data.data.token,
        mode: mode as any,
        host: data.data.host,
        emitRawAudioSamples: true,
      });

      if (mode === "webcall") {
        await client.startAudioPlayback();
      }

    } catch (error) {
      console.error(error);
      setIsConnecting(false);
      setSelectedAgent(null); // Go back if failed
    }
  };

  const handleAgentSelect = (agent: typeof AGENTS[0]) => {
    setSelectedAgent(agent);
    // Auto start
    connect(agent);
  };

  const disconnect = () => {
    forceReset();
  };

  const toggleMute = () => {
    if (!client) return;
    if (isMuted) {
      client.unmute();
      setIsMuted(false);
    } else {
      client.mute();
      setIsMuted(true);
    }
  };

  return (
    <div className="min-h-screen bg-black text-white font-sans selection:bg-blue-500/30">

      <AnimatePresence mode="wait">
        {!selectedAgent ? (
          // HOME VIEW: Agent Grid
          <motion.div
            key="home"
            initial={{ opacity: 0 }}
            animate={{ opacity: 1 }}
            exit={{ opacity: 0 }}
            className="max-w-6xl mx-auto px-6 min-h-screen flex flex-col justify-center"
          >
            <div className="mb-16 text-center">
              <h1 className="text-5xl font-bold bg-linear-to-r from-blue-400 to-purple-400 bg-clip-text text-transparent mb-6">
                Select Your AI Companion
              </h1>
              <p className="text-gray-400 text-xl max-w-2xl mx-auto">
                Choose an agent to start a real-time voice conversation.
                Experience the power of ultra-low latency interactions.
              </p>
            </div>

            <div className="flex justify-center">
              {AGENTS.map((agent) => (
                <motion.button
                  layoutId={`card-${agent.id}`}
                  key={agent.id}
                  onClick={() => handleAgentSelect(agent)}
                  className="group relative flex items-center gap-4 p-4 rounded-2xl bg-gray-900 border border-gray-800 hover:border-gray-700 transition-colors text-left max-w-md w-full overflow-hidden"
                  whileHover={{ y: -2, scale: 1.02 }}
                  whileTap={{ scale: 0.98 }}
                >
                  <div className={`absolute inset-0 opacity-0 group-hover:opacity-5 transition-opacity bg-linear-to-br ${agent.color}`} />

                  <div className={`p-3 rounded-xl bg-linear-to-br ${agent.color} shadow-lg shrink-0`}>
                    <agent.icon className="w-6 h-6 text-white" />
                  </div>

                  <div className="flex-1 min-w-0">
                    <h3 className="text-lg font-bold group-hover:text-blue-400 transition-colors truncate">
                      {agent.name}
                    </h3>
                    <p className="text-gray-500 text-xs truncate">
                      {agent.role}
                    </p>
                  </div>

                  <ChevronRight className="w-5 h-5 text-gray-600 group-hover:text-white transition-colors shrink-0" />
                </motion.button>
              ))}
            </div>
          </motion.div>
        ) : (
          // ACTIVE CALL VIEW
          <motion.div
            key="active"
            className="fixed inset-0 flex flex-col"
          >
            {/* HEADER */}
            <div className="absolute top-0 left-0 right-0 p-8 flex justify-end items-start z-50">
              {/* Agent Mini Card (Animated to top right) */}
              <motion.div
                layoutId={`card-${selectedAgent.id}`}
                className="flex items-center gap-4 p-2 pl-3 pr-6 rounded-full bg-gray-900/80 border border-gray-800 backdrop-blur-xl"
              >
                <div className={`p-2 rounded-full bg-linear-to-br ${selectedAgent.color}`}>
                  <selectedAgent.icon className="w-5 h-5 text-white" />
                </div>
                <div className="text-left">
                  <h3 className="font-bold text-sm leading-tight text-white">
                    {selectedAgent.name}
                  </h3>
                  <p className="text-[10px] uppercase font-bold text-gray-400 tracking-wider">
                    {isConnecting ? 'Connecting...' : 'Active Call'}
                  </p>
                </div>
              </motion.div>
            </div>

            {/* MAIN CONTENT CENTER */}
            <div className="flex-1 flex flex-col items-center justify-center relative -mt-16">

              {/* Layout Wrapper: Switches to row when game is active */}
              <div className={`flex items-center justify-center transition-all duration-500 ease-in-out ${selectedAgent?.id === 'tictactoe-agent' && (gameState?.board || (!gameState && !isConnecting))
                ? 'flex-col md:flex-row gap-12'
                : 'flex-col gap-0'
                }`}>

                {/* Avatar Section */}
                <div className="relative flex flex-col items-center">
                  {/* Status pulsing if connecting */}
                  {isConnecting && (
                    <div className="absolute inset-0 rounded-full animate-pulse bg-blue-500/20 blur-xl" />
                  )}

                  <WaveAvatar
                    isSpeaking={isAgentTalking}
                    isConnected={!isConnecting}
                    className="w-48 h-48 md:w-64 md:h-64"
                  />
                </div>

                {/* Tic Tac Toe Board Section */}
                {selectedAgent?.id === 'tictactoe-agent' && gameState && (
                  <motion.div
                    initial={{ opacity: 0, x: 20 }}
                    animate={{ opacity: 1, x: 0 }}
                    transition={{ delay: 0.3 }}
                    className="max-h-[400px] flex flex-col items-center gap-6 z-10"
                  >
                    <TicTacToeBoard
                      board={gameState.board}
                      status={gameState.status}
                      winner={gameState.winner}
                    />
                    {/* Debug/Manual Start Button */}
                    <button
                      onClick={async () => {
                        await fetch('/api/tictactoe/new-game', { method: 'POST' });
                      }}
                      className="text-xs text-gray-500 underline hover:text-white"
                    >
                      Force New Game
                    </button>
                  </motion.div>
                )}

                {/* Show Start Button if game not started but agent connected */}
                {selectedAgent?.id === 'tictactoe-agent' && (!gameState || !gameState.board) && !isConnecting && (
                  <motion.div
                    initial={{ opacity: 0 }}
                    animate={{ opacity: 1 }}
                    className="flex flex-col items-center justify-center"
                  >
                    <button
                      onClick={async () => {
                        await fetch('/api/tictactoe/new-game', { method: 'POST' });
                      }}
                      className="px-6 py-3 bg-blue-600 hover:bg-blue-500 rounded-full font-bold text-white shadow-lg transition-all whitespace-nowrap"
                    >
                      Start Game Board
                    </button>
                  </motion.div>
                )}
              </div>

              {/* Status Text - Centered below both */}
              {!isConnecting && (
                <motion.div
                  initial={{ opacity: 0, y: 10 }}
                  animate={{ opacity: 1, y: 0 }}
                  className="mt-8 text-center z-0"
                >
                  {/* Only show status if board isn't dominating the view, or make it smaller */}
                  <p className={`text-xl font-semibold transition-colors duration-300 ${isAgentTalking ? 'text-emerald-200' : 'text-gray-200'
                    }`}>
                    {isAgentTalking ? 'Speaking...' : 'Listening...'}
                  </p>
                  <p className="text-gray-500 text-sm mt-2">
                    {isAgentTalking ? 'Agent is responding' : 'Say something to continue'}
                  </p>
                </motion.div>
              )}

              {/* Connecting Text */}
              {isConnecting && (
                <motion.div
                  initial={{ opacity: 0, y: 10 }}
                  animate={{ opacity: 1, y: 0 }}
                  className="mt-8 text-blue-400 font-medium animate-pulse"
                >
                  Establishing secure connection...
                </motion.div>
              )}

              {/* Floating Controls */}
              <motion.div
                initial={{ opacity: 0, y: 50 }}
                animate={{ opacity: 1, y: 0 }}
                transition={{ delay: 0.3 }}
                className="absolute bottom-24 left-1/2 -translate-x-1/2 flex items-center gap-4"
              >
                <button
                  onClick={toggleMute}
                  className={`p-4 rounded-full transition-all duration-300 shadow-2xl ${isMuted
                    ? 'bg-white text-gray-900 hover:bg-gray-200'
                    : 'bg-gray-800 text-white hover:bg-gray-700 border border-gray-700'
                    }`}
                >
                  {isMuted ? <MicOff className="w-6 h-6" /> : <Mic className="w-6 h-6" />}
                </button>

                <button
                  onClick={disconnect}
                  className="p-4 rounded-full bg-red-500/10 text-red-500 hover:bg-red-500 hover:text-white border border-red-500/20 transition-all shadow-2xl backdrop-blur-md"
                >
                  <PhoneOff className="w-6 h-6" />
                </button>
              </motion.div>

            </div>

            {/* Background Ambience */}
            <div className="absolute inset-0 -z-10 overflow-hidden pointer-events-none">
              <div className={`absolute top-1/2 left-1/2 -translate-x-1/2 -translate-y-1/2 w-[800px] h-[800px] bg-linear-to-tr ${selectedAgent.color} opacity-10 blur-[120px] rounded-full`} />
            </div>

          </motion.div>
        )}
      </AnimatePresence>
    </div>
  );
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/api/hackernews/top/route.ts
================================================
import { NextResponse } from 'next/server';

export async function GET() {
    try {
        // Fetch top 500 story IDs
        const topStoriesRes = await fetch('https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty');
        if (!topStoriesRes.ok) throw new Error('Failed to fetch top stories');

        const topStoryIds = await topStoriesRes.json();
        const top5Ids = topStoryIds.slice(0, 5); // Just get the top 5 for the voice agent to read

        // Fetch details for each of the top 5 stories
        const storyDetails = await Promise.all(
            top5Ids.map(async (id: number) => {
                const res = await fetch(`https://hacker-news.firebaseio.com/v0/item/${id}.json?print=pretty`);
                return res.json();
            })
        );

        const stories = storyDetails.map(story => ({
            title: story.title,
            url: story.url,
            score: story.score,
            by: story.by,
        }));

        return NextResponse.json({ stories });
    } catch (error) {
        console.error("[API] Error fetching Hacker News:", error);
        return NextResponse.json(
            { error: 'Failed to fetch Hacker News' },
            { status: 500 }
        );
    }
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/api/invite-agent/route.ts
================================================
import { NextResponse } from 'next/server';

export async function POST(req: Request) {
    try {
        const { searchParams } = new URL(req.url);
        const mode = searchParams.get('mode');

        const body = await req.json();

        let { agentId } = body;

        // Map frontend IDs to Environment Variable IDs
        if (agentId === 'tictactoe-agent') {
            agentId = process.env.TICTACTOE_AGENT_ID;
            console.log('[API] Mapped tictactoe-agent to:', agentId);
        } else if (agentId === 'hackernews-agent') {
            agentId = process.env.HACKERNEWS_AGENT_ID;
            console.log('[API] Mapped hackernews-agent to:', agentId);
        } else if (agentId === 'weather-agent' || !agentId) {
            // Default or Weather
            agentId = process.env.WEATHER_AGENT_ID;
        }

        const apiKey = process.env.SMALLESTAI_API_KEY;

        // Validate agentId
        if (!agentId || typeof agentId !== 'string' || agentId.trim() === '') {
            return NextResponse.json(
                { error: 'agentId is required and must be a non-empty string' },
                { status: 400 }
            );
        }

        // Validate apiKey
        if (!apiKey || typeof apiKey !== 'string' || apiKey.trim() === '') {
            console.error("Missing SMALLESTAI_API_KEY in server environment variables.");
            return NextResponse.json(
                { error: 'Server misconfiguration: API key missing' },
                { status: 500 }
            );
        }

        const payload = { agentId, synthesizer: { voice_id: "voice_Q2yr65SMlu", model: "waves_lightning_v2", provider: "smallest" } };

        console.log(`[API] Received request for mode: ${mode}`);
        console.log(`[API] Payload:`, { agentId, apiKeyLength: apiKey?.length });

        const upstreamUrl = `https://atoms-api.smallest.ai/api/v1/conversation/${mode}`;
        console.log(`[API] Forwarding to: ${upstreamUrl}`);

        const response = await fetch(
            upstreamUrl,
            {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${apiKey}`,
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(payload),
            }
        );

        console.log(`[API] Upstream status: ${response.status} ${response.statusText}`);

        const data = await response.json();

        if (!response.ok) {
            console.error(`Error creating ${mode}:`, data || response.statusText);
            // Return detailed error to client for debugging
            return NextResponse.json({
                error: 'Upstream API error',
                details: data,
                status: response.status,
                upstreamUrl
            }, { status: response.status });
        }

        console.log(data);
        return NextResponse.json(data, { status: 201 });

    } catch (error: unknown) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        console.error(
            `Error creating conversation:`,
            errorMessage
        );
        return NextResponse.json(
            { error: `Failed to create conversation` },
            { status: 500 }
        );
    }
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/api/tictactoe/get-state/route.ts
================================================
import { NextResponse } from 'next/server';
import { getGame, getLatestGameId } from '@/app/lib/tictactoe-engine';

export async function GET(req: Request) {
    const { searchParams } = new URL(req.url);
    const latest = searchParams.get('latest');
    let gameId = searchParams.get('gameId');

    if (latest === 'true') {
        const latestId = getLatestGameId();
        if (latestId) {
            gameId = latestId;
        } else {
            return NextResponse.json(
                { error: 'No games started yet' },
                { status: 404 }
            );
        }
    }

    console.log(`[API] get-state request for gameId: ${gameId}`);

    if (!gameId) {
        return NextResponse.json(
            { error: 'gameId is required' },
            { status: 400 }
        );
    }

    const gameState = getGame(gameId);
    if (!gameState) {
        return NextResponse.json(
            { error: 'Game not found' },
            { status: 404 }
        );
    }

    return NextResponse.json(gameState);
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/api/tictactoe/make-move/route.ts
================================================
import { NextResponse } from 'next/server';
import { getGame, saveGame, checkWinner, makeComputerMove } from '@/app/lib/tictactoe-engine';

export async function POST(req: Request) {
    try {
        const body = await req.json();
        console.log("[API] make-move received body:", body);

        let { gameId, position } = body;

        // Validating input
        if (!gameId || typeof gameId !== 'string') {
            console.error("[API] Invalid gameId:", gameId);
            return NextResponse.json(
                { error: 'gameId is required' },
                { status: 400 }
            );
        }

        // Handle position as string or number
        if (typeof position === 'string') {
            position = parseInt(position, 10);
        }

        if (typeof position !== 'number' || isNaN(position) || position < 0 || position > 8) {
            console.error("[API] Invalid position:", position);
            return NextResponse.json(
                { error: 'position must be a number between 0 and 8' },
                { status: 400 }
            );
        }

        const gameState = getGame(gameId);
        if (!gameState) {
            console.error("[API] Game not found:", gameId);
            return NextResponse.json(
                { error: 'Game not found' },
                { status: 404 }
            );
        }

        if (gameState.status !== 'in_progress') {
            return NextResponse.json({
                ...gameState,
                message: `Game is already over. Winner: ${gameState.winner}`
            });
        }

        if (gameState.board[position] !== '') {
            return NextResponse.json(
                { error: 'Position already taken' },
                { status: 400 }
            );
        }

        // Player move (X)
        gameState.board[position] = 'X';

        // Check win
        const result = checkWinner(gameState.board);
        if (result) {
            gameState.status = result === 'Draw' ? 'draw' : 'x_wins';
            gameState.winner = result === 'Draw' ? null : 'X';
        } else {
            // Computer move (O)
            const computerMove = makeComputerMove(gameState.board);
            if (computerMove !== -1) {
                gameState.board[computerMove] = 'O';
                const cpuResult = checkWinner(gameState.board);
                if (cpuResult) {
                    gameState.status = cpuResult === 'Draw' ? 'draw' : 'o_wins';
                    gameState.winner = cpuResult === 'Draw' ? null : 'O';
                }
            }
        }

        saveGame(gameId, gameState);
        console.log(`[API] Move processed for ${gameId}. Status: ${gameState.status}`);

        return NextResponse.json({
            gameId,
            board: gameState.board,
            status: gameState.status,
            winner: gameState.winner,
            message: gameState.status === 'in_progress' ? 'Your turn' : `Game over! ${gameState.winner ? gameState.winner + ' wins!' : 'Draw!'}`
        });

    } catch (error) {
        console.error("[API] Error in make-move:", error);
        return NextResponse.json(
            { error: 'Internal server error' },
            { status: 500 }
        );
    }
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/api/tictactoe/new-game/route.ts
================================================
import { NextResponse } from 'next/server';
import { v4 as uuidv4 } from 'uuid';
import { saveGame } from '@/app/lib/tictactoe-engine';

export async function POST(req: Request) {
    console.log("[API] new-game request received");
    try {
        const gameId = uuidv4();
        const initialState = {
            board: ['', '', '', '', '', '', '', '', ''],
            status: 'in_progress' as const,
            winner: null,
            gameId: gameId
        };

        saveGame(gameId, initialState);
        console.log(`[API] Created new game: ${gameId}`);

        return NextResponse.json({
            gameId,
            board: initialState.board,
            status: initialState.status,
            message: "New game started! You are X. Speak your move (e.g., 'top left', 'middle center')."
        });
    } catch (error) {
        console.error("Error creating new game:", error);
        return NextResponse.json(
            { error: 'Failed to create game' },
            { status: 500 }
        );
    }
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/components/TicTacToeBoard.tsx
================================================
import { motion } from "framer-motion";
import { useEffect } from "react";

interface TicTacToeBoardProps {
    board: string[];
    status: 'in_progress' | 'x_wins' | 'o_wins' | 'draw';
    winner: 'X' | 'O' | null;
}

export function TicTacToeBoard({ board, status, winner }: TicTacToeBoardProps) {

    const getStatusText = () => {
        if (status === 'in_progress') return "Your Turn (Say 'Top Left', 'Middle', etc.)";
        if (status === 'draw') return "It's a Draw!";
        if (winner === 'X') return "You Win!";
        return "AI Wins!";
    };

    return (
        <div className="flex flex-col items-center">
            <h3 className="text-xs font-bold mb-4 text-white">{getStatusText()}</h3>

            <div className="grid grid-cols-3 gap-2 bg-gray-800 p-2 rounded-xl">
                {board.map((cell, index) => (
                    <motion.div
                        key={index}
                        initial={{ scale: 0.8, opacity: 0 }}
                        animate={{ scale: 1, opacity: 1 }}
                        className={`w-20 h-20 flex items-center justify-center text-4xl font-bold rounded-lg cursor-default
              ${cell === 'X' ? 'bg-blue-500/20 text-blue-400' :
                                cell === 'O' ? 'bg-purple-500/20 text-purple-400' :
                                    'bg-gray-700/50 hover:bg-gray-700'}`}
                    >
                        {cell}
                    </motion.div>
                ))}
            </div>

            {status !== 'in_progress' && (
                <motion.div
                    initial={{ opacity: 0 }}
                    animate={{ opacity: 1 }}
                    className="mt-4 text-sm text-gray-400"
                >
                    Result: {status.replace('_', ' ').toUpperCase()}
                </motion.div>
            )}
        </div>
    );
}



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/components/WaveAvatar.tsx
================================================
import React, { useEffect, useState, useRef } from "react";

interface WaveAvatarProps {
    isSpeaking: boolean;
    isConnected: boolean;
    className?: string;
}

export const WaveAvatar: React.FC<WaveAvatarProps> = ({
    isSpeaking,
    isConnected,
    className = "",
}) => {
    const [audioLevel, setAudioLevel] = useState(0);
    const animationFrameRef = useRef<number | null>(null);

    useEffect(() => {
        if (!isSpeaking) {
            setAudioLevel(0);
            if (animationFrameRef.current !== null) {
                cancelAnimationFrame(animationFrameRef.current);
                animationFrameRef.current = null;
            }
            return;
        }

        const updateAudioLevel = () => {
            // Simulate audio levels since we don't have direct stream access
            // Use a combination of sine waves to create a more organic "speaking" pattern
            const time = Date.now() / 150;
            const base = (Math.sin(time) + 1) / 2; // 0 to 1
            const variance = Math.random() * 0.3;

            const level = Math.min(base * 0.7 + variance + 0.2, 1);
            setAudioLevel(level);

            animationFrameRef.current = requestAnimationFrame(updateAudioLevel);
        };

        updateAudioLevel();

        return () => {
            if (animationFrameRef.current !== null) {
                cancelAnimationFrame(animationFrameRef.current);
            }
        };
    }, [isSpeaking]);

    const waveIntensity = isSpeaking ? audioLevel * 100 : 0;

    return (
        <div className={`relative flex items-center justify-center ${className}`}>
            {/* Animated wave rings - constrained within container */}
            {[1, 2, 3].map((ring) => (
                <div
                    key={ring}
                    className="absolute rounded-full border-2 border-white/30 pointer-events-none"
                    style={{
                        width: `${80 + (ring * 8)}%`,
                        height: `${80 + (ring * 8)}%`,
                        backgroundColor: 'rgba(127, 229, 184, 0.12)',
                        transform: `scale(${1 + (waveIntensity * 0.0002 * ring)})`,
                        opacity: isSpeaking ? 0.4 - (ring * 0.1) : 0,
                        transition: "transform 0.15s ease-out, opacity 0.2s ease-out",
                        animation: isSpeaking ? `pulse 2s ease-out ${ring * 0.3}s infinite` : 'none',
                    }}
                />
            ))}

            {/* Outer glow effect */}
            <div
                className="absolute inset-0 rounded-full blur-2xl pointer-events-none"
                style={{
                    background: `radial-gradient(circle, rgba(127, 229, 184, 0.35), rgba(91, 197, 206, 0.18))`,
                    transform: `scale(${1.3 + (waveIntensity * 0.0005)})`,
                    opacity: isSpeaking ? 0.5 : 0.2,
                    transition: "transform 0.15s ease-out, opacity 0.2s ease-out",
                }}
            />

            {/* Main avatar container */}
            <div
                className={`relative w-full h-full rounded-full flex items-center justify-center transition-all duration-300 border-[3px] border-white/80 shadow-xl`}
                style={{
                    background: `linear-gradient(135deg, #7FE5B8 0%, #6DD5C3 50%, #5BC5CE 100%)`,
                    transform: `scale(${1 + (waveIntensity * 0.0003)})`,
                    transition: "transform 0.1s ease-out",
                }}
            >
                {/* Inner circle */}
                <div
                    className={`w-[88%] h-[88%] rounded-full transition-all duration-300`}
                    style={{
                        background: `linear-gradient(135deg, #6DD5C3 0%, #5BC5CE 100%)`,
                        boxShadow: isSpeaking
                            ? `0 0 ${20 + waveIntensity * 0.5}px rgba(127, 229, 184, 0.7), inset 0 0 20px rgba(255, 255, 255, 0.2)`
                            : "0 0 12px rgba(127, 229, 184, 0.4), inset 0 0 15px rgba(255, 255, 255, 0.15)",
                        transition: "box-shadow 0.15s ease-out",
                    }}
                />
            </div>
        </div>
    );
};



================================================
FILE: voice-agents/atoms_sdk_web_agent/app/lib/tictactoe-engine.ts
================================================
export type Player = 'X' | 'O';
export type GameStatus = 'in_progress' | 'x_wins' | 'o_wins' | 'draw';

export interface GameState {
    board: string[]; // 9 cells
    status: GameStatus;
    winner: Player | null;
    gameId: string;
}

// In-memory storage (reset on server restart)
const games: Record<string, GameState> = {};
let latestGameId: string | null = null;

export function saveGame(gameId: string, state: GameState) {
    games[gameId] = state;
    latestGameId = gameId; // Track latest for demo polling
}

export function getLatestGameId(): string | null {
    return latestGameId;
}

export function getGame(gameId: string): GameState | undefined {
    return games[gameId];
}

export function checkWinner(board: string[]): Player | 'Draw' | null {
    const winningLines = [
        [0, 1, 2], [3, 4, 5], [6, 7, 8], // Rows
        [0, 3, 6], [1, 4, 7], [2, 5, 8], // Cols
        [0, 4, 8], [2, 4, 6]             // Diagonals
    ];

    for (const [a, b, c] of winningLines) {
        if (board[a] && board[a] === board[b] && board[a] === board[c]) {
            return board[a] as Player;
        }
    }

    if (board.every(cell => cell !== '')) {
        return 'Draw';
    }

    return null;
}

export function minimax(board: string[], depth: number, isMaximizing: boolean): number {
    const winner = checkWinner(board);
    if (winner === 'O') return 10 - depth;
    if (winner === 'X') return depth - 10;
    if (winner === 'Draw') return 0;

    if (isMaximizing) {
        let bestScore = -Infinity;
        for (let i = 0; i < 9; i++) {
            if (board[i] === '') {
                board[i] = 'O';
                const score = minimax(board, depth + 1, false);
                board[i] = '';
                bestScore = Math.max(score, bestScore);
            }
        }
        return bestScore;
    } else {
        let bestScore = Infinity;
        for (let i = 0; i < 9; i++) {
            if (board[i] === '') {
                board[i] = 'X';
                const score = minimax(board, depth + 1, true);
                board[i] = '';
                bestScore = Math.min(score, bestScore);
            }
        }
        return bestScore;
    }
}

export function makeComputerMove(board: string[]): number {
    // 1. First move optimization (Center or Corner) to save computation
    const emptyCount = board.filter(c => c === '').length;
    if (emptyCount >= 8) {
        if (board[4] === '') return 4;
        return 0;
    }

    let bestScore = -Infinity;
    let bestMove = -1;

    for (let i = 0; i < 9; i++) {
        if (board[i] === '') {
            board[i] = 'O';
            const score = minimax(board, 0, false);
            board[i] = '';
            if (score > bestScore) {
                bestScore = score;
                bestMove = i;
            }
        }
    }
    return bestMove;
}



================================================
FILE: voice-agents/background_agent/README.md
================================================
# Background Agent

Multi-node architecture with real-time sentiment analysis running alongside the main agent.

## Features

- **BackgroundAgentNode** â€” Processes events without producing audio output
- **Multi-node sessions** â€” Multiple agents running in parallel
- **Event handling** â€” Reacting to `UserStartedSpeaking`, `UserStoppedSpeaking`, `TranscriptUpdate`
- **Cross-agent communication** â€” Main agent queries background agent state
- **Auto-escalation** â€” Transfer based on detected frustration

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AgentSession                        â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  SentimentAnalyzer  â”‚   â”‚     SupportAgent        â”‚  â”‚
â”‚  â”‚  (BackgroundNode)   â”‚   â”‚    (OutputAgentNode)    â”‚  â”‚
â”‚  â”‚                     â”‚   â”‚                         â”‚  â”‚
â”‚  â”‚  - Listens to all   â”‚   â”‚  - Handles conversation â”‚  â”‚
â”‚  â”‚    events           â”‚â—„â”€â”€â”‚  - Queries sentiment    â”‚  â”‚
â”‚  â”‚  - Analyzes text    â”‚   â”‚  - Auto-escalates       â”‚  â”‚
â”‚  â”‚  - Stores state     â”‚   â”‚                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â–²                          â–²                 â”‚
â”‚            â”‚      Events flow to      â”‚                 â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€ both nodes â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage).

## Usage

```bash
uv pip install -r requirements.txt
uv run app.py
```

Connect with the CLI:

```bash
smallestai agent chat
```

## Recommended Usage

- Background processing alongside your main agent â€” sentiment analysis, compliance monitoring, real-time analytics
- Multi-node sessions where agents run in parallel and share state
- For sequential pipeline processing, [Language Switching](../language_switching/) is recommended

## Key Snippets

### BackgroundAgentNode

Unlike `OutputAgentNode`, background agents:
- Don't produce audio output
- Don't auto-handle interrupts
- Process events silently in the background
- Store state for other agents to query

```python
from smallestai.atoms.agent.nodes import BackgroundAgentNode

class SentimentAnalyzer(BackgroundAgentNode):
    def __init__(self):
        super().__init__(name="sentiment-analyzer")
        self.current_sentiment = "neutral"

    async def process_event(self, event: SDKEvent):
        if isinstance(event, SDKAgentTranscriptUpdateEvent):
            if event.role == "user":
                self.current_sentiment = await self._analyze(event.content)
```

### Multi-Node Session

Add multiple nodes to run them in parallel:

```python
async def setup_session(session: AgentSession):
    background_agent = SentimentAnalyzer()
    main_agent = SupportAgent(sentiment_analyzer=background_agent)
    
    # Both nodes receive all events
    session.add_node(background_agent)
    session.add_node(main_agent)
    
    await session.start()
```

### Speaking Events

React to user speaking state:

```python
async def process_event(self, event: SDKEvent):
    if isinstance(event, SDKSystemUserStartedSpeakingEvent):
        # User started talking
        pass
    elif isinstance(event, SDKSystemUserStoppedSpeakingEvent):
        # User finished talking
        pass
```

## Example Output

```
Assistant: Hello! I'm here to help. What can I assist you with today?
User: I've been waiting for my order for 3 weeks and nobody will help me!
[SentimentAnalyzer] Detected frustrated sentiment (frustration count: 1)
Assistant: I completely understand your frustration, and I'm sorry for the delay...

User: This is ridiculous! I want a refund now!
[SentimentAnalyzer] Detected frustrated sentiment (frustration count: 2)
Assistant: I hear you, and I want to make this right...

User: I can't believe how terrible this service is!
[SentimentAnalyzer] Detected frustrated sentiment (frustration count: 3)
[Auto-escalation triggered]
Assistant: I can hear this has been frustrating. Let me connect you with a supervisor...
```

## Use Cases

- **Escalation triggers** â€” Auto-transfer when frustration is high
- **Call quality monitoring** â€” Track sentiment across calls
- **Agent coaching** â€” Real-time feedback for human agents
- **Analytics** â€” Post-call sentiment reports

## Structure

```
background_agent/
â”œâ”€â”€ app.py                  # Session setup with multi-node architecture
â”œâ”€â”€ sentiment_analyzer.py   # BackgroundAgentNode for sentiment analysis
â””â”€â”€ support_agent.py        # OutputAgentNode with sentiment-aware responses
```

## API Reference

- [Agents â€” Overview](https://atoms-docs.smallest.ai/dev/build/agents/overview)
- [Core Concepts â€” Nodes](https://atoms-docs.smallest.ai/dev/introduction/core-concepts/nodes)

## Next Steps

- [Language Switching](../language_switching/) â€” Chained node pipelines with `add_edge()`
- [Interrupt Control](../interrupt_control/) â€” Mute/unmute control



================================================
FILE: voice-agents/background_agent/app.py
================================================
"""Background Agent Example - Multi-node architecture with sentiment analysis."""

from loguru import logger

from sentiment_analyzer import SentimentAnalyzer
from support_agent import SupportAgent

from smallestai.atoms.agent.events import SDKEvent, SDKSystemUserJoinedEvent
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession


async def setup_session(session: AgentSession):
    """Configure multi-node session with background processing.
    
    Architecture:
    - SentimentAnalyzer (BackgroundAgentNode): Processes all events, analyzes sentiment
    - SupportAgent (OutputAgentNode): Handles conversation, queries sentiment state
    
    Both nodes run in parallel, receiving the same events.
    """
    
    # Create background agent for sentiment analysis
    sentiment_analyzer = SentimentAnalyzer()
    
    # Create main agent with reference to background agent
    support_agent = SupportAgent(sentiment_analyzer=sentiment_analyzer)
    
    # Add both nodes to session - they run in parallel
    session.add_node(sentiment_analyzer)
    session.add_node(support_agent)
    
    await session.start()

    @session.on_event("on_event_received")
    async def on_event_received(_, event: SDKEvent):
        logger.info(f"Event received: {event.type}")

        if isinstance(event, SDKSystemUserJoinedEvent):
            greeting = (
                "Hello! I'm here to help. What can I assist you with today?"
            )
            support_agent.context.add_message({"role": "assistant", "content": greeting})
            await support_agent.speak(greeting)

    await session.wait_until_complete()
    
    # Log final sentiment summary
    summary = sentiment_analyzer.get_sentiment_summary()
    logger.info(f"Call sentiment summary: {summary}")
    logger.success("Session complete")


if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()



================================================
FILE: voice-agents/background_agent/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/background_agent/sentiment_analyzer.py
================================================
"""Background agent for real-time sentiment analysis."""

import os
from typing import Dict, List

from dotenv import load_dotenv
from loguru import logger

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.events import (
    SDKAgentTranscriptUpdateEvent,
    SDKEvent,
    SDKSystemUserStartedSpeakingEvent,
    SDKSystemUserStoppedSpeakingEvent,
)
from smallestai.atoms.agent.nodes import BackgroundAgentNode

load_dotenv()


class SentimentAnalyzer(BackgroundAgentNode):
    """Analyzes user sentiment in real-time without producing output.
    
    This background agent:
    - Listens to transcript updates
    - Analyzes sentiment of user messages
    - Stores results for the main agent to query
    - Does NOT produce any audio output
    
    Use cases:
    - Escalation triggers (detect frustration)
    - Call quality monitoring
    - Real-time coaching for human agents
    """

    def __init__(self):
        super().__init__(name="sentiment-analyzer")
        self.llm = OpenAIClient(
            model="gpt-4o-mini",
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # Store sentiment history
        self.sentiment_history: List[Dict] = []
        self.current_sentiment: str = "neutral"
        self.frustration_count: int = 0
        
        # Track speaking state
        self.user_is_speaking: bool = False

    async def process_event(self, event: SDKEvent):
        """Process events in the background.
        
        BackgroundAgentNode receives all events but doesn't auto-handle them.
        We manually process the ones we care about.
        """
        
        # Track when user starts/stops speaking
        if isinstance(event, SDKSystemUserStartedSpeakingEvent):
            self.user_is_speaking = True
            logger.debug("[SentimentAnalyzer] User started speaking")
            
        elif isinstance(event, SDKSystemUserStoppedSpeakingEvent):
            self.user_is_speaking = False
            logger.debug("[SentimentAnalyzer] User stopped speaking")
            
        # Analyze sentiment when we get user transcripts
        elif isinstance(event, SDKAgentTranscriptUpdateEvent):
            if event.role == "user":
                await self._analyze_sentiment(event.content)

    async def _analyze_sentiment(self, text: str):
        """Analyze sentiment of user message."""
        try:
            response = await self.llm.chat(
                messages=[
                    {
                        "role": "system",
                        "content": """Analyze the sentiment of this customer message.
Respond with exactly one word: positive, neutral, negative, or frustrated.
Only respond with that single word."""
                    },
                    {"role": "user", "content": text}
                ],
                stream=False
            )
            
            sentiment = response.content.strip().lower()
            
            # Validate response
            if sentiment not in ["positive", "neutral", "negative", "frustrated"]:
                sentiment = "neutral"
            
            # Update state
            self.current_sentiment = sentiment
            self.sentiment_history.append({
                "text": text,
                "sentiment": sentiment
            })
            
            # Track frustration for escalation
            if sentiment in ["negative", "frustrated"]:
                self.frustration_count += 1
                logger.warning(
                    f"[SentimentAnalyzer] Detected {sentiment} sentiment "
                    f"(frustration count: {self.frustration_count})"
                )
            else:
                # Reset on positive interaction
                if sentiment == "positive":
                    self.frustration_count = max(0, self.frustration_count - 1)
                    
            logger.info(f"[SentimentAnalyzer] Sentiment: {sentiment}")
            
        except Exception as e:
            logger.error(f"[SentimentAnalyzer] Analysis failed: {e}")

    def should_escalate(self) -> bool:
        """Check if the call should be escalated based on sentiment."""
        return self.frustration_count >= 3

    def get_sentiment_summary(self) -> Dict:
        """Get summary of sentiment analysis for the call."""
        if not self.sentiment_history:
            return {"overall": "neutral", "history": []}
            
        sentiments = [s["sentiment"] for s in self.sentiment_history]
        
        # Simple majority sentiment
        from collections import Counter
        counts = Counter(sentiments)
        overall = counts.most_common(1)[0][0]
        
        return {
            "overall": overall,
            "current": self.current_sentiment,
            "frustration_count": self.frustration_count,
            "should_escalate": self.should_escalate(),
            "history": self.sentiment_history[-5:]  # Last 5 entries
        }



================================================
FILE: voice-agents/background_agent/support_agent.py
================================================
"""Main support agent that works alongside the sentiment analyzer."""

import os
from typing import TYPE_CHECKING, List

from dotenv import load_dotenv

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.clients.types import ToolCall, ToolResult
from smallestai.atoms.agent.events import SDKAgentEndCallEvent
from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.tools import ToolRegistry, function_tool

if TYPE_CHECKING:
    from sentiment_analyzer import SentimentAnalyzer

load_dotenv()


class SupportAgent(OutputAgentNode):
    """Support agent with access to background sentiment analysis.
    
    Demonstrates:
    - Working alongside a BackgroundAgentNode
    - Querying background agent state
    - Auto-escalation based on sentiment
    """

    def __init__(self, sentiment_analyzer: "SentimentAnalyzer"):
        super().__init__(name="support-agent")
        
        # Reference to background agent for querying state
        self.sentiment_analyzer = sentiment_analyzer
        
        self.llm = OpenAIClient(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=os.getenv("OPENAI_API_KEY")
        )

        # Initialize tools
        self.tool_registry = ToolRegistry()
        self.tool_registry.discover(self)
        self.tool_schemas = self.tool_registry.get_schemas()

        self.context.add_message({
            "role": "system",
            "content": """You are a helpful customer support agent.

You have tools to:
- Check the current customer sentiment (use this to adapt your tone)
- Check if escalation is needed
- End the call
- Transfer to a supervisor

IMPORTANT: Be extra empathetic if sentiment is negative or frustrated.
If the customer seems very upset, proactively offer to transfer to a supervisor.

Keep responses concise and helpful.""",
        })

    async def generate_response(self):
        """Generate response with sentiment awareness."""
        
        # Check if auto-escalation is needed before responding
        if self.sentiment_analyzer.should_escalate():
            yield "I can hear this has been frustrating. "
            yield "I sincerely apologize for the experience you've had. "
            yield "Let me do my absolute best to help you resolve this right now."
            # Reset frustration after acknowledgment
            self.sentiment_analyzer.frustration_count = 0
            return

        response = await self.llm.chat(
            messages=self.context.messages,
            stream=True,
            tools=self.tool_schemas
        )

        tool_calls: List[ToolCall] = []
        full_response = ""

        async for chunk in response:
            if chunk.content:
                full_response += chunk.content
                yield chunk.content
            if chunk.tool_calls:
                tool_calls.extend(chunk.tool_calls)

        if full_response and not tool_calls:
            self.context.add_message({"role": "assistant", "content": full_response})

        if tool_calls:
            results: List[ToolResult] = await self.tool_registry.execute(
                tool_calls=tool_calls, parallel=True
            )

            self.context.add_messages([
                {
                    "role": "assistant",
                    "content": "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments),
                            },
                        }
                        for tc in tool_calls
                    ],
                },
                *[
                    {"role": "tool", "tool_call_id": tc.id, "content": "" if result.content is None else str(result.content)}
                    for tc, result in zip(tool_calls, results)
                ],
            ])

            final_response = await self.llm.chat(
                messages=self.context.messages, stream=True
            )

            async for chunk in final_response:
                if chunk.content:
                    yield chunk.content

    # =========================================
    # SENTIMENT TOOLS (query background agent)
    # =========================================

    @function_tool()
    def get_customer_sentiment(self) -> str:
        """Get the current customer sentiment analysis.
        
        Returns the customer's current mood and frustration level.
        Use this to adapt your tone and approach.
        """
        summary = self.sentiment_analyzer.get_sentiment_summary()
        return (
            f"Current sentiment: {summary['current']}. "
            f"Overall mood: {summary['overall']}. "
            f"Frustration level: {summary['frustration_count']}/3 "
            f"(escalation threshold)."
        )

    @function_tool()
    def check_escalation_needed(self) -> str:
        """Check if the customer needs to be transferred to a supervisor.
        
        Based on sentiment analysis, determines if escalation is warranted.
        """
        if self.sentiment_analyzer.should_escalate():
            return "YES - Customer frustration is high. Recommend immediate transfer to supervisor."
        return "NO - Customer sentiment is manageable. Continue assisting."

    # =========================================
    # CALL CONTROL TOOLS
    # =========================================

    @function_tool()
    async def end_call(self) -> None:
        """End the call gracefully."""
        await self.send_event(SDKAgentEndCallEvent())
        return None

    @function_tool()
    async def transfer_to_supervisor(self) -> str:
        """Transfer the call to a supervisor for escalation.
        
        Note: Transfer functionality requires phone number configuration.
        For demo purposes, this logs the intent and returns a message.
        """
        from loguru import logger
        summary = self.sentiment_analyzer.get_sentiment_summary()
        logger.info(f"[SupportAgent] Transfer requested. Sentiment summary: {summary}")
        return "I've noted your request to speak with a supervisor. In a production setup, you would now be transferred."



================================================
FILE: voice-agents/bank_csr/README.md
================================================
# ðŸ¦ Bank CSR â€” AI Voice Banking Agent

A production-grade voice-based Customer Support Representative (**"Rekha"**) for an India-based bank, built with the [Atoms SDK](https://docs.smallest.ai). Rekha handles identity verification, live SQL queries against a real database, deterministic number-crunching, banking actions (FD creation/breaking, TDS certificates), compliance audit logging, and call transfers â€” all over a phone call.

> **"How much did I spend on Amazon in the last 2 years, and how is it trending?"**
>
> Rekha doesn't hallucinate an answer. She writes a SQL query, executes it against a real database, runs a deterministic Python computation on the results, and *then* speaks the answer â€” with amounts in the Indian numbering system (lakh, crore).

---

## Features

| Feature | Description |
|---------|-------------|
| **Real database queries** | LLM writes SQL â†’ agent validates & executes against an in-memory SQLite database |
| **Multi-round tool chaining** | `execute_query` â†’ `analyze_data` â†’ spoken response, all in a single conversational turn |
| **Deterministic computation** | Totals, trends, comparisons, rankings done in pure Python â€” no LLM math hallucinations |
| **Session-based identity verification** | KBA with 2 factors (Level 1) or 3 factors (Level 2). Verify once, never re-ask |
| **Banking actions** | Create/break Fixed Deposits, send TDS certificates â€” with balance validation and penalty calculation |
| **Compliance audit logging** | Silent `BackgroundAgentNode` writes every event, tool call, and action to an audit table |
| **Cold & warm call transfers** | Immediate handoff or brief-the-supervisor-first escalation, with hold music |
| **India-specific voice behaviour** | Amounts in lakh/crore, digits read one-at-a-time, secure handling of PINs/OTPs/CVVs |

---

## Demo

Here's what a typical call flow looks like:

```
Rekha:  Namaste! Welcome to Smallest Bank. I'm Rekha, your customer
        support representative. How may I help you today?

You:    Hi Rekha, I want to check my account balance.

Rekha:  Sure, I'll need to verify your identity first. Could you please
        share your full name and date of birth?

You:    Ajay Kumar, 15th February 1988.

Rekha:  Thank you, Mr. Kumar. Your identity has been verified.
        Your savings account ending zero zero zero four has an available
        balance of eight lakh twenty thousand two hundred and one rupees,
        as of April 30th, 2025.

You:    How much did I spend on Amazon since January 2024?

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Tool chain (behind the scenes):                          â”‚
        â”‚  1. execute_query â†’ SELECT date, debit FROM transactions â”‚
        â”‚     WHERE description LIKE '%Amazon%'                    â”‚
        â”‚     AND date >= '2024-01-01'                             â”‚
        â”‚  2. analyze_data(results, "total")                       â”‚
        â”‚     â†’ {"total": 519000, "count": 17, "currency": "INR"} â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Rekha:  Your total Amazon spend since January 2024 is five lakh nineteen
        thousand rupees across 17 transactions.

You:    Break that down by year.

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Tool chain:                                              â”‚
        â”‚  1. execute_query â†’ same query with date grouping        â”‚
        â”‚  2. analyze_data(results, "trend_yearly")                â”‚
        â”‚     â†’ 2024: â‚¹3,76,000  |  2025: â‚¹1,43,000              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Rekha:  In calendar year 2024, you spent three lakh seventy-six thousand
        rupees on Amazon. So far in 2025 through April, it's one lakh
        forty-three thousand rupees.

You:    Create an FD of 2 lakhs for 1 year.

Rekha:  For this action I'll need one more verification factor. Could you
        share the last 4 digits of your savings account number?

You:    Zero zero zero four.

Rekha:  Verified. I'll create a Fixed Deposit of two lakh rupees for one
        year at seven point five percent. Estimated interest is fifteen
        thousand rupees, maturity amount two lakh fifteen thousand. Shall
        I proceed?

You:    Yes.

Rekha:  Done! Your FD has been created. Your new savings balance is six
        lakh twenty thousand two hundred and one rupees.

You:    I want to speak to a supervisor about my credit limit.

Rekha:  I'll brief my supervisor and connect you right away.
        [Warm transfer â†’ supervisor receives context â†’ hold music plays]
```

---

## Requirements

- Python â‰¥ 3.12
- An [OpenAI API key](https://platform.openai.com/api-keys) (GPT-4o)
- A [Smallest AI API key](https://platform.smallest.ai) (for deployment)

### Dependencies

Only one direct dependency â€” the Atoms SDK bundles everything else:

```
smallestai >= 4.3.0
```

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage).

---

## Usage

### 1. Install

```bash
uv pip install -r voice-agents/bank_csr/requirements.txt
```

### 2. Set environment variables

```bash
export OPENAI_API_KEY=sk-...
export SMALLEST_API_KEY=...        # from platform.smallest.ai
export TRANSFER_NUMBER=+91...      # optional: phone number for call transfers
```

Or create a `.env` file in the project root:

```env
OPENAI_API_KEY=sk-...
SMALLEST_API_KEY=...
TRANSFER_NUMBER=+916366821717
```

### 3. Run locally

```bash
uv run voice-agents/bank_csr/app.py
```

Starts a WebSocket server on `localhost:8080`.

### 4. Test via CLI

```bash
smallestai agent chat
```

### 5. Deploy to Smallest Platform

```bash
smallestai agent deploy --entry app.py
```

Then make a call from the [Smallest Platform](https://platform.smallest.ai) dashboard.

---

## When to Use This

âœ… **Use this example when you need to:**
- Build a voice agent that queries a real database (SQL) instead of hardcoding answers
- Chain multiple tool calls in a single turn (query â†’ compute â†’ respond)
- Perform deterministic computation (totals, trends) without relying on LLM arithmetic
- Implement session-based identity verification with escalation levels
- Add silent compliance/audit logging alongside a conversational agent
- Handle banking or financial actions with validation and state changes
- Support both cold and warm call transfers

âŒ **This is NOT the right example if you:**
- Just need a simple Q&A chatbot â†’ see [`getting_started`](../getting_started)
- Only need basic tool calling â†’ see [`agent_with_tools`](../agent_with_tools)
- Only need call transfer/end call â†’ see [`call_control`](../call_control)
- Need a background processing node without the full banking stack â†’ see [`background_agent`](../background_agent)

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AgentSession                                 â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  AuditLogger         â”‚     â”‚  CSRAgent (Rekha)                  â”‚  â”‚
â”‚  â”‚  BackgroundAgentNode  â”‚     â”‚  OutputAgentNode                   â”‚  â”‚
â”‚  â”‚                       â”‚     â”‚                                    â”‚  â”‚
â”‚  â”‚  â€¢ Logs call start    â”‚     â”‚  â€¢ 10 function tools               â”‚  â”‚
â”‚  â”‚  â€¢ Logs transcripts   â”‚     â”‚  â€¢ Multi-round generate_response   â”‚  â”‚
â”‚  â”‚  â€¢ Logs tool calls    â”‚     â”‚  â€¢ OpenAI GPT-4o (streaming)       â”‚  â”‚
â”‚  â”‚  â€¢ Logs verifications â”‚     â”‚  â€¢ Session-based verification      â”‚  â”‚
â”‚  â”‚  â€¢ Logs banking acts  â”‚     â”‚  â€¢ SQL query + analysis pipeline   â”‚  â”‚
â”‚  â”‚  â€¢ Session summary    â”‚     â”‚  â€¢ Cold & warm call transfers      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                                 â”‚                      â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                        â”‚                                             â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚               â”‚   SQLite DB     â”‚                                    â”‚
â”‚               â”‚   (in-memory)   â”‚                                    â”‚
â”‚               â”‚                 â”‚                                    â”‚
â”‚               â”‚  â€¢ customers    â”‚                                    â”‚
â”‚               â”‚  â€¢ accounts     â”‚                                    â”‚
â”‚               â”‚  â€¢ transactions â”‚                                    â”‚
â”‚               â”‚  â€¢ fixed_deps   â”‚                                    â”‚
â”‚               â”‚  â€¢ cards        â”‚                                    â”‚
â”‚               â”‚  â€¢ audit_log    â”‚                                    â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Both nodes receive the same event stream. The `AuditLogger` silently observes and logs; the `CSRAgent` handles the conversation.

---

## Files

| File | Lines | Description |
|------|-------|-------------|
| `app.py` | ~66 | Entry point â€” creates `BankingDB`, wires `AuditLogger` + `CSRAgent`, handles greeting |
| `csr_agent.py` | ~751 | Rekha (`OutputAgentNode`) â€” system prompt, `generate_response`, all 10 tools |
| `audit_logger.py` | ~108 | `AuditLogger` (`BackgroundAgentNode`) â€” compliance event logging |
| `database.py` | ~463 | SQLite schema, seed data (1 customer, 75+ transactions), query helpers |
| `pyproject.toml` | ~9 | Dependencies |

---

## Tools

| # | Tool | Category | Description |
|---|------|----------|-------------|
| 1 | `verify_customer` | Identity | KBA with up to 5 factors (name, DOB, account last 4, city, debit card last 4). Level 1 = 2 factors, Level 2 = 3 factors. |
| 2 | `execute_query` | Data | Run a read-only `SELECT` query against the banking SQLite DB. Dangerous keywords blocked. |
| 3 | `analyze_data` | Compute | Deterministic Python analysis: `total`, `trend_monthly`, `trend_yearly`, `comparison`, `top_merchants`, `summary_stats` |
| 4 | `get_account_summary` | Data | Quick overview: savings balance, active FDs, cards |
| 5 | `create_fixed_deposit` | Action | Create FD from savings. Validates balance, minimum â‚¹10,000, 1 or 2 year tenure, 7.5% rate |
| 6 | `break_fixed_deposit` | Action | Break FD (partial or full). 1% penalty, credits savings |
| 7 | `send_tds_certificate` | Action | Queue TDS certificate for FY 2024-25 to an email address |
| 8 | `transfer_to_human_agent` | Call Control | Cold transfer â€” immediate handoff with relaxing hold music |
| 9 | `warm_transfer_to_supervisor` | Call Control | Warm transfer â€” briefs supervisor with context, uplifting hold music |
| 10 | `end_call` | Call Control | Gracefully end the call |

---

## How It Works

### 1. Session Setup (`app.py`)

Two nodes run in parallel inside a single `AgentSession`:

```python
db = BankingDB()

audit = AuditLogger(db=db)        # BackgroundAgentNode â€” silent logging
session.add_node(audit)

csr = CSRAgent(db=db, audit=audit) # OutputAgentNode â€” conversation
session.add_node(csr)

await session.start()
```

### 2. Multi-Round Tool Chaining (`csr_agent.py`)

The core of the agent is a `generate_response` loop that keeps calling tools until the LLM produces a final text response:

```python
async def generate_response(self):
    MAX_ROUNDS = 5

    for _round in range(MAX_ROUNDS):
        response = await self.llm.chat(
            messages=self.context.messages,
            stream=True,
            tools=self.tool_schemas,
        )

        tool_calls = []
        full_response = ""

        async for chunk in response:
            if chunk.content:
                full_response += chunk.content
                yield chunk.content          # stream text to TTS
            if chunk.tool_calls:
                tool_calls.extend(chunk.tool_calls)

        if not tool_calls:
            # No tools called â†’ final answer
            return

        # Execute tools, add results to context, loop again
        results = await self.tool_registry.execute(
            tool_calls=tool_calls, parallel=True
        )
        # ... append tool messages to context ...
```

This enables chains like:

```
User: "How much did I spend on Amazon, and is it trending up?"

Round 1: LLM calls execute_query(sql="SELECT date, debit FROM transactions WHERE ...")
Round 2: LLM calls analyze_data(data_json=<query results>, analysis_type="trend_yearly")
Round 3: LLM produces spoken response using the computed trend data
```

### 3. Deterministic Analysis (`analyze_data`)

All number-crunching is done in pure Python â€” the LLM never does arithmetic:

```python
@function_tool()
def analyze_data(self, data_json: str, analysis_type: str) -> Any:
    rows = json.loads(data_json)

    if analysis_type == "total":
        total = sum(self._get_amount(r) for r in rows)
        return {"total": total, "count": len(rows), "currency": "INR"}

    elif analysis_type == "trend_yearly":
        # Group by year, compute YoY deltas and percentages
        ...

    elif analysis_type == "top_merchants":
        # Rank merchants by total spend
        ...
```

Available analysis types: `total`, `trend_monthly`, `trend_yearly`, `comparison`, `top_merchants`, `summary_stats`

### 4. Identity Verification (`verify_customer`)

Verification happens against the live database, not hardcoded values:

```python
@function_tool()
def verify_customer(self, name="", dob="", account_last_four="", city="", debit_card_last_four=""):
    # Fetch ground truth from DB
    row = self.db.execute_read_query(
        "SELECT c.name, c.dob, c.city, a.account_number, ca.last_four "
        "FROM customers c JOIN accounts a ... JOIN cards ca ..."
    )
    # Compare each provided factor against DB
    # Level 1 (2 factors) â†’ account info access
    # Level 2 (3 factors) â†’ banking actions allowed
```

Once verified, `self.is_verified = True` persists for the entire session â€” no re-asking.

### 5. Audit Logging (`audit_logger.py`)

The `AuditLogger` is a `BackgroundAgentNode` that runs silently alongside the main agent:

```python
class AuditLogger(BackgroundAgentNode):
    async def process_event(self, event: SDKEvent):
        if isinstance(event, SDKSystemUserJoinedEvent):
            self.db.log_audit("CALL_START", ...)
        elif isinstance(event, SDKAgentTranscriptUpdateEvent):
            self.db.log_audit("TRANSCRIPT", ...)

    def log_tool_call(self, tool_name, args, result):
        self.db.log_audit("TOOL_CALL", ...)

    def log_banking_action(self, action, details):
        self.db.log_audit("BANKING_ACTION", ...)
```

At session end, a compliance summary is logged:

```json
{
    "call_start": "2025-04-30T10:15:00",
    "total_events": 12,
    "transcript_turns": 8,
    "tool_invocations": 5,
    "banking_actions": 1,
    "verification_attempts": 1
}
```

### 6. Call Transfers

**Cold transfer** â€” immediate handoff:

```python
@function_tool()
async def transfer_to_human_agent(self) -> None:
    await self.send_event(
        SDKAgentTransferConversationEvent(
            transfer_call_number=os.getenv("TRANSFER_NUMBER"),
            transfer_options=TransferOption(type=TransferOptionType.COLD_TRANSFER),
            on_hold_music="relaxing_sound",
        )
    )
```

**Warm transfer** â€” brief the supervisor first:

```python
@function_tool()
async def warm_transfer_to_supervisor(self, reason: str) -> None:
    await self.send_event(
        SDKAgentTransferConversationEvent(
            transfer_call_number=os.getenv("TRANSFER_NUMBER"),
            transfer_options=TransferOption(
                type=TransferOptionType.WARM_TRANSFER,
                private_handoff_option=WarmTransferPrivateHandoffOption(
                    type=WarmTransferHandoffOptionType.PROMPT,
                    prompt=f"Customer escalation at Smallest Bank: {reason}",
                ),
            ),
            on_hold_music="uplifting_beats",
        )
    )
```

### 7. Extending to External Observability

The `AuditLogger` writes to SQLite by default, but the `BackgroundAgentNode` pattern makes it trivial to stream events to any external observability platform â€” **without changing the main agent at all**.

Because the background node runs in parallel and never blocks the conversation, you get real-time visibility into live transcription, tool calls, and banking actions with zero impact on latency.

Replace or extend the logging calls in `audit_logger.py`:

```python
# Instead of (or in addition to) SQLite:
self.db.log_audit("TOOL_CALL", ...)

# Stream to Langfuse:
self._trace.span(name=f"tool:{tool_name}", input=args, output=result)

# Or Datadog, LangSmith, etc:
self.dd_client.send_event(name="tool_call", tags={"tool": tool_name})
```

The key insight: the `BackgroundAgentNode` receives the **exact same event stream** as the main agent â€” transcript updates, tool calls, user join/leave, everything. Swapping the sink from SQLite to Langfuse (or any platform) is a one-file change.

> **See the [`observability`](../observability) cookbook for a full working integration with [Langfuse](https://langfuse.com)** â€” live traces, tool call spans, transcript events, and session summaries streaming in real-time.

---

## Database Schema

The agent queries a real SQLite database with these tables:

| Table | Key Columns | Records |
|-------|-------------|---------|
| `customers` | name, dob, city, cibil_score, address | 1 customer (Ajay Kumar) |
| `accounts` | account_number, balance, currency | 1 savings account |
| `fixed_deposits` | principal, interest_rate, tenure, maturity_date, is_active | 2 active FDs |
| `cards` | type (debit/credit), last_four, credit_limit, apr | 2 cards |
| `transactions` | date, description, debit, credit, balance | 75+ transactions (Jan 2024 â€“ Apr 2025) |
| `audit_log` | timestamp, event_type, details | Written by AuditLogger at runtime |

### Precomputed Totals (for verification)

These are what the database will return â€” the agent computes them live, nothing is hardcoded:

| Query | Result |
|-------|--------|
| Savings balance | â‚¹8,20,201 |
| Amazon spend (calendar 2024) | â‚¹3,76,000 |
| Amazon spend (since Jan 2024) | â‚¹5,19,000 |
| Swiggy spend (since Jan 2024) | â‚¹1,71,800 |
| Adani Electricity (since Jan 2024) | â‚¹2,37,000 |
| Uber (since Jan 2024) | â‚¹19,000 |
| Adani + Uber combined | â‚¹2,56,000 |
| Credits to Jyoti Kumar | â‚¹6,00,000 |
| Dividend income | â‚¹11,10,000 |
| Municipal tax paid | â‚¹1,20,000 |
| Total FD principal | â‚¹35,00,000 |

---

## Customer Test Data

Use these details when testing identity verification:

| Factor | Value | How to say it |
|--------|-------|---------------|
| Name | Ajay Kumar | "Ajay Kumar" |
| Date of birth | 1988-02-15 | "Fifteenth February nineteen eighty-eight" |
| City | Mumbai | "Mumbai" |
| Account last 4 | 0004 | "Zero zero zero four" |
| Debit card last 4 | 0437 | "Zero four three seven" |

**Level 1** (balance/spend queries) â€” any 2 factors.
**Level 2** (FD create/break, TDS) â€” any 3 factors.

---

## Example Conversations

### Spending query with tool chaining

```
You:    How much did I spend on Amazon in 2024?
Rekha:  [execute_query â†’ analyze_data("total")]
        Your total Amazon spend in calendar year 2024 was approximately
        three lakh seventy-six thousand rupees across 13 transactions.
```

### Year-over-year comparison

```
You:    How does my Amazon spend compare between 2024 and 2025?
Rekha:  [execute_query â†’ analyze_data("trend_yearly")]
        In 2024, you spent three lakh seventy-six thousand rupees on Amazon.
        So far in 2025 through April, it's one lakh forty-three thousand rupees.
```

### FD creation

```
You:    Create an FD for 5 lakhs, 1 year.
Rekha:  [verify_customer(Level 2) â†’ create_fixed_deposit(500000, 1)]
        Done! FD of five lakh rupees created for one year at seven point five
        percent. Estimated maturity: five lakh thirty-seven thousand five hundred
        rupees. Your new savings balance is three lakh twenty thousand rupees.
```

### FD break with penalty

```
You:    Break one lakh from my FD ending zero zero zero seven.
Rekha:  [break_fixed_deposit("003-002-500-007", 100000)]
        Breaking one lakh rupees from FD ending zero zero zero seven.
        A one percent penalty of one thousand rupees applies.
        Ninety-nine thousand rupees will be credited to your savings.
        Shall I proceed?
```

### Warm transfer escalation

```
You:    I want to speak to your supervisor about my credit limit.
Rekha:  [warm_transfer_to_supervisor("credit limit inquiry")]
        I'll brief my supervisor and connect you right away.
        [Supervisor receives: "Customer escalation: credit limit inquiry"]
        [Customer hears uplifting beats hold music]
```

---

## API Reference

This example uses the following Atoms SDK components:

| Component | Import | Purpose |
|-----------|--------|---------|
| `AtomsApp` | `smallestai.atoms.agent.server` | WebSocket server + session lifecycle |
| `AgentSession` | `smallestai.atoms.agent.session` | Session management, node graph |
| `OutputAgentNode` | `smallestai.atoms.agent.nodes` | Conversational agent with TTS output |
| `BackgroundAgentNode` | `smallestai.atoms.agent.nodes` | Silent parallel processing node |
| `OpenAIClient` | `smallestai.atoms.agent.clients.openai` | Streaming LLM client |
| `ToolRegistry` | `smallestai.atoms.agent.tools` | Tool discovery, schema generation, execution |
| `@function_tool` | `smallestai.atoms.agent.tools` | Decorator to register tools from methods |
| `SDKAgentTransferConversationEvent` | `smallestai.atoms.agent.events` | Cold/warm call transfers |
| `SDKAgentEndCallEvent` | `smallestai.atoms.agent.events` | End call |
| `SDKSystemUserJoinedEvent` | `smallestai.atoms.agent.events` | User joined trigger |

Full SDK docs: [docs.smallest.ai](https://docs.smallest.ai)

---

## Next Steps

- **Simpler starting point** â†’ [`getting_started`](../getting_started) â€” basic agent with no tools
- **Just tool calling** â†’ [`agent_with_tools`](../agent_with_tools) â€” single-round tool usage
- **Call control only** â†’ [`call_control`](../call_control) â€” transfers and end call
- **Background processing** â†’ [`background_agent`](../background_agent) â€” sentiment analysis node
- **Observability with Langfuse** â†’ [`observability`](../observability) â€” stream live traces, tool calls, and transcripts to Langfuse
- **Interrupt control** â†’ [`interrupt_control`](../interrupt_control) â€” mute/unmute user
- **Language switching** â†’ [`language_switching`](../language_switching) â€” multi-language support
- **Swap the database** â†’ Replace `BankingDB` with a real PostgreSQL/MySQL connection for production
- **Add more analysis types** â†’ Extend `analyze_data` with forecasting, anomaly detection, etc.
- **Multi-customer support** â†’ Use `session_context.initial_variables` to pass the customer ID at call start



================================================
FILE: voice-agents/bank_csr/app.py
================================================
"""Bank CSR Example - Multi-round tool chaining with real database access."""

from loguru import logger

from audit_logger import AuditLogger
from csr_agent import CSRAgent
from database import BankingDB

from smallestai.atoms.agent.events import SDKEvent, SDKSystemUserJoinedEvent
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession


async def setup_session(session: AgentSession):
    """Configure banking CSR session.

    Architecture:
    - BankingDB: In-memory SQLite with synthetic banking data
    - AuditLogger (BackgroundAgentNode): Silent compliance logging
    - CSRAgent (OutputAgentNode): Handles conversation, verification, SQL queries,
      deterministic analysis, and banking actions via multi-round tool chaining

    Both nodes run in parallel, receiving the same events.
    """

    db = BankingDB()

    # Background audit logger â€” silent compliance node
    audit = AuditLogger(db=db)
    session.add_node(audit)

    # Main conversational agent
    csr = CSRAgent(db=db, audit=audit)
    session.add_node(csr)

    await session.start()

    @session.on_event("on_event_received")
    async def on_event_received(_, event: SDKEvent):
        logger.info(f"Event received: {event.type}")

        if isinstance(event, SDKSystemUserJoinedEvent):
            greeting = (
                "Namaste! Welcome to Smallest Bank. "
                "I'm Rekha, your customer support representative. "
                "How may I help you today?"
            )
            csr.context.add_message({"role": "assistant", "content": greeting})
            await csr.speak(greeting)

    await session.wait_until_complete()

    # Log audit summary at session end
    summary = audit.get_summary()
    logger.info(f"Audit summary: {summary}")
    full_log = db.get_audit_log()
    logger.info(f"Total audit entries: {len(full_log)}")

    db.close()
    logger.success("Session complete")


if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()



================================================
FILE: voice-agents/bank_csr/audit_logger.py
================================================
"""Background agent for compliance audit logging.

Silently observes all session events and tool invocations, writing
structured entries to the SQLite audit_log table.  Produces no
audio output â€” purely a compliance/observability node.
"""

import json
from datetime import datetime
from typing import Dict, List, Optional

from loguru import logger

from smallestai.atoms.agent.events import (
    SDKAgentTranscriptUpdateEvent,
    SDKEvent,
    SDKSystemUserJoinedEvent,
)
from smallestai.atoms.agent.nodes import BackgroundAgentNode

from database import BankingDB


class AuditLogger(BackgroundAgentNode):
    """Logs every meaningful event to the audit_log table.

    Architecture:
    - Receives the same event stream as the main CSRAgent
    - Writes to the shared BankingDB.audit_log table
    - The CSRAgent also pushes tool-invocation records via
      ``log_tool_call()`` so the audit trail captures tool usage
    """

    def __init__(self, db: BankingDB):
        super().__init__(name="audit-logger")
        self.db = db
        self._call_start: Optional[str] = None
        self._transcript: List[Dict[str, str]] = []

    # -- event handling ------------------------------------------------------

    async def process_event(self, event: SDKEvent):
        """Inspect every event; log the ones we care about."""

        if isinstance(event, SDKSystemUserJoinedEvent):
            self._call_start = datetime.utcnow().isoformat()
            self.db.log_audit(
                "CALL_START",
                json.dumps({"timestamp": self._call_start}),
            )
            logger.info("[AuditLogger] Call started")

        elif isinstance(event, SDKAgentTranscriptUpdateEvent):
            entry = {"role": event.role, "content": event.content}
            self._transcript.append(entry)
            self.db.log_audit(
                "TRANSCRIPT",
                json.dumps(entry),
            )

    # -- called by the CSRAgent after each tool execution --------------------

    def log_tool_call(self, tool_name: str, args: dict, result: str):
        """Record a tool invocation in the audit log."""
        self.db.log_audit(
            "TOOL_CALL",
            json.dumps({
                "tool": tool_name,
                "arguments": args,
                "result_preview": result[:500] if result else "",
            }),
        )
        logger.debug(f"[AuditLogger] Tool logged: {tool_name}")

    def log_verification(self, success: bool, factors_used: List[str]):
        """Record an identity verification attempt."""
        self.db.log_audit(
            "IDENTITY_VERIFICATION",
            json.dumps({
                "success": success,
                "factors": factors_used,
            }),
        )

    def log_banking_action(self, action: str, details: dict):
        """Record a banking action (FD create/break, TDS send)."""
        self.db.log_audit(
            "BANKING_ACTION",
            json.dumps({"action": action, **details}),
        )

    # -- summary (called at session end) ------------------------------------

    def get_summary(self) -> Dict:
        """Return a compliance summary for the call."""
        log = self.db.get_audit_log()
        tool_calls = [e for e in log if e["event_type"] == "TOOL_CALL"]
        actions = [e for e in log if e["event_type"] == "BANKING_ACTION"]
        verifications = [e for e in log if e["event_type"] == "IDENTITY_VERIFICATION"]
        return {
            "call_start": self._call_start,
            "total_events": len(log),
            "transcript_turns": len(self._transcript),
            "tool_invocations": len(tool_calls),
            "banking_actions": len(actions),
            "verification_attempts": len(verifications),
        }



================================================
FILE: voice-agents/bank_csr/csr_agent.py
================================================
"""Rekha â€” Banking Customer Support Representative agent.

An OutputAgentNode that demonstrates:
- Multi-round tool chaining (query â†’ analyse â†’ respond)
- Real SQL queries against a live SQLite database
- Deterministic Python computation (no LLM for number-crunching)
- Session-based identity verification
- Banking actions (FD create/break, TDS certificate)
- Audit logging via a companion BackgroundAgentNode
"""

import json
import os
from collections import defaultdict
from datetime import datetime
from typing import Any, Dict, List

from dotenv import load_dotenv
from loguru import logger

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.clients.types import ToolCall, ToolResult
from smallestai.atoms.agent.events import (
    SDKAgentEndCallEvent,
    SDKAgentTransferConversationEvent,
    TransferOption,
    TransferOptionType,
    WarmTransferHandoffOptionType,
    WarmTransferPrivateHandoffOption,
)
from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.tools import ToolRegistry, function_tool

from database import BankingDB, DB_SCHEMA_DESCRIPTION

load_dotenv()

# ---------------------------------------------------------------------------
# System prompt
# ---------------------------------------------------------------------------

SYSTEM_PROMPT = f"""You are **Rekha**, a voice-based Customer Support Representative for **Smallest Bank** (a dummy Indian bank).

## Your Role
Help the customer with account information, spending insights, and a small set of banking actions.
You have access to a real SQL database with the customer's accounts, transactions, fixed deposits, and cards.

## Voice & Conversation Style
- Sound like a professional Indian bank support rep: calm, helpful, efficient.
- Keep responses short: 1â€“3 sentences at a time. This is a phone call, not an essay.
- Confirm important details out loud before executing any action.

### Reading sensitive identifiers aloud
- Account numbers: read only the last 4 digits unless the customer explicitly asks.
- Card numbers: read only the last 4 digits. NEVER read full card numbers.
- Address: read only city and state unless the customer asks for the full address.

### Pronunciation rules (India-specific)
- "CVV" â†’ say "C-V-V"
- "OTP" â†’ say "O-T-P"
- "EMI" â†’ say "E-M-I"
- â‚¹ â†’ say "rupees"
- Read rupee amounts in the **Indian numbering system** (lakh, crore).
  - â‚¹350000 â†’ "three lakh fifty thousand rupees"
  - â‚¹84250 â†’ "eighty-four thousand two hundred and fifty rupees"
- Read mobile numbers and card digits **one digit at a time**, never grouped.

## Security â€” HARD RULES
You must **NEVER** ask for, accept, repeat, store, or verify using:
OTP, Card PIN, ATM PIN, MPIN, CVV, NetBanking password, UPI PIN, full card numbers.

If the customer starts sharing any of these, interrupt:
> "Please stop there. For your security, I cannot take OTP, PIN, CVV, or passwords on a call."

## Identity Verification
Use the `verify_customer` tool. Verify **once per conversation**.
- **Level 1** (account info, balances, spend queries): 2 matching factors.
- **Level 2** (banking actions â€” FD create/break, TDS send): 3 matching factors.

Allowed factors: full name, date of birth, city/state, last 4 digits of savings account, last 4 of debit card.
After successful verification **do NOT re-verify** unless a high-risk action is requested after a long detour.
If verification fails twice, offer to connect to a human agent.

## How to Answer Questions
1. **Always** use `execute_query` to fetch data from the database. Do NOT invent numbers.
2. For any computation (totals, trends, comparisons), pass the raw query results to `analyze_data`. Do NOT compute in your head.
3. Use the results from `analyze_data` to formulate your spoken response.
4. Speak amounts in the Indian numbering system (lakh / crore).

## Banking Actions You Can Take
- Break an FD (up to the full principal) and transfer to Savings.
- Create a new FD from Savings balance.
- Send TDS certificate for the last Financial Year over email.
- Cold transfer to a human agent when the customer asks or you cannot help.
- Warm transfer to a supervisor when the customer asks for escalation (briefs the supervisor first).
Anything else â†’ politely refuse and offer to transfer to a human agent.

## Available Database
{DB_SCHEMA_DESCRIPTION}

## Edge Cases
- Data is available only through 2025-04-30. If asked beyond that, say so.
- If you don't have information, say so and offer a human agent.
- NEVER invent balances, rates, or offers not in the database.
""".strip()


# ---------------------------------------------------------------------------
# Agent
# ---------------------------------------------------------------------------


class CSRAgent(OutputAgentNode):
    """Rekha â€” Banking CSR with real database access and multi-round tool chaining."""

    def __init__(self, db: BankingDB, audit: Any = None):
        super().__init__(name="csr-agent", is_interruptible=True)

        self.db = db
        self.audit = audit

        self.llm = OpenAIClient(
            model="gpt-4o",
            temperature=0.3,
            api_key=os.getenv("OPENAI_API_KEY"),
        )

        # Verification state (persists for the session)
        self.is_verified: bool = False
        self.verification_level: int = 0  # 0 = none, 1 = basic, 2 = high-risk

        # Tool setup
        self.tool_registry = ToolRegistry()
        self.tool_registry.discover(self)
        self.tool_schemas = self.tool_registry.get_schemas()

        # System prompt
        self.context.add_message({"role": "system", "content": SYSTEM_PROMPT})

    # =========================================================================
    # Multi-round tool chaining in generate_response
    # =========================================================================

    async def generate_response(self):
        """Generate a response, looping through tool calls until the LLM
        produces a final text-only answer.  This enables chains like:
            execute_query â†’ analyze_data â†’ spoken answer
        """
        MAX_ROUNDS = 5

        for _round in range(MAX_ROUNDS):
            response = await self.llm.chat(
                messages=self.context.messages,
                stream=True,
                tools=self.tool_schemas,
            )

            tool_calls: List[ToolCall] = []
            full_response = ""

            async for chunk in response:
                if chunk.content:
                    full_response += chunk.content
                    yield chunk.content
                if chunk.tool_calls:
                    tool_calls.extend(chunk.tool_calls)

            # If no tools were called, we're done
            if not tool_calls:
                if full_response:
                    self.context.add_message(
                        {"role": "assistant", "content": full_response}
                    )
                return

            # Provide audible feedback while tools execute
            if _round == 0:
                yield "One moment while I look into that. "

            # Execute all tool calls
            results: List[ToolResult] = await self.tool_registry.execute(
                tool_calls=tool_calls, parallel=True
            )

            # Log tool calls to audit
            if self.audit:
                for tc, result in zip(tool_calls, results):
                    try:
                        args = json.loads(tc.arguments)
                    except Exception:
                        args = {"raw": tc.arguments}
                    self.audit.log_tool_call(tc.name, args, result.content or "")

            # Add assistant + tool messages to context
            self.context.add_messages([
                {
                    "role": "assistant",
                    "content": full_response or "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments),
                            },
                        }
                        for tc in tool_calls
                    ],
                },
                *[
                    {
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": "" if r.content is None else str(r.content),
                    }
                    for tc, r in zip(tool_calls, results)
                ],
            ])

            # Loop continues â€” LLM sees tool results and may call more tools

        # Safety: if we hit max rounds, let the LLM wrap up
        final = await self.llm.chat(
            messages=self.context.messages, stream=True
        )
        async for chunk in final:
            if chunk.content:
                yield chunk.content

    # =========================================================================
    # TOOL: Identity verification
    # =========================================================================

    @function_tool()
    def verify_customer(
        self,
        name: str = "",
        dob: str = "",
        account_last_four: str = "",
        city: str = "",
        debit_card_last_four: str = "",
    ) -> str:
        """Verify customer identity using Knowledge-Based Authentication.

        Provide at least 2 factors for Level 1 (info queries) or 3 for Level 2
        (banking actions).  Allowed factors: name, dob (YYYY-MM-DD), last 4
        digits of savings account number, city, last 4 digits of debit card.

        Args:
            name: Customer's full name as per bank records.
            dob: Date of birth in YYYY-MM-DD format.
            account_last_four: Last 4 characters of savings account number.
            city: City from the customer's address.
            debit_card_last_four: Last 4 digits of debit card.
        """
        if self.is_verified:
            return (
                f"Customer is already verified at Level {self.verification_level}. "
                "No need to re-verify."
            )

        # Fetch ground truth
        row = self.db.execute_read_query(
            "SELECT c.name, c.dob, c.city, a.account_number, ca.last_four AS debit_last_four "
            "FROM customers c "
            "JOIN accounts a ON a.customer_id = c.id "
            "JOIN cards ca ON ca.customer_id = c.id AND ca.type = 'debit' "
            "LIMIT 1"
        )
        if not row:
            return "ERROR: No customer data found."

        truth = row[0]
        factors_matched: List[str] = []

        if name and name.strip().lower() == truth["name"].strip().lower():
            factors_matched.append("name")

        if dob and dob.strip() == truth["dob"]:
            factors_matched.append("dob")

        if account_last_four:
            actual_last4 = truth["account_number"].replace("-", "")[-4:]
            if account_last_four.strip() == actual_last4:
                factors_matched.append("account_last_four")

        if city and city.strip().lower() == truth["city"].strip().lower():
            factors_matched.append("city")

        if debit_card_last_four:
            if debit_card_last_four.strip() == truth["debit_last_four"]:
                factors_matched.append("debit_card_last_four")

        n = len(factors_matched)
        if self.audit:
            self.audit.log_verification(success=n >= 2, factors_used=factors_matched)

        if n >= 3:
            self.is_verified = True
            self.verification_level = 2
            return f"Identity verified (Level 2 â€” high-risk actions allowed). Factors matched: {', '.join(factors_matched)}."
        elif n >= 2:
            self.is_verified = True
            self.verification_level = 1
            return f"Identity verified (Level 1 â€” account info access). Factors matched: {', '.join(factors_matched)}."
        else:
            return (
                f"Verification failed. Only {n} factor(s) matched: "
                f"{', '.join(factors_matched) if factors_matched else 'none'}. "
                "Please ask the customer for more details or offer to connect to a human agent."
            )

    # =========================================================================
    # TOOL: Execute SQL query
    # =========================================================================

    @function_tool()
    def execute_query(self, sql: str) -> Any:
        """Execute a read-only SQL query against the banking database.

        Write a SELECT query to retrieve data. Only SELECT is allowed.
        Returns JSON array of result rows.

        Args:
            sql: A SELECT query to run.  Use LIKE for fuzzy merchant matching.
                 Use strftime for date grouping.
        """
        try:
            rows = self.db.execute_read_query(sql)
            # Cap output to avoid exceeding context
            if len(rows) > 200:
                return rows[:200]
            return rows
        except Exception as e:
            return f"QUERY ERROR: {e}"

    # =========================================================================
    # TOOL: Deterministic analysis (pure Python, no LLM)
    # =========================================================================

    @function_tool()
    def analyze_data(self, data_json: str, analysis_type: str) -> Any:
        """Run deterministic numerical analysis on data. This uses pure Python
        computation â€” no LLM.  Always prefer this over computing in prose.

        Args:
            data_json: A JSON array of rows (from execute_query results).
            analysis_type: One of:
                - "total" â€” sum a numeric column (expects rows with 'amount' or 'total' key)
                - "trend_monthly" â€” monthly totals (expects 'date' and 'amount'/'debit' columns)
                - "trend_yearly" â€” yearly totals with year-over-year delta
                - "comparison" â€” compare two named groups (expects 'group' and 'amount' keys)
                - "top_merchants" â€” rank by total spend (expects 'description' and 'debit' columns)
                - "summary_stats" â€” min, max, avg, count of a numeric column
        """
        try:
            rows = json.loads(data_json)
        except json.JSONDecodeError:
            return "ERROR: data_json is not valid JSON."

        if not rows:
            return {"result": "No data to analyse."}

        analysis_type = analysis_type.strip().lower()

        if analysis_type == "total":
            return self._analyze_total(rows)
        elif analysis_type == "trend_monthly":
            return self._analyze_trend_monthly(rows)
        elif analysis_type == "trend_yearly":
            return self._analyze_trend_yearly(rows)
        elif analysis_type == "comparison":
            return self._analyze_comparison(rows)
        elif analysis_type == "top_merchants":
            return self._analyze_top_merchants(rows)
        elif analysis_type == "summary_stats":
            return self._analyze_summary(rows)
        else:
            return f"ERROR: Unknown analysis_type '{analysis_type}'. Use: total, trend_monthly, trend_yearly, comparison, top_merchants, summary_stats."

    # -- analysis helpers (all return JSON strings) --------------------------

    @staticmethod
    def _get_amount(row: dict) -> int:
        """Extract amount from a row, trying common column names."""
        for key in ("amount", "total", "debit", "credit", "value", "sum_amount"):
            if key in row and row[key]:
                return int(row[key])
        return 0

    def _analyze_total(self, rows: List[dict]) -> dict:
        total = sum(self._get_amount(r) for r in rows)
        return {"total": total, "count": len(rows), "currency": "INR"}

    def _analyze_trend_monthly(self, rows: List[dict]) -> dict:
        monthly: Dict[str, int] = defaultdict(int)
        for r in rows:
            date_str = r.get("date", "")
            if len(date_str) >= 7:
                month_key = date_str[:7]  # YYYY-MM
            else:
                month_key = "unknown"
            monthly[month_key] += self._get_amount(r)

        sorted_months = sorted(monthly.items())
        trend = []
        prev = None
        for month, amount in sorted_months:
            entry: Dict[str, Any] = {"month": month, "amount": amount}
            if prev is not None:
                delta = amount - prev
                pct = round(delta / prev * 100, 1) if prev != 0 else 0
                entry["change"] = delta
                entry["change_pct"] = pct
            trend.append(entry)
            prev = amount

        return {"monthly_trend": trend, "currency": "INR"}

    def _analyze_trend_yearly(self, rows: List[dict]) -> dict:
        yearly: Dict[str, int] = defaultdict(int)
        for r in rows:
            date_str = r.get("date", "")
            year = date_str[:4] if len(date_str) >= 4 else "unknown"
            yearly[year] += self._get_amount(r)

        sorted_years = sorted(yearly.items())
        trend = []
        prev = None
        for year, amount in sorted_years:
            entry: Dict[str, Any] = {"year": year, "amount": amount}
            if prev is not None:
                delta = amount - prev
                pct = round(delta / prev * 100, 1) if prev != 0 else 0
                entry["yoy_change"] = delta
                entry["yoy_change_pct"] = pct
            trend.append(entry)
            prev = amount

        return {"yearly_trend": trend, "currency": "INR"}

    def _analyze_comparison(self, rows: List[dict]) -> dict:
        groups: Dict[str, int] = defaultdict(int)
        for r in rows:
            g = r.get("group", r.get("period", "unknown"))
            groups[str(g)] += self._get_amount(r)

        items = list(groups.items())
        result: Dict[str, Any] = {"groups": dict(items)}
        if len(items) == 2:
            a, b = items[0][1], items[1][1]
            result["difference"] = b - a
            result["change_pct"] = round((b - a) / a * 100, 1) if a != 0 else 0
        return result

    def _analyze_top_merchants(self, rows: List[dict]) -> dict:
        merchants: Dict[str, int] = defaultdict(int)
        for r in rows:
            desc = r.get("description", "unknown")
            merchants[desc] += self._get_amount(r)

        ranked = sorted(merchants.items(), key=lambda x: x[1], reverse=True)
        return {
            "ranking": [{"merchant": m, "total": t} for m, t in ranked],
            "currency": "INR",
        }

    def _analyze_summary(self, rows: List[dict]) -> dict:
        values = [self._get_amount(r) for r in rows]
        if not values:
            return {"error": "No numeric values found."}
        return {
            "count": len(values),
            "total": sum(values),
            "min": min(values),
            "max": max(values),
            "average": round(sum(values) / len(values), 2),
            "currency": "INR",
        }

    # =========================================================================
    # TOOL: Get account summary (quick helper)
    # =========================================================================

    @function_tool()
    def get_account_summary(self) -> Dict[str, Any]:
        """Retrieve a quick summary: savings balance, FDs, and cards.

        Use this for a high-level overview. For detailed queries, use execute_query.
        """
        try:
            balance = self.db.get_balance()
            fds = self.db.execute_read_query(
                "SELECT account_number, principal, tenure, interest_rate, "
                "maturity_date, is_active FROM fixed_deposits WHERE is_active = 1"
            )
            cards = self.db.execute_read_query(
                "SELECT type, last_four, expiry, credit_limit, apr, offers FROM cards"
            )
            return {
                "savings_balance": balance,
                "fixed_deposits": fds,
                "cards": cards,
                "currency": "INR",
                "as_of": "2025-04-30",
            }
        except Exception as e:
            return f"ERROR: {e}"

    # =========================================================================
    # TOOL: Create Fixed Deposit
    # =========================================================================

    @function_tool()
    def create_fixed_deposit(self, amount: int, tenure_years: int) -> Dict[str, Any]:
        """Create a new Fixed Deposit from Savings Account balance.

        Validates available balance, deducts from savings, and creates the FD.
        Interest rate is 7.50% for all tenures.

        Args:
            amount: FD principal amount in rupees. Minimum â‚¹10,000.
            tenure_years: Tenure in years (1 or 2).
        """
        if amount < 10000:
            return "ERROR: Minimum FD amount is â‚¹10,000."

        if tenure_years not in (1, 2):
            return "ERROR: Supported tenures are 1 year or 2 years."

        balance = self.db.get_balance()
        if amount > balance:
            return (
                f"ERROR: Insufficient balance. Available: â‚¹{balance:,}. "
                f"Requested: â‚¹{amount:,}."
            )

        # Create FD
        rate = 7.50
        today = datetime.now().strftime("%Y-%m-%d")
        maturity_year = datetime.now().year + tenure_years
        maturity_date = datetime.now().replace(year=maturity_year).strftime("%Y-%m-%d")
        tenure_str = f"{tenure_years} Year" + ("s" if tenure_years > 1 else "")
        est_interest = int(amount * rate / 100 * tenure_years)
        maturity_amount = amount + est_interest

        # Generate account number
        fd_count = self.db.execute_read_query(
            "SELECT COUNT(*) AS cnt FROM fixed_deposits"
        )[0]["cnt"]
        fd_acct = f"00{fd_count + 3}-002-500-0{fd_count + 10}"

        self.db.execute_write(
            "INSERT INTO fixed_deposits "
            "(customer_id, account_number, principal, open_date, tenure, "
            "interest_rate, maturity_date, is_active) VALUES (?,?,?,?,?,?,?,?)",
            (1, fd_acct, amount, today, tenure_str, rate, maturity_date, 1),
        )

        # Deduct from savings
        new_balance = balance - amount
        self.db.update_balance(new_balance)

        if self.audit:
            self.audit.log_banking_action("CREATE_FD", {
                "fd_account": fd_acct,
                "principal": amount,
                "tenure": tenure_str,
                "rate": rate,
                "maturity_date": maturity_date,
            })

        return {
            "status": "FD created successfully",
            "fd_account": fd_acct,
            "principal": amount,
            "tenure": tenure_str,
            "interest_rate": rate,
            "estimated_interest": est_interest,
            "maturity_amount": maturity_amount,
            "maturity_date": maturity_date,
            "new_savings_balance": new_balance,
        }

    # =========================================================================
    # TOOL: Break Fixed Deposit
    # =========================================================================

    @function_tool()
    def break_fixed_deposit(self, fd_account: str, amount: int) -> Dict[str, Any]:
        """Break (partially or fully) a Fixed Deposit and credit Savings.

        A 1% penalty is applied on the broken amount.

        Args:
            fd_account: The FD account number to break from.
            amount: Amount to break in rupees (up to the full principal).
        """
        fds = self.db.execute_read_query(
            f"SELECT id, principal, interest_rate, open_date, is_active "
            f"FROM fixed_deposits WHERE account_number = '{fd_account}' AND is_active = 1"
        )

        if not fds:
            return f"ERROR: No active FD found with account number {fd_account}."

        fd = fds[0]
        if amount > fd["principal"]:
            return (
                f"ERROR: Break amount (â‚¹{amount:,}) exceeds FD principal "
                f"(â‚¹{fd['principal']:,})."
            )

        # 1% penalty
        penalty = int(amount * 0.01)
        credit_amount = amount - penalty

        # Update FD
        new_principal = fd["principal"] - amount
        if new_principal <= 0:
            self.db.execute_write(
                f"UPDATE fixed_deposits SET is_active = 0, principal = 0 "
                f"WHERE id = {fd['id']}"
            )
        else:
            self.db.execute_write(
                f"UPDATE fixed_deposits SET principal = {new_principal} "
                f"WHERE id = {fd['id']}"
            )

        # Credit savings
        balance = self.db.get_balance()
        new_balance = balance + credit_amount
        self.db.update_balance(new_balance)

        if self.audit:
            self.audit.log_banking_action("BREAK_FD", {
                "fd_account": fd_account,
                "amount_broken": amount,
                "penalty": penalty,
                "credited_to_savings": credit_amount,
            })

        return {
            "status": "FD broken successfully",
            "amount_broken": amount,
            "penalty_1_pct": penalty,
            "credited_to_savings": credit_amount,
            "new_savings_balance": new_balance,
            "remaining_fd_principal": new_principal,
        }

    # =========================================================================
    # TOOL: Send TDS Certificate
    # =========================================================================

    @function_tool()
    def send_tds_certificate(self, email: str) -> Dict[str, Any]:
        """Send TDS certificate for the last Financial Year (FY 2024-25) to
        the given email address.

        Args:
            email: Email address to send the certificate to.
        """
        if "@" not in email or "." not in email:
            return "ERROR: Invalid email address."

        # Mask email for confirmation
        parts = email.split("@")
        masked = parts[0][0] + "***@" + parts[1]

        if self.audit:
            self.audit.log_banking_action("SEND_TDS_CERTIFICATE", {
                "email_masked": masked,
                "financial_year": "2024-25",
            })

        return {
            "status": "TDS certificate for FY 2024-25 has been queued for delivery.",
            "email_masked": masked,
            "note": "It will arrive within 24 hours.",
        }

    # =========================================================================
    # TOOL: Cold transfer to human agent
    # =========================================================================

    @function_tool()
    async def transfer_to_human_agent(self) -> None:
        """Cold-transfer the call to a human agent (immediate handoff).

        Use when:
        - Verification fails twice
        - The customer explicitly asks for a human agent
        - The request is outside your capabilities
        """
        transfer_number = os.getenv("TRANSFER_NUMBER", "+916366821717")
        await self.send_event(
            SDKAgentTransferConversationEvent(
                transfer_call_number=transfer_number,
                transfer_options=TransferOption(
                    type=TransferOptionType.COLD_TRANSFER,
                ),
                on_hold_music="relaxing_sound",
            )
        )
        return None

    # =========================================================================
    # TOOL: Warm transfer to supervisor
    # =========================================================================

    @function_tool()
    async def warm_transfer_to_supervisor(self, reason: str) -> None:
        """Warm-transfer: brief the supervisor first, then connect the customer.

        Use when:
        - The customer asks for a supervisor or manager
        - A complex issue needs escalation with context
        - You want to brief the receiving agent before handoff

        Args:
            reason: Brief summary of the customer's issue for the supervisor.
        """
        transfer_number = os.getenv("TRANSFER_NUMBER", "+916366821717")
        await self.send_event(
            SDKAgentTransferConversationEvent(
                transfer_call_number=transfer_number,
                transfer_options=TransferOption(
                    type=TransferOptionType.WARM_TRANSFER,
                    private_handoff_option=WarmTransferPrivateHandoffOption(
                        type=WarmTransferHandoffOptionType.PROMPT,
                        prompt=f"Customer escalation at Smallest Bank: {reason}",
                    ),
                ),
                on_hold_music="uplifting_beats",
            )
        )
        return None

    # =========================================================================
    # TOOL: End call
    # =========================================================================

    @function_tool()
    async def end_call(self) -> None:
        """End the call gracefully when the customer says goodbye."""
        await self.send_event(SDKAgentEndCallEvent())
        return None



================================================
FILE: voice-agents/bank_csr/database.py
================================================
"""Banking database â€” SQLite schema, seed data, and query helpers.

Creates an in-memory SQLite database seeded with synthetic banking data
for a single customer (Ajay Kumar). The database is created fresh for
every agent session so state is isolated.
"""

import json
import re
import sqlite3
from datetime import datetime
from typing import Any, Dict, List, Optional

from loguru import logger

# ---------------------------------------------------------------------------
# Schema
# ---------------------------------------------------------------------------

SCHEMA_SQL = """
CREATE TABLE IF NOT EXISTS customers (
    id              INTEGER PRIMARY KEY,
    name            TEXT    NOT NULL,
    gender          TEXT,
    dob             TEXT,           -- YYYY-MM-DD
    cibil_score     INTEGER,
    mothers_maiden  TEXT,
    address         TEXT,
    city            TEXT,
    state           TEXT,
    pincode         TEXT
);

CREATE TABLE IF NOT EXISTS accounts (
    id              INTEGER PRIMARY KEY,
    customer_id     INTEGER NOT NULL,
    type            TEXT    NOT NULL,  -- 'savings'
    account_number  TEXT    NOT NULL,
    balance         INTEGER NOT NULL,  -- in whole rupees
    currency        TEXT    DEFAULT 'INR',
    FOREIGN KEY (customer_id) REFERENCES customers(id)
);

CREATE TABLE IF NOT EXISTS fixed_deposits (
    id              INTEGER PRIMARY KEY,
    customer_id     INTEGER NOT NULL,
    account_number  TEXT    NOT NULL,
    principal       INTEGER NOT NULL,  -- in whole rupees
    open_date       TEXT    NOT NULL,  -- YYYY-MM-DD
    tenure          TEXT    NOT NULL,  -- e.g. '1 Year', '2 Years'
    interest_rate   REAL    NOT NULL,  -- e.g. 7.50
    maturity_date   TEXT,              -- YYYY-MM-DD
    is_active       INTEGER DEFAULT 1,
    FOREIGN KEY (customer_id) REFERENCES customers(id)
);

CREATE TABLE IF NOT EXISTS cards (
    id              INTEGER PRIMARY KEY,
    customer_id     INTEGER NOT NULL,
    type            TEXT    NOT NULL,  -- 'debit' / 'credit'
    network         TEXT,             -- 'Mastercard' / 'Visa'
    last_four       TEXT    NOT NULL,
    expiry          TEXT,
    credit_limit    INTEGER,
    apr             REAL,
    offers          TEXT,
    FOREIGN KEY (customer_id) REFERENCES customers(id)
);

CREATE TABLE IF NOT EXISTS transactions (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    account_id      INTEGER NOT NULL,
    date            TEXT    NOT NULL,  -- YYYY-MM-DD
    description     TEXT    NOT NULL,
    debit           INTEGER DEFAULT 0,  -- in whole rupees
    credit          INTEGER DEFAULT 0,
    balance         INTEGER DEFAULT 0,
    FOREIGN KEY (account_id) REFERENCES accounts(id)
);

CREATE TABLE IF NOT EXISTS audit_log (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp       TEXT    NOT NULL,
    event_type      TEXT    NOT NULL,
    details         TEXT
);
"""

# ---------------------------------------------------------------------------
# Seed data  (derived from synthetic-data-examples CSVs, cleaned)
# ---------------------------------------------------------------------------

CUSTOMER = {
    "id": 1,
    "name": "Ajay Kumar",
    "gender": "Male",
    "dob": "1988-02-15",
    "cibil_score": 820,
    "mothers_maiden": "Sunita Tiwari",
    "address": "Suite 105, Embassy Enclave, Powai, Mumbai 400072, Maharashtra",
    "city": "Mumbai",
    "state": "Maharashtra",
    "pincode": "400072",
}

ACCOUNT = {
    "id": 1,
    "customer_id": 1,
    "type": "savings",
    "account_number": "002-002-500-004",
    "balance": 820201,
    "currency": "INR",
}

FIXED_DEPOSITS = [
    {
        "id": 1,
        "customer_id": 1,
        "account_number": "003-002-500-007",
        "principal": 1500000,
        "open_date": "2025-03-15",
        "tenure": "1 Year",
        "interest_rate": 7.50,
        "maturity_date": "2026-03-15",
        "is_active": 1,
    },
    {
        "id": 2,
        "customer_id": 1,
        "account_number": "004-002-500-009",
        "principal": 2000000,
        "open_date": "2024-11-01",
        "tenure": "2 Years",
        "interest_rate": 7.50,
        "maturity_date": "2026-11-01",
        "is_active": 1,
    },
]

CARDS = [
    {
        "id": 1,
        "customer_id": 1,
        "type": "debit",
        "network": "Mastercard",
        "last_four": "0437",
        "expiry": "2029-10-31",
        "credit_limit": None,
        "apr": None,
        "offers": None,
    },
    {
        "id": 2,
        "customer_id": 1,
        "type": "credit",
        "network": None,
        "last_four": "0432",
        "expiry": "2028-11-30",
        "credit_limit": 1500000,
        "apr": 29.99,
        "offers": "Eligible for 50% increase in credit limit",
    },
]

# Cleaned & date-normalized transaction ledger.
# Dates are YYYY-MM-DD.  Amounts in whole rupees.
# fmt: off
TRANSACTIONS: List[Dict[str, Any]] = [
    {"date": "2024-01-01", "description": "Credit from ABC Traders",        "debit": 0,      "credit": 10000,  "balance": 630000},
    {"date": "2024-01-10", "description": "FBI Cards",                      "debit": 35000,   "credit": 0,      "balance": 595000},
    {"date": "2024-01-18", "description": "Swiggy",                         "debit": 2800,    "credit": 0,      "balance": 720200},
    {"date": "2024-01-28", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 870200},
    {"date": "2024-01-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 745000},
    {"date": "2024-02-10", "description": "Adani Electricity",              "debit": 18000,   "credit": 0,      "balance": 727000},
    {"date": "2024-02-15", "description": "Swiggy",                         "debit": 4000,    "credit": 0,      "balance": 723000},
    {"date": "2024-03-01", "description": "Amazon",                         "debit": 24000,   "credit": 0,      "balance": 846200},
    {"date": "2024-03-10", "description": "Amazon",                         "debit": 18000,   "credit": 0,      "balance": 828200},
    {"date": "2024-03-11", "description": "Swiggy",                         "debit": 4000,    "credit": 0,      "balance": 824200},
    {"date": "2024-03-18", "description": "Jyoti Kumar",                    "debit": 50000,   "credit": 0,      "balance": 774200},
    {"date": "2024-03-28", "description": "Amazon",                         "debit": 15000,   "credit": 0,      "balance": 759200},
    {"date": "2024-03-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 909200},
    {"date": "2024-03-31", "description": "FBI Cards",                      "debit": 74000,   "credit": 0,      "balance": 835200},
    {"date": "2024-04-01", "description": "Adani Electricity",              "debit": 14000,   "credit": 0,      "balance": 821200},
    {"date": "2024-04-15", "description": "Jyoti Kumar",                    "debit": 50000,   "credit": 0,      "balance": 771200},
    {"date": "2024-04-22", "description": "Swiggy",                         "debit": 6000,    "credit": 0,      "balance": 765200},
    {"date": "2024-04-24", "description": "Amazon",                         "debit": 13000,   "credit": 0,      "balance": 752200},
    {"date": "2024-04-30", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 902200},
    {"date": "2024-05-01", "description": "Adani Electricity",              "debit": 14000,   "credit": 0,      "balance": 888200},
    {"date": "2024-05-10", "description": "Reliance Dividend",              "debit": 0,       "credit": 210000, "balance": 1098200},
    {"date": "2024-05-14", "description": "Swiggy",                         "debit": 8000,    "credit": 0,      "balance": 1090200},
    {"date": "2024-05-16", "description": "Dell Computer Zone",             "debit": 400000,  "credit": 0,      "balance": 690200},
    {"date": "2024-05-24", "description": "Amazon",                         "debit": 33000,   "credit": 0,      "balance": 657200},
    {"date": "2024-05-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 807200},
    {"date": "2024-06-01", "description": "Adani Electricity",              "debit": 14000,   "credit": 0,      "balance": 793200},
    {"date": "2024-06-14", "description": "Amazon",                         "debit": 32000,   "credit": 0,      "balance": 761200},
    {"date": "2024-06-18", "description": "Annual Municipal Tax",           "debit": 120000,  "credit": 0,      "balance": 641200},
    {"date": "2024-06-24", "description": "Tata Capital Dividend",          "debit": 0,       "credit": 200000, "balance": 841200},
    {"date": "2024-06-28", "description": "Swiggy",                         "debit": 23000,   "credit": 0,      "balance": 818200},
    {"date": "2024-06-30", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 968200},
    {"date": "2024-07-01", "description": "Adani Electricity",              "debit": 12000,   "credit": 0,      "balance": 956200},
    {"date": "2024-07-10", "description": "Amazon",                         "debit": 25000,   "credit": 0,      "balance": 931200},
    {"date": "2024-07-15", "description": "Swiggy",                         "debit": 12000,   "credit": 0,      "balance": 919200},
    {"date": "2024-07-28", "description": "Jyoti Kumar",                    "debit": 50000,   "credit": 0,      "balance": 869200},
    {"date": "2024-07-30", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 1019200},
    {"date": "2024-08-01", "description": "Adani Electricity",              "debit": 21000,   "credit": 0,      "balance": 998200},
    {"date": "2024-08-15", "description": "Amazon",                         "debit": 34000,   "credit": 0,      "balance": 964200},
    {"date": "2024-08-17", "description": "Swiggy",                         "debit": 12000,   "credit": 0,      "balance": 952200},
    {"date": "2024-08-23", "description": "Airtel Broadband Annual Fee",    "debit": 35000,   "credit": 0,      "balance": 917200},
    {"date": "2024-08-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 1067200},
    {"date": "2024-09-01", "description": "Adani Electricity",              "debit": 21000,   "credit": 0,      "balance": 1046200},
    {"date": "2024-09-14", "description": "Amazon",                         "debit": 42000,   "credit": 0,      "balance": 1004200},
    {"date": "2024-09-22", "description": "Jyoti Kumar",                    "debit": 50000,   "credit": 0,      "balance": 954200},
    {"date": "2024-09-30", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 1104200},
    {"date": "2024-10-01", "description": "Adani Electricity",              "debit": 21000,   "credit": 0,      "balance": 1083200},
    {"date": "2024-10-02", "description": "Samsung Electronics",            "debit": 150000,  "credit": 0,      "balance": 966200},
    {"date": "2024-10-05", "description": "Amazon",                         "debit": 30000,   "credit": 0,      "balance": 1053200},
    {"date": "2024-10-10", "description": "Swiggy",                         "debit": 12000,   "credit": 0,      "balance": 1041200},
    {"date": "2024-10-14", "description": "Amazon",                         "debit": 29000,   "credit": 0,      "balance": 937200},
    {"date": "2024-10-15", "description": "Uber",                           "debit": 3000,    "credit": 0,      "balance": 1003201},
    {"date": "2024-10-16", "description": "Uber",                           "debit": 4000,    "credit": 0,      "balance": 999201},
    {"date": "2024-10-20", "description": "Jyoti Kumar",                    "debit": 50000,   "credit": 0,      "balance": 991200},
    {"date": "2024-10-20", "description": "Swiggy",                         "debit": 14000,   "credit": 0,      "balance": 923200},
    {"date": "2024-10-22", "description": "ATM Cash Withdrawal",            "debit": 10000,   "credit": 0,      "balance": 981200},
    {"date": "2024-10-28", "description": "Jyoti Kumar",                    "debit": 50000,   "credit": 0,      "balance": 873200},
    {"date": "2024-10-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 1131200},
    {"date": "2024-10-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 1023200},
    {"date": "2024-11-01", "description": "Adani Electricity",              "debit": 15000,   "credit": 0,      "balance": 1116200},
    {"date": "2024-11-01", "description": "Adani Electricity",              "debit": 14000,   "credit": 0,      "balance": 1009200},
    {"date": "2024-11-15", "description": "Amazon",                         "debit": 43000,   "credit": 0,      "balance": 966200},
    {"date": "2024-11-20", "description": "Swiggy",                         "debit": 13000,   "credit": 0,      "balance": 953200},
    {"date": "2024-11-24", "description": "Reliance Dividend",              "debit": 0,       "credit": 200000, "balance": 1153200},
    {"date": "2024-11-28", "description": "Indigo Airlines",                "debit": 243999,  "credit": 0,      "balance": 909201},
    {"date": "2024-11-30", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 1059201},
    {"date": "2024-12-01", "description": "Adani Electricity",              "debit": 15000,   "credit": 0,      "balance": 1044201},
    {"date": "2024-12-10", "description": "Amazon",                         "debit": 38000,   "credit": 0,      "balance": 1006201},
    {"date": "2024-12-22", "description": "Swiggy",                         "debit": 12000,   "credit": 0,      "balance": 987201},
    {"date": "2024-12-28", "description": "Indigo Airlines",                "debit": 84000,   "credit": 0,      "balance": 903201},
    {"date": "2024-12-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 1053201},
    {"date": "2024-12-31", "description": "Jyoti Kumar",                    "debit": 300000,  "credit": 0,      "balance": 753201},
    {"date": "2025-01-01", "description": "Adani Electricity",              "debit": 15000,   "credit": 0,      "balance": 738201},
    {"date": "2025-01-05", "description": "Amazon",                         "debit": 49000,   "credit": 0,      "balance": 689201},
    {"date": "2025-01-10", "description": "Uber",                           "debit": 4000,    "credit": 0,      "balance": 685201},
    {"date": "2025-01-15", "description": "Swiggy",                         "debit": 15000,   "credit": 0,      "balance": 670201},
    {"date": "2025-01-22", "description": "Swiggy",                         "debit": 20000,   "credit": 0,      "balance": 650201},
    {"date": "2025-01-29", "description": "Reliance Dividend",              "debit": 0,       "credit": 200000, "balance": 850201},
    {"date": "2025-01-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 1000201},
    {"date": "2025-02-01", "description": "Suzuki Cars Plaza",              "debit": 735000,  "credit": 0,      "balance": 265201},
    {"date": "2025-02-01", "description": "Adani Electricity",              "debit": 14000,   "credit": 0,      "balance": 251201},
    {"date": "2025-02-15", "description": "Amazon",                         "debit": 30000,   "credit": 0,      "balance": 221201},
    {"date": "2025-02-28", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 371201},
    {"date": "2025-03-01", "description": "Adani Electricity",              "debit": 14000,   "credit": 0,      "balance": 357201},
    {"date": "2025-03-10", "description": "Tata Capital Dividend",          "debit": 0,       "credit": 300000, "balance": 657201},
    {"date": "2025-03-15", "description": "Amazon",                         "debit": 30000,   "credit": 0,      "balance": 627201},
    {"date": "2025-03-22", "description": "Uber",                           "debit": 4000,    "credit": 0,      "balance": 623201},
    {"date": "2025-03-28", "description": "Airtel Mobile Postpaid Annual",  "debit": 36000,   "credit": 0,      "balance": 587201},
    {"date": "2025-03-31", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 737201},
    {"date": "2025-04-01", "description": "Adani Electricity",              "debit": 15000,   "credit": 0,      "balance": 722201},
    {"date": "2025-04-10", "description": "Amazon",                         "debit": 34000,   "credit": 0,      "balance": 688201},
    {"date": "2025-04-15", "description": "Uber",                           "debit": 4000,    "credit": 0,      "balance": 684201},
    {"date": "2025-04-18", "description": "Swiggy",                         "debit": 14000,   "credit": 0,      "balance": 670201},
    {"date": "2025-04-30", "description": "Rent Received",                  "debit": 0,       "credit": 150000, "balance": 820201},
]
# fmt: on


# ---------------------------------------------------------------------------
# Database helper
# ---------------------------------------------------------------------------


class BankingDB:
    """Thin wrapper around an in-memory SQLite database."""

    def __init__(self):
        self.conn = sqlite3.connect(":memory:")
        self.conn.row_factory = sqlite3.Row
        self._create_schema()
        self._seed()
        logger.info("[BankingDB] Database initialised and seeded")

    # -- setup ---------------------------------------------------------------

    def _create_schema(self):
        self.conn.executescript(SCHEMA_SQL)

    def _seed(self):
        cur = self.conn.cursor()

        # Customer
        c = CUSTOMER
        cur.execute(
            "INSERT INTO customers VALUES (?,?,?,?,?,?,?,?,?,?)",
            (c["id"], c["name"], c["gender"], c["dob"], c["cibil_score"],
             c["mothers_maiden"], c["address"], c["city"], c["state"], c["pincode"]),
        )

        # Account
        a = ACCOUNT
        cur.execute(
            "INSERT INTO accounts VALUES (?,?,?,?,?,?)",
            (a["id"], a["customer_id"], a["type"], a["account_number"],
             a["balance"], a["currency"]),
        )

        # Fixed deposits
        for fd in FIXED_DEPOSITS:
            cur.execute(
                "INSERT INTO fixed_deposits VALUES (?,?,?,?,?,?,?,?,?)",
                (fd["id"], fd["customer_id"], fd["account_number"],
                 fd["principal"], fd["open_date"], fd["tenure"],
                 fd["interest_rate"], fd["maturity_date"], fd["is_active"]),
            )

        # Cards
        for card in CARDS:
            cur.execute(
                "INSERT INTO cards VALUES (?,?,?,?,?,?,?,?,?)",
                (card["id"], card["customer_id"], card["type"], card["network"],
                 card["last_four"], card["expiry"], card["credit_limit"],
                 card["apr"], card["offers"]),
            )

        # Transactions
        for tx in TRANSACTIONS:
            cur.execute(
                "INSERT INTO transactions (account_id, date, description, debit, credit, balance) "
                "VALUES (?,?,?,?,?,?)",
                (1, tx["date"], tx["description"], tx["debit"], tx["credit"], tx["balance"]),
            )

        self.conn.commit()

    # -- query helpers -------------------------------------------------------

    def execute_read_query(self, sql: str) -> List[Dict[str, Any]]:
        """Execute a SELECT query and return rows as list of dicts.

        Raises ValueError if the query is not a SELECT statement.
        """
        cleaned = sql.strip()

        # Safety: only allow SELECT
        if not re.match(r"(?i)^\s*SELECT\b", cleaned):
            raise ValueError("Only SELECT queries are allowed.")

        # Block dangerous keywords
        dangerous = re.compile(
            r"\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|TRUNCATE|REPLACE|ATTACH|DETACH|PRAGMA)\b",
            re.IGNORECASE,
        )
        if dangerous.search(cleaned):
            raise ValueError("Query contains disallowed keywords.")

        cur = self.conn.execute(cleaned)
        columns = [desc[0] for desc in cur.description]
        rows = cur.fetchall()
        return [dict(zip(columns, row)) for row in rows]

    def execute_write(self, sql: str, params: tuple = ()) -> int:
        """Execute a write query (INSERT/UPDATE). Returns lastrowid."""
        cur = self.conn.execute(sql, params)
        self.conn.commit()
        return cur.lastrowid

    def get_balance(self) -> int:
        """Get current savings account balance."""
        row = self.conn.execute(
            "SELECT balance FROM accounts WHERE id = 1"
        ).fetchone()
        return row["balance"] if row else 0

    def update_balance(self, new_balance: int):
        """Update savings account balance."""
        self.conn.execute(
            "UPDATE accounts SET balance = ? WHERE id = 1", (new_balance,)
        )
        self.conn.commit()

    def log_audit(self, event_type: str, details: str):
        """Write an entry to the audit_log table."""
        self.conn.execute(
            "INSERT INTO audit_log (timestamp, event_type, details) VALUES (?,?,?)",
            (datetime.utcnow().isoformat(), event_type, details),
        )
        self.conn.commit()

    def get_audit_log(self) -> List[Dict[str, Any]]:
        """Return the full audit log."""
        cur = self.conn.execute(
            "SELECT timestamp, event_type, details FROM audit_log ORDER BY id"
        )
        return [dict(row) for row in cur.fetchall()]

    def close(self):
        self.conn.close()


# ---------------------------------------------------------------------------
# Schema description for the LLM system prompt
# ---------------------------------------------------------------------------

DB_SCHEMA_DESCRIPTION = """
DATABASE SCHEMA (SQLite):

Table: customers
  - id (INTEGER PK)
  - name (TEXT)
  - gender (TEXT)
  - dob (TEXT, YYYY-MM-DD)
  - cibil_score (INTEGER)
  - mothers_maiden (TEXT)
  - address (TEXT)
  - city (TEXT)
  - state (TEXT)
  - pincode (TEXT)

Table: accounts
  - id (INTEGER PK)
  - customer_id (INTEGER FK -> customers.id)
  - type (TEXT: 'savings')
  - account_number (TEXT)
  - balance (INTEGER, in whole rupees)
  - currency (TEXT)

Table: fixed_deposits
  - id (INTEGER PK)
  - customer_id (INTEGER FK -> customers.id)
  - account_number (TEXT)
  - principal (INTEGER, in whole rupees)
  - open_date (TEXT, YYYY-MM-DD)
  - tenure (TEXT, e.g. '1 Year')
  - interest_rate (REAL, e.g. 7.50)
  - maturity_date (TEXT, YYYY-MM-DD)
  - is_active (INTEGER, 1=yes)

Table: cards
  - id (INTEGER PK)
  - customer_id (INTEGER FK -> customers.id)
  - type (TEXT: 'debit'/'credit')
  - network (TEXT)
  - last_four (TEXT)
  - expiry (TEXT)
  - credit_limit (INTEGER, rupees, nullable)
  - apr (REAL, nullable)
  - offers (TEXT, nullable)

Table: transactions
  - id (INTEGER PK AUTOINCREMENT)
  - account_id (INTEGER FK -> accounts.id)
  - date (TEXT, YYYY-MM-DD)
  - description (TEXT â€” merchant/payee name)
  - debit (INTEGER, rupees, 0 if credit)
  - credit (INTEGER, rupees, 0 if debit)
  - balance (INTEGER, rupees)

NOTES:
- There is one customer (id=1) in the database.
- All monetary amounts are in whole Indian Rupees (INR).
- Transaction dates range from 2024-01-01 to 2025-04-30.
- Use LIKE for fuzzy merchant matching, e.g. WHERE description LIKE '%Amazon%'.
- Use strftime('%Y', date) or strftime('%Y-%m', date) for date grouping.
""".strip()



================================================
FILE: voice-agents/bank_csr/pyproject.toml
================================================
[project]
name = "atoms-bank-csr"
version = "0.1.0"
description = "Banking Customer Support Rep agent with real database access"
requires-python = ">=3.12"
dependencies = [
    "smallestai>=4.3.0",
]



================================================
FILE: voice-agents/bank_csr/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/call_control/README.md
================================================
# Call Control

Comprehensive call control: end calls, cold transfers, and warm transfers.

## Features

- **SDKAgentEndCallEvent** â€” End calls gracefully
- **Cold Transfer** â€” Immediate handoff to another agent
- **Warm Transfer** â€” Brief the receiving agent before handoff
- **Hold Music Options** â€” Different music styles while transferring

## Demo

**End Call**:
```
User: That's all I needed, goodbye!
Assistant: Great, glad I could help! Take care!
[Call ends]
```

**Cold Transfer**:
```
User: I need to speak to a real person
Assistant: I'll connect you to a human agent now. Please hold.
[User hears relaxing music, then connects to +1234567890]
```

**Warm Transfer**:
```
User: I need to speak to a supervisor about my billing issue
Assistant: I'll brief my supervisor and connect you right away.
[Supervisor receives: "Customer escalation: billing issue"]
[User hears uplifting music, then connects to supervisor]
```

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage).

Configure transfer numbers in `app.py`:
```python
agent = SupportAgent(
    cold_transfer_number="+1234567890",   # General support
    warm_transfer_number="+1987654321"    # Supervisor
)
```

## Usage

```bash
uv pip install -r requirements.txt
uv run app.py
```

Connect with the CLI:

```bash
smallestai agent chat
```

## Recommended Usage

- Agents that need to end calls gracefully or transfer callers to human agents
- Cold and warm transfer patterns for support escalation
- For IVR-style department routing, [Inbound IVR](../inbound_ivr/) is recommended

## Key Snippets

### End Call

```python
from smallestai.atoms.agent.events import SDKAgentEndCallEvent

@function_tool()
async def end_call(self) -> None:
    """End the call gracefully."""
    await self.send_event(SDKAgentEndCallEvent())
```

### Cold Transfer (Immediate)

```python
from smallestai.atoms.agent.events import (
    SDKAgentTransferConversationEvent,
    TransferOption,
    TransferOptionType,
)

@function_tool()
async def cold_transfer(self) -> None:
    """Immediate transfer to human agent."""
    await self.send_event(
        SDKAgentTransferConversationEvent(
            transfer_call_number="+1234567890",
            transfer_options=TransferOption(
                type=TransferOptionType.COLD_TRANSFER
            ),
            on_hold_music="relaxing_sound"
        )
    )
```

### Warm Transfer (With Briefing)

```python
@function_tool()
async def warm_transfer(self, reason: str) -> None:
    """Brief supervisor first, then transfer."""
    await self.send_event(
        SDKAgentTransferConversationEvent(
            transfer_call_number="+1987654321",
            transfer_options=TransferOption(
                type=TransferOptionType.WARM_TRANSFER,
                private_handoff_option={
                    "type": "prompt",
                    "prompt": f"Customer escalation: {reason}"
                }
            ),
            on_hold_music="uplifting_beats"
        )
    )
```

## Transfer Types

| Type | Description | Use Case |
|------|-------------|----------|
| `COLD_TRANSFER` | Immediate handoff | Quick transfers, general support |
| `WARM_TRANSFER` | Agent briefs human first | Escalations, complex issues |

## Hold Music Options

| Option | Description |
|--------|-------------|
| `"ringtone"` | Standard ring tone |
| `"relaxing_sound"` | Calm, ambient music |
| `"uplifting_beats"` | Energetic, positive music |
| `"none"` | Silence |

## Tools Included

| Tool | Description | When to Use |
|------|-------------|-------------|
| `end_call` | End the call gracefully | User says "goodbye" |
| `cold_transfer` | Immediate transfer | User asks for "human" |
| `warm_transfer` | Brief supervisor first | User asks for "manager" |
| `lookup_order` | Look up order status | Business logic example |

## Structure

```
call_control/
â”œâ”€â”€ app.py            # Server entry point with transfer number config
â””â”€â”€ support_agent.py  # Agent with all call control tools
```

## API Reference

- [Call Control](https://atoms-docs.smallest.ai/dev/build/phone-calling/call-control)
- [Core Concepts â€” Events](https://atoms-docs.smallest.ai/dev/introduction/core-concepts/events)

## Next Steps

- [Agent with Tools](../agent_with_tools/) â€” Custom business tools
- [Getting Started](../getting_started/) â€” Basic agent setup



================================================
FILE: voice-agents/call_control/app.py
================================================
"""Call Control Example - End calls and transfer to humans."""

from support_agent import SupportAgent
from loguru import logger

from smallestai.atoms.agent.events import SDKEvent, SDKSystemUserJoinedEvent
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession


async def setup_session(session: AgentSession):
    """Configure support agent with call control capabilities."""
    
    # Configure transfer numbers
    agent = SupportAgent(
        cold_transfer_number="+916366821717",   # General support
        warm_transfer_number="+916366821717"    # Supervisor/escalation
    )
    
    session.add_node(agent)
    await session.start()

    @session.on_event("on_event_received")
    async def on_event_received(_, event: SDKEvent):
        logger.info(f"Event received: {event.type}")

        if isinstance(event, SDKSystemUserJoinedEvent):
            greeting = (
                "Hi! I'm your support agent. I can help with orders, "
                "or connect you to a human. How can I assist?"
            )
            # Add to context so LLM knows conversation has started
            agent.context.add_message({"role": "assistant", "content": greeting})
            await agent.speak(greeting)

    await session.wait_until_complete()
    logger.success("Session complete")


if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()



================================================
FILE: voice-agents/call_control/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/call_control/support_agent.py
================================================
"""Support agent demonstrating all call control capabilities."""

import os
from typing import List

from dotenv import load_dotenv

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.clients.types import ToolCall, ToolResult
from smallestai.atoms.agent.events import (
    SDKAgentEndCallEvent,
    SDKAgentTransferConversationEvent,
    TransferOption,
    TransferOptionType,
    WarmTransferPrivateHandoffOption,
    WarmTransferHandoffOptionType,
)
from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.tools import ToolRegistry, function_tool

load_dotenv()


class SupportAgent(OutputAgentNode):
    """Support agent with comprehensive call control capabilities.
    
    Demonstrates:
    - End call gracefully
    - Cold transfer (immediate handoff)
    - Warm transfer (briefing before handoff)
    - Different hold music options
    """

    def __init__(
        self, 
        cold_transfer_number: str = "+1234567890",
        warm_transfer_number: str = "+1987654321"
    ):
        super().__init__(name="support-agent")
        self.cold_transfer_number = cold_transfer_number
        self.warm_transfer_number = warm_transfer_number
        
        self.llm = OpenAIClient(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=os.getenv("OPENAI_API_KEY")
        )

        # Initialize tool registry
        self.tool_registry = ToolRegistry()
        self.tool_registry.discover(self)
        self.tool_schemas = self.tool_registry.get_schemas()

        self.context.add_message({
            "role": "system",
            "content": """You are a customer support agent for TechCo.

You can help with:
- Product information and order status
- Technical issues

CALL CONTROL ACTIONS:
1. "goodbye"/"bye"/"that's all" â†’ Use end_call
2. "talk to someone"/"human agent" â†’ Use cold_transfer (faster)
3. "supervisor"/"manager"/"escalate" â†’ Use warm_transfer (you brief them first)

Before ending: Confirm user doesn't need anything else.
Before cold transfer: Say "I'll connect you now, please hold."
Before warm transfer: Say "I'll brief my supervisor and connect you."
""",
        })

    async def generate_response(self):
        """Generate response with tool calling support."""
        response = await self.llm.chat(
            messages=self.context.messages,
            stream=True,
            tools=self.tool_schemas
        )

        tool_calls: List[ToolCall] = []
        full_response = ""

        async for chunk in response:
            if chunk.content:
                full_response += chunk.content
                yield chunk.content
            if chunk.tool_calls:
                tool_calls.extend(chunk.tool_calls)

        # Add assistant response to context (if no tool calls)
        if full_response and not tool_calls:
            self.context.add_message({"role": "assistant", "content": full_response})

        if tool_calls:
            results: List[ToolResult] = await self.tool_registry.execute(
                tool_calls=tool_calls, parallel=True
            )

            self.context.add_messages([
                {
                    "role": "assistant",
                    "content": "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments),
                            },
                        }
                        for tc in tool_calls
                    ],
                },
                *[
                    {
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": "" if result.content is None else str(result.content),
                    }
                    for tc, result in zip(tool_calls, results)
                ],
            ])

            final_response = await self.llm.chat(
                messages=self.context.messages, stream=True
            )

            async for chunk in final_response:
                if chunk.content:
                    yield chunk.content

    # =========================================
    # CALL CONTROL TOOLS
    # =========================================

    @function_tool()
    async def end_call(self) -> None:
        """End the call gracefully.
        
        Use when the user says goodbye or indicates they're done.
        The call will be terminated after this event is sent.
        """
        await self.send_event(SDKAgentEndCallEvent())
        return None

    @function_tool()
    async def cold_transfer(self) -> None:
        """Immediately transfer to a human agent (cold transfer).
        
        Use when user asks for a "real person" or "human agent".
        The call is transferred immediately without briefing.
        User will hear hold music while connecting.
        """
        await self.send_event(
            SDKAgentTransferConversationEvent(
                transfer_call_number=self.cold_transfer_number,
                transfer_options=TransferOption(
                    type=TransferOptionType.COLD_TRANSFER
                ),
                on_hold_music="relaxing_sound"
            )
        )
        return None

    @function_tool()
    async def warm_transfer(self, reason: str) -> None:
        """Brief the supervisor first, then transfer (warm transfer).
        
        Use when user asks for a "supervisor" or "manager" or wants to escalate.
        You'll first brief the receiving agent, then they take over.
        
        Args:
            reason: Brief summary of the issue for the supervisor.
        """
        await self.send_event(
            SDKAgentTransferConversationEvent(
                transfer_call_number=self.warm_transfer_number,
                transfer_options=TransferOption(
                    type=TransferOptionType.WARM_TRANSFER,
                    private_handoff_option=WarmTransferPrivateHandoffOption(
                        type=WarmTransferHandoffOptionType.PROMPT,
                        prompt=f"Customer escalation: {reason}"
                    )
                ),
                on_hold_music="uplifting_beats"
            )
        )
        return None

    # =========================================
    # BUSINESS TOOLS
    # =========================================

    @function_tool()
    def lookup_order(self, order_id: str) -> str:
        """Look up order status by order ID.
        
        Args:
            order_id: The order ID to look up.
        """
        # Mock order data
        orders = {
            "ORD-001": "Shipped on Jan 10, arriving Jan 15",
            "ORD-002": "Processing, expected ship Jan 14",
            "ORD-003": "Delivered Jan 8",
        }
        return orders.get(order_id, f"Order {order_id} not found. Please verify the order ID.")



================================================
FILE: voice-agents/campaigns/README.md
================================================
# Campaign Management

Scripts for managing outbound calling campaigns.

## Features

- **Audience Creation** â€” Build contact lists for campaigns
- **Campaign Setup** â€” Link agents, audiences, and phone numbers
- **Campaign Control** â€” Start, stop, pause, and monitor
- **Contact Management** â€” Add contacts to existing audiences

## Workflow

```
1. Create Agent (via dashboard or API)
        â†“
2. Create Audience (contact list)
        â†“
3. Create Campaign (link agent + audience + phone)
        â†“
4. Start Campaign
        â†“
5. Monitor Status
        â†“
6. Stop/Pause as needed
```

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage). Add `SMALLEST_API_KEY` and `AGENT_ID` to your `.env`.

## Usage

```bash
uv pip install -r requirements.txt
```

### Step 1: Create Audience

```bash
uv run create_audience.py
```

This creates an audience and saves `AUDIENCE_ID` to `.env`.

### Step 2: Create Campaign

```bash
uv run create_campaign.py
```

This creates a campaign linking your agent, audience, and phone number.

### Step 3: Manage Campaign

```bash
# Start calling
uv run manage_campaign.py start

# Check status
uv run manage_campaign.py status

# Pause (can resume later)
uv run manage_campaign.py pause

# Stop completely
uv run manage_campaign.py stop

# List all campaigns
uv run manage_campaign.py list
```

### Optional: Add More Contacts

```bash
uv run add_contacts.py
```

## Recommended Usage

- Automated outbound calling at scale â€” sales outreach, appointment reminders, surveys
- Managing audiences, phone numbers, and campaign lifecycle
- For inbound call handling, [Inbound IVR](../inbound_ivr/) is recommended

## Key Snippets

### Creating an Audience

```python
from smallestai.atoms import Audience

audience = Audience()

result = audience.create(
    name="My Contacts",
    phone_numbers=["+1234567890", "+1234567891"],
    names=[("John", "Doe"), ("Jane", "Smith")],
    description="Optional description"
)

audience_id = result["data"]["_id"]
```

### Creating a Campaign

```python
from smallestai.atoms import Campaign

campaign = Campaign()

result = campaign.create(
    name="My Campaign",
    agent_id="agent_123",
    audience_id="audience_456",
    phone_ids=["phone_789"],  # Get from get_phone_numbers()
    max_retries=3,
    retry_delay=15
)

campaign_id = result["data"]["_id"]
```

### Campaign Control

```python
campaign = Campaign()

# Start
campaign.start(campaign_id)

# Pause
campaign.pause(campaign_id)

# Stop
campaign.stop(campaign_id)

# Get status
status = campaign.get(campaign_id)
```

### Using AtomsClient

You can also use the unified client:

```python
from smallestai.atoms import AtomsClient

client = AtomsClient()

# Audience operations
client.audience.create(...)

# Campaign operations
client.campaign.create(...)
client.campaign.start(campaign_id)

# Get phone numbers
phones = client.get_phone_numbers()
```

## Campaign Settings

| Setting | Type | Description |
|---------|------|-------------|
| `name` | string | Campaign name |
| `agent_id` | string | Agent to use for calls |
| `audience_id` | string | Contact list to call |
| `phone_ids` | list | Outbound phone number IDs |
| `max_retries` | int | Retry attempts if no answer (0-10) |
| `retry_delay` | int | Minutes between retries (1-1440) |

## Scripts Included

```
campaigns/
â”œâ”€â”€ create_audience.py    # Create a new audience with contacts
â”œâ”€â”€ add_contacts.py       # Add contacts to existing audience
â”œâ”€â”€ create_campaign.py    # Create a campaign
â””â”€â”€ manage_campaign.py    # Start/stop/pause/status commands
```

## Best Practices

1. **Test First** â€” Use a small audience to test your agent
2. **Monitor Status** â€” Check campaign progress regularly
3. **Handle Retries** â€” Configure appropriate retry settings
4. **Respect Regulations** â€” Follow TCPA and local calling regulations
5. **Track Results** â€” Use the analytics cookbook to analyze outcomes

## API Reference

- [Phone Calling â€” Overview](https://atoms-docs.smallest.ai/dev/build/phone-calling/overview)

## Next Steps

- [Analytics](../analytics/) â€” Call logs and metrics for campaign results
- [Knowledge Base RAG](../knowledge_base_rag/) â€” KB-enabled agents



================================================
FILE: voice-agents/campaigns/add_contacts.py
================================================
"""
Add Contacts to Audience Script

Add new contacts to an existing audience.

Usage:
    python add_contacts.py
"""

import os
from dotenv import load_dotenv
from smallestai.atoms import Audience

load_dotenv()


def main():
    """Add contacts to an existing audience."""
    
    audience_id = os.getenv("AUDIENCE_ID")
    if not audience_id:
        print("Error: AUDIENCE_ID not set. Run create_audience.py first.")
        return
    
    audience = Audience()
    
    print(f"Adding contacts to audience: {audience_id}")
    
    # Add new contacts
    result = audience.add_contacts(
        audience_id=audience_id,
        phone_numbers=[
            "+1234567893",
            "+1234567894",
        ],
        names=[
            ("Alice", "Williams"),
            ("Charlie", "Brown"),
        ]
    )
    
    print(f"âœ“ Contacts added")
    
    # Show updated member count
    members = audience.get_members(audience_id)
    total = members.get("data", {}).get("totalCount", "unknown")
    print(f"  Total members: {total}")


if __name__ == "__main__":
    main()



================================================
FILE: voice-agents/campaigns/create_audience.py
================================================
"""
Create Audience Script

Creates an audience (contact list) for outbound campaigns.

Usage:
    python create_audience.py
"""

import os
from dotenv import load_dotenv
from smallestai.atoms import Audience

load_dotenv()


def main():
    """Create an audience with sample contacts."""
    
    audience = Audience()
    
    print("Creating audience...")
    
    # Create audience with phone numbers and optional names
    result = audience.create(
        name="Demo Campaign Audience",
        phone_numbers=[
            "+1234567890",
            "+1234567891",
            "+1234567892",
        ],
        names=[
            ("John", "Doe"),
            ("Jane", "Smith"),
            ("Bob", "Johnson"),
        ],
        description="Demo audience for testing outbound campaigns"
    )
    
    audience_id = result["data"]["_id"]
    print(f"âœ“ Audience created: {audience_id}")
    
    # Save for later use
    env_path = os.path.join(os.path.dirname(__file__), ".env")
    with open(env_path, "a") as f:
        f.write(f"AUDIENCE_ID={audience_id}\n")
    
    print(f"âœ“ AUDIENCE_ID saved to .env")
    
    return audience_id


if __name__ == "__main__":
    main()



================================================
FILE: voice-agents/campaigns/create_campaign.py
================================================
"""
Create Campaign Script

Creates an outbound campaign linking agent, audience, and phone numbers.

Usage:
    python create_campaign.py
"""

import os
from dotenv import load_dotenv
from smallestai.atoms import Campaign, AtomsClient

load_dotenv()


def main():
    """Create an outbound campaign."""
    
    # Get required IDs from environment
    agent_id = os.getenv("AGENT_ID")
    audience_id = os.getenv("AUDIENCE_ID")
    
    if not agent_id:
        print("Error: AGENT_ID not set. Create an agent first.")
        return
    
    if not audience_id:
        print("Error: AUDIENCE_ID not set. Run create_audience.py first.")
        return
    
    # Get available phone numbers
    client = AtomsClient()
    phone_numbers = client.get_phone_numbers()
    
    if not phone_numbers.get("data"):
        print("Error: No phone numbers available. Acquire a number first.")
        return
    
    # Use the first available phone number
    phone_id = phone_numbers["data"][0]["_id"]
    phone_display = phone_numbers["data"][0]["attributes"]["phoneNumber"]
    print(f"Using phone number: {phone_display}")
    
    # Create campaign
    campaign = Campaign()
    
    print("Creating campaign...")
    
    result = campaign.create(
        name="Demo Outbound Campaign",
        agent_id=agent_id,
        audience_id=audience_id,
        phone_ids=[phone_id],
        description="Demo campaign for testing",
        max_retries=3,      # Retry up to 3 times if no answer
        retry_delay=15      # Wait 15 minutes between retries
    )
    
    campaign_id = result["data"]["_id"]
    print(f"âœ“ Campaign created: {campaign_id}")
    
    # Save for later use
    env_path = os.path.join(os.path.dirname(__file__), ".env")
    with open(env_path, "a") as f:
        f.write(f"CAMPAIGN_ID={campaign_id}\n")
    
    print(f"âœ“ CAMPAIGN_ID saved to .env")
    print(f"\nNext: Run start_campaign.py to start dialing")
    
    return campaign_id


if __name__ == "__main__":
    main()



================================================
FILE: voice-agents/campaigns/manage_campaign.py
================================================
"""
Campaign Management Script

Start, stop, pause, and check status of campaigns.

Usage:
    python manage_campaign.py start
    python manage_campaign.py stop
    python manage_campaign.py pause
    python manage_campaign.py status
"""

import os
import sys
from dotenv import load_dotenv
from smallestai.atoms import Campaign

load_dotenv()


def get_campaign_id():
    """Get campaign ID from environment."""
    campaign_id = os.getenv("CAMPAIGN_ID")
    if not campaign_id:
        print("Error: CAMPAIGN_ID not set. Run create_campaign.py first.")
        sys.exit(1)
    return campaign_id


def start_campaign():
    """Start the campaign."""
    campaign = Campaign()
    campaign_id = get_campaign_id()
    
    print(f"Starting campaign: {campaign_id}")
    result = campaign.start(campaign_id)
    print(f"âœ“ Campaign started")
    print(f"  Task ID: {result.get('data', {}).get('taskId', 'N/A')}")


def stop_campaign():
    """Stop the campaign."""
    campaign = Campaign()
    campaign_id = get_campaign_id()
    
    print(f"Stopping campaign: {campaign_id}")
    result = campaign.stop(campaign_id)
    print(f"âœ“ Campaign stopped")


def pause_campaign():
    """Pause the campaign."""
    campaign = Campaign()
    campaign_id = get_campaign_id()
    
    print(f"Pausing campaign: {campaign_id}")
    result = campaign.pause(campaign_id)
    print(f"âœ“ Campaign paused")


def get_status():
    """Get campaign status."""
    campaign = Campaign()
    campaign_id = get_campaign_id()
    
    print(f"Getting status for: {campaign_id}")
    result = campaign.get(campaign_id)
    
    data = result.get("data", {}).get("campaign", {})
    print(f"\nCampaign: {data.get('name', 'N/A')}")
    print(f"Status: {data.get('status', 'N/A')}")
    print(f"Description: {data.get('description', 'N/A')}")
    print(f"Participants: {data.get('participantsCount', 0)}")
    
    # Metrics if available
    metrics = result.get("data", {}).get("metrics", {})
    if metrics:
        print(f"\nMetrics:")
        print(f"  Total participants: {metrics.get('total_participants', 0)}")
        print(f"  Contacts called: {metrics.get('contacts_called', 0)}")
        print(f"  Contacts connected: {metrics.get('contacts_connected', 0)}")


def list_campaigns():
    """List all campaigns."""
    campaign = Campaign()
    
    print("Listing all campaigns...")
    result = campaign.list()
    
    campaigns = result.get("data", {}).get("campaigns", [])
    if not campaigns:
        print("No campaigns found.")
        return
    
    total = result.get("data", {}).get("totalCampaignCount", len(campaigns))
    print(f"\nFound {len(campaigns)} of {total} campaign(s):\n")
    for c in campaigns:
        print(f"  ID: {c.get('_id', 'N/A')}")
        print(f"  Name: {c.get('name', 'N/A')}")
        print(f"  Status: {c.get('status', 'N/A')}")
        print()


def main():
    """Main entry point."""
    if len(sys.argv) < 2:
        print("Usage: python manage_campaign.py <command>")
        print("\nCommands:")
        print("  start   - Start the campaign")
        print("  stop    - Stop the campaign")
        print("  pause   - Pause the campaign")
        print("  status  - Get campaign status")
        print("  list    - List all campaigns")
        sys.exit(1)
    
    command = sys.argv[1].lower()
    
    if command == "start":
        start_campaign()
    elif command == "stop":
        stop_campaign()
    elif command == "pause":
        pause_campaign()
    elif command == "status":
        get_status()
    elif command == "list":
        list_campaigns()
    else:
        print(f"Unknown command: {command}")
        sys.exit(1)


if __name__ == "__main__":
    main()



================================================
FILE: voice-agents/campaigns/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/getting_started/README.md
================================================
# Getting Started

Your first Atoms agent â€” from zero to a running AI assistant.

## Features

- **OutputAgentNode** â€” The base class for conversational agents
- **generate_response()** â€” Streaming LLM responses
- **AtomsApp** â€” Running the agent server
- **Event handling** â€” Greeting users on join

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage).

## Usage

```bash
uv pip install -r requirements.txt
uv run app.py
```

Connect with the CLI:

```bash
smallestai agent chat
```

## Recommended Usage

- Your starting point â€” the simplest possible Atoms agent with LLM responses
- Learning the core concepts: `OutputAgentNode`, `generate_response()`, `AtomsApp`
- For function calling, [Agent with Tools](../agent_with_tools/) is recommended

## Key Snippets

### Define an Agent

```python
from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.clients.openai import OpenAIClient

class MyAgent(OutputAgentNode):
    def __init__(self):
        super().__init__(name="my-agent")
        self.llm = OpenAIClient(model="gpt-4o-mini")

    async def generate_response(self):
        response = await self.llm.chat(
            messages=self.context.messages,
            stream=True
        )
        async for chunk in response:
            if chunk.content:
                yield chunk.content
```

### Run the Server

```python
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession

async def setup_session(session: AgentSession):
    agent = MyAgent()
    session.add_node(agent)
    await session.start()
    await session.wait_until_complete()

if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()
```

## Structure

```
getting_started/
â”œâ”€â”€ app.py       # Server entry point
â””â”€â”€ my_agent.py  # Simple conversational agent
```

## Example Output

```
Assistant: Hello! I'm your AI assistant. How can I help you today?
You: What's the capital of France?
Assistant: The capital of France is Paris.
```

## API Reference

- [Atoms SDK â€” Quick Start](https://atoms-docs.smallest.ai/dev/introduction/quickstart)
- [Core Concepts â€” Nodes](https://atoms-docs.smallest.ai/dev/introduction/core-concepts/nodes)

## Next Steps

- [Agent with Tools](../agent_with_tools/) â€” Add custom function tools the LLM can call
- [Call Control](../call_control/) â€” End calls and transfer to humans



================================================
FILE: voice-agents/getting_started/app.py
================================================
"""Getting Started - Your first Atoms agent."""

from my_agent import MyAgent
from loguru import logger

from smallestai.atoms.agent.events import SDKEvent, SDKSystemUserJoinedEvent
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession


async def setup_session(session: AgentSession):
    """Configure the agent session."""
    agent = MyAgent()
    session.add_node(agent)
    await session.start()

    @session.on_event("on_event_received")
    async def on_event_received(_, event: SDKEvent):
        logger.info(f"Event received: {event.type}")

        if isinstance(event, SDKSystemUserJoinedEvent):
            greeting = "Hello! I'm your AI assistant. How can I help you today?"
            # Add to context so LLM knows conversation has started
            agent.context.add_message({"role": "assistant", "content": greeting})
            await agent.speak(greeting)

    await session.wait_until_complete()
    logger.success("Session complete")


if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()



================================================
FILE: voice-agents/getting_started/my_agent.py
================================================
"""Simple conversational agent."""

import os
from dotenv import load_dotenv

from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.clients.openai import OpenAIClient

load_dotenv()


class MyAgent(OutputAgentNode):
    """Basic agent that streams LLM responses."""
    
    def __init__(self):
        super().__init__(name="my-agent")
        self.llm = OpenAIClient(
            model="gpt-4o-mini",
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.context.add_message({
            "role": "system",
            "content": "You are a helpful assistant. Be concise and friendly."
        })

    async def generate_response(self):
        """Stream LLM response chunks."""
        response = await self.llm.chat(
            messages=self.context.messages,
            stream=True
        )
        
        full_response = ""
        async for chunk in response:
            if chunk.content:
                full_response += chunk.content
                yield chunk.content
        
        # Add assistant response to context for conversation continuity
        if full_response:
            self.context.add_message({"role": "assistant", "content": full_response})


================================================
FILE: voice-agents/getting_started/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/inbound_ivr/README.md
================================================
# Inbound IVR

IVR-style agent with intent routing, department transfers, and mute/unmute control.

## Features

- **Intent Detection** â€” Understanding caller needs
- **Department Routing** â€” Transfers to appropriate teams
- **Cold vs Warm Transfer** â€” Simple routing vs briefed handoff
- **Mute/Unmute** â€” Controlling caller's microphone
- **IVR Design Patterns** â€” Quick, professional call handling

## Demo

### Simple Routing
```
Agent: Thank you for calling TechCorp. I can help you reach Sales, Support, 
       Billing, or Returns. How may I direct your call?
User: I need to talk to someone about my bill.
Agent: I'll connect you to our Billing department. Is that correct?
User: Yes, please.
Agent: Transferring you now. Please hold.
[Cold transfer to Billing]
```

### Complex Issue (Warm Transfer)
```
Agent: How may I direct your call?
User: I've been having issues with my account for weeks and nobody can help.
Agent: I understand this has been frustrating. Let me connect you with a senior 
       support specialist who can resolve this. I'll brief them on your situation 
       first. Please hold.
[Warm transfer with context: "Recurring account issue, customer frustrated"]
```

### Intent Clarification
```
Agent: How may I direct your call?
User: I need help with something.
Agent: I'd be happy to help. Could you tell me more about what you need?
       Are you looking to make a purchase, get technical support, 
       discuss billing, or process a return?
User: My product isn't working right.
Agent: I'll connect you with our Technical Support team. They're available 24/7.
```

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage).

Configure department phone numbers in `ivr_agent.py`.

## Usage

```bash
uv pip install -r requirements.txt
uv run app.py
```

Connect with the CLI:

```bash
smallestai agent chat
```

## Recommended Usage

- Automated receptionist that routes callers to the right department
- Intent detection with cold and warm transfer patterns
- For sentiment-based escalation, [Background Agent](../background_agent/) is recommended

## Key Snippets

### Department Configuration

Define your departments with phone numbers and metadata:

```python
DEPARTMENTS = {
    "sales": {
        "number": "+1111111111",
        "description": "New orders, pricing, product information",
        "hours": "9 AM - 6 PM EST"
    },
    "support": {
        "number": "+2222222222",
        "description": "Technical issues, troubleshooting",
        "hours": "24/7"
    }
}
```

### Cold Transfer (Quick Routing)

Direct handoff without briefing the receiving agent:

```python
@function_tool()
async def transfer_to_department(self, department: str, reason: str):
    await self.send_event(
        SDKAgentTransferConversationEvent(
            transfer_call_number=DEPARTMENTS[department]["number"],
            transfer_options=TransferOption(
                type=TransferOptionType.COLD_TRANSFER
            ),
            on_hold_music="relaxing_sound"
        )
    )
```

### Warm Transfer (Briefed Handoff)

Brief the receiving agent before connecting:

```python
transfer_options = TransferOption(
    type=TransferOptionType.WARM_TRANSFER,
    private_handoff_option=WarmTransferPrivateHandoffOption(
        type=WarmTransferHandoffOptionType.PROMPT,
        prompt=f"Incoming call for {department}. Reason: {reason}"
    )
)
```

### Mute/Unmute User

Control the caller's microphone:

```python
from smallestai.atoms.agent.events import (
    SDKAgentControlMuteUserEvent,
    SDKAgentControlUnmuteUserEvent,
)

@function_tool()
async def mute_caller(self):
    """Mute during holds or private consultations."""
    await self.send_event(SDKAgentControlMuteUserEvent())

@function_tool()
async def unmute_caller(self):
    """Resume normal conversation."""
    await self.send_event(SDKAgentControlUnmuteUserEvent())
```

## Hold Music Options

| Option | Description |
|--------|-------------|
| `"ringtone"` | Standard phone ringing |
| `"relaxing_sound"` | Calm ambient music |
| `"uplifting_beats"` | Energetic hold music |
| `"none"` | Silence |

## Best Practices

1. **Be Concise** â€” IVR interactions should be quick
2. **Confirm Before Transfer** â€” Always verify the department
3. **Use Warm Transfer for Complex Issues** â€” Provide context
4. **Offer Alternatives** â€” If a department is closed
5. **Track Intents** â€” Log routing for analytics

## Structure

```
inbound_ivr/
â”œâ”€â”€ app.py         # Server entry point with greeting
â””â”€â”€ ivr_agent.py   # IVR agent with routing logic
```

## API Reference

- [Call Control](https://atoms-docs.smallest.ai/dev/build/phone-calling/call-control)
- [Core Concepts â€” Events](https://atoms-docs.smallest.ai/dev/introduction/core-concepts/events)

## Next Steps

- [Call Control](../call_control/) â€” More transfer patterns
- [Background Agent](../background_agent/) â€” Sentiment-based routing



================================================
FILE: voice-agents/inbound_ivr/app.py
================================================
"""Inbound IVR Example - Intent routing and department transfers."""

from loguru import logger

from ivr_agent import IVRAgent

from smallestai.atoms.agent.events import SDKEvent, SDKSystemUserJoinedEvent
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession


async def setup_session(session: AgentSession):
    """Configure IVR agent session."""
    
    agent = IVRAgent()
    session.add_node(agent)
    await session.start()

    @session.on_event("on_event_received")
    async def on_event_received(_, event: SDKEvent):
        logger.info(f"Event received: {event.type}")

        if isinstance(event, SDKSystemUserJoinedEvent):
            greeting = (
                "Thank you for calling TechCorp. "
                "I can help you reach Sales, Support, Billing, or Returns. "
                "How may I direct your call?"
            )
            agent.context.add_message({"role": "assistant", "content": greeting})
            await agent.speak(greeting)

    await session.wait_until_complete()
    
    # Log routing stats
    logger.info(f"Call routed to: {agent.detected_intent or 'no transfer'}")
    logger.info(f"Transfer count: {agent.transfer_count}")
    logger.success("Session complete")


if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()



================================================
FILE: voice-agents/inbound_ivr/ivr_agent.py
================================================
"""IVR Agent with intent routing and department transfers."""

import os
from typing import List

from dotenv import load_dotenv
from loguru import logger

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.clients.types import ToolCall, ToolResult
from smallestai.atoms.agent.events import (
    SDKAgentControlMuteUserEvent,
    SDKAgentControlUnmuteUserEvent,
    SDKAgentEndCallEvent,
    SDKAgentTransferConversationEvent,
    TransferOption,
    TransferOptionType,
    WarmTransferHandoffOptionType,
    WarmTransferPrivateHandoffOption,
)
from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.tools import ToolRegistry, function_tool

load_dotenv()


# Department configuration
DEPARTMENTS = {
    "sales": {
        "number": "+1111111111",
        "description": "New orders, pricing, product information",
        "hours": "9 AM - 6 PM EST"
    },
    "support": {
        "number": "+2222222222",
        "description": "Technical issues, troubleshooting, account problems",
        "hours": "24/7"
    },
    "billing": {
        "number": "+3333333333",
        "description": "Invoices, payments, refunds",
        "hours": "9 AM - 5 PM EST"
    },
    "returns": {
        "number": "+4444444444",
        "description": "Return requests, exchanges, order cancellations",
        "hours": "9 AM - 5 PM EST"
    }
}


class IVRAgent(OutputAgentNode):
    """IVR-style agent that routes callers to the right department.
    
    Demonstrates:
    - Intent detection and routing
    - Department transfers (cold and warm)
    - Mute/unmute user during holds
    - Multi-department configuration
    """

    def __init__(self):
        super().__init__(name="ivr-agent")
        
        self.llm = OpenAIClient(
            model="gpt-4o-mini",
            temperature=0.3,  # Lower temperature for consistent routing
            api_key=os.getenv("OPENAI_API_KEY")
        )

        # Track call state
        self.detected_intent: str = None
        self.transfer_count: int = 0
        
        # Initialize tools
        self.tool_registry = ToolRegistry()
        self.tool_registry.discover(self)
        self.tool_schemas = self.tool_registry.get_schemas()

        # Build department list for prompt
        dept_list = "\n".join([
            f"- {name}: {info['description']}"
            for name, info in DEPARTMENTS.items()
        ])

        self.context.add_message({
            "role": "system",
            "content": f"""You are a professional IVR (Interactive Voice Response) agent for TechCorp.

Your job is to:
1. Greet callers warmly
2. Understand what they need help with
3. Route them to the correct department

AVAILABLE DEPARTMENTS:
{dept_list}

ROUTING RULES:
- Ask clarifying questions if the intent is unclear
- Confirm the department before transferring
- Offer alternatives if the requested department is closed
- Use warm transfer for complex issues, cold transfer for simple routing

IMPORTANT:
- Be concise - IVR interactions should be quick
- Confirm before transferring: "I'll connect you to [department]. Is that correct?"
- If user says "operator" or "representative", transfer to support

Always use the appropriate tool to transfer or end the call.""",
        })

    async def generate_response(self):
        """Generate response with intent routing."""
        
        response = await self.llm.chat(
            messages=self.context.messages,
            stream=True,
            tools=self.tool_schemas
        )

        tool_calls: List[ToolCall] = []
        full_response = ""

        async for chunk in response:
            if chunk.content:
                full_response += chunk.content
                yield chunk.content
            if chunk.tool_calls:
                tool_calls.extend(chunk.tool_calls)

        if full_response and not tool_calls:
            self.context.add_message({"role": "assistant", "content": full_response})

        if tool_calls:
            results: List[ToolResult] = await self.tool_registry.execute(
                tool_calls=tool_calls, parallel=True
            )

            self.context.add_messages([
                {
                    "role": "assistant",
                    "content": "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments),
                            },
                        }
                        for tc in tool_calls
                    ],
                },
                *[
                    {
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": "" if result.content is None else str(result.content),
                    }
                    for tc, result in zip(tool_calls, results)
                ],
            ])

            final_response = await self.llm.chat(
                messages=self.context.messages, stream=True
            )

            async for chunk in final_response:
                if chunk.content:
                    yield chunk.content

    # =========================================
    # DEPARTMENT TOOLS
    # =========================================

    @function_tool()
    def get_departments(self) -> str:
        """Get list of available departments and their descriptions.
        
        Use this to help the caller understand their options.
        """
        dept_info = []
        for name, info in DEPARTMENTS.items():
            dept_info.append(
                f"{name.title()}: {info['description']} (Hours: {info['hours']})"
            )
        return "\n".join(dept_info)

    @function_tool()
    async def transfer_to_department(
        self, 
        department: str, 
        reason: str,
        warm_transfer: bool = False
    ) -> str:
        """Transfer the caller to a specific department.
        
        Args:
            department: The department to transfer to (sales, support, billing, returns)
            reason: Brief summary of why the caller needs this department
            warm_transfer: If True, brief the receiving agent first. Use for complex issues.
        """
        dept = department.lower()
        
        if dept not in DEPARTMENTS:
            return f"Unknown department: {department}. Available: {', '.join(DEPARTMENTS.keys())}"
        
        dept_info = DEPARTMENTS[dept]
        self.detected_intent = dept
        self.transfer_count += 1
        
        logger.info(f"[IVR] Transferring to {dept}: {reason}")
        
        # Determine transfer type
        if warm_transfer:
            transfer_options = TransferOption(
                type=TransferOptionType.WARM_TRANSFER,
                private_handoff_option=WarmTransferPrivateHandoffOption(
                    type=WarmTransferHandoffOptionType.PROMPT,
                    prompt=f"Incoming call for {dept}. Reason: {reason}"
                )
            )
        else:
            transfer_options = TransferOption(
                type=TransferOptionType.COLD_TRANSFER
            )
        
        await self.send_event(
            SDKAgentTransferConversationEvent(
                transfer_call_number=dept_info["number"],
                transfer_options=transfer_options,
                on_hold_music="relaxing_sound"
            )
        )
        
        return f"Transferring to {dept.title()} department."

    # =========================================
    # MUTE/UNMUTE TOOLS
    # =========================================

    @function_tool()
    async def mute_caller(self) -> str:
        """Mute the caller's microphone.
        
        Use this when you need to:
        - Play hold music without background noise
        - Consult with another agent privately
        """
        await self.send_event(SDKAgentControlMuteUserEvent())
        logger.info("[IVR] Caller muted")
        return "Caller microphone muted."

    @function_tool()
    async def unmute_caller(self) -> str:
        """Unmute the caller's microphone.
        
        Use this after muting to resume normal conversation.
        """
        await self.send_event(SDKAgentControlUnmuteUserEvent())
        logger.info("[IVR] Caller unmuted")
        return "Caller microphone unmuted."

    # =========================================
    # CALL CONTROL
    # =========================================

    @function_tool()
    async def end_call(self) -> None:
        """End the call.
        
        Use when:
        - Caller says goodbye
        - Issue is resolved without transfer
        - Caller requests to end the call
        """
        await self.send_event(SDKAgentEndCallEvent())
        return None



================================================
FILE: voice-agents/inbound_ivr/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/interrupt_control/README.md
================================================
# Interrupt Control

Control user interruptions at runtime using mute/unmute events.

## Features

- **SDKAgentControlMuteUserEvent** â€” Mutes user's microphone (platform-level)
- **SDKAgentControlUnmuteUserEvent** â€” Unmutes user's microphone
- **Auto-unmute** after each response to avoid leaving the user stuck
- **Tool-based toggle** â€” LLM decides when to mute/unmute based on context

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage).

## Usage

```bash
uv pip install -r requirements.txt
uv run app.py
```

Connect with the CLI:

```bash
smallestai agent chat
```

**Try these:**
- "Speak 7 words and I'll try to interrupt you"
- "Disable interruptions and tell me a fact"
- "Read me an important message"

## Recommended Usage

| Use Case | Example |
|----------|---------|
| Legal disclaimers | "By continuing, you agree to our terms..." |
| Safety instructions | "Please do not operate machinery while..." |
| Important announcements | "Your account will be charged $50..." |
| Reading long content | Terms and conditions, policies |
| Critical information | Security codes, confirmation numbers |

Use sparingly â€” for normal conversational agents where users expect to interrupt, this is not needed.

## Key Snippets

```
User: "Read me 7 words without interruption"
     â†“
Agent calls set_interruptible(False)
     â†“
Platform mutes user's mic
     â†“
Agent speaks: "Here are exactly seven words for you"
     â†“
Response completes â†’ Auto-unmute
     â†“
User can speak again
```

### Impact

| When Muted | When Unmuted (default) |
|------------|------------------------|
| User's mic is silenced | Normal conversation |
| User cannot interrupt | User can interrupt anytime |
| Agent speaks uninterrupted | Natural back-and-forth |
| Auto-unmutes after response | â€” |

### Key Code

```python
from smallestai.atoms.agent.events import (
    SDKAgentControlMuteUserEvent,
    SDKAgentControlUnmuteUserEvent,
)

@function_tool()
async def set_interruptible(self, enabled: bool) -> str:
    if enabled:
        await self.send_event(SDKAgentControlUnmuteUserEvent())
        self.user_muted = False
        return "User unmuted."
    else:
        await self.send_event(SDKAgentControlMuteUserEvent())
        self.user_muted = True
        return "User muted. Speak now."
```

### Auto-Unmute Pattern

Always unmute after the response to avoid leaving user stuck:

```python
async def generate_response(self):
    # ... generate response ...
    
    # Auto-unmute at end
    if self.user_muted:
        await self.send_event(SDKAgentControlUnmuteUserEvent())
        self.user_muted = False
```

## Best Practices

1. **Keep muted sections short** â€” Don't mute for long monologues
2. **Always auto-unmute** â€” Never leave user permanently muted
3. **Don't announce muting** â€” Just mute and speak the content
4. **Use sparingly** â€” Only for truly important content
5. **Test on platform** â€” Mute/unmute are platform-level events

## Limitations

- Mute/unmute only works on deployed agents (not local CLI)
- User is fully silenced â€” no partial muting
- Mute persists until explicitly unmuted or response ends

## Structure

```
interrupt_control/
â”œâ”€â”€ app.py                 # Server entry point
â””â”€â”€ configurable_agent.py  # Agent with mute/unmute tools
```

## API Reference

- [Core Concepts â€” Events](https://atoms-docs.smallest.ai/dev/introduction/core-concepts/events)
- [Core Concepts â€” Nodes](https://atoms-docs.smallest.ai/dev/introduction/core-concepts/nodes)

## Next Steps

- [Background Agent](../background_agent/) â€” Sentiment detection
- [Inbound IVR](../inbound_ivr/) â€” Call routing



================================================
FILE: voice-agents/interrupt_control/app.py
================================================
"""Interrupt Control Example - Mute/unmute to block or allow user interruptions."""

from loguru import logger

from configurable_agent import ConfigurableAgent

from smallestai.atoms.agent.events import SDKEvent, SDKSystemUserJoinedEvent
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession


async def setup_session(session: AgentSession):
    """Configure agent with interrupt control (mute/unmute) capabilities."""
    
    agent = ConfigurableAgent()
    session.add_node(agent)
    await session.start()

    @session.on_event("on_event_received")
    async def on_event_received(_, event: SDKEvent):
        logger.info(f"Event received: {event.type}")

        if isinstance(event, SDKSystemUserJoinedEvent):
            greeting = (
                "Hello! I'm your assistant with configurable settings. "
                "I can control whether you can interrupt me during important messages. "
                "How can I help you today?"
            )
            agent.context.add_message({"role": "assistant", "content": greeting})
            await agent.speak(greeting)

    await session.wait_until_complete()
    logger.success("Session complete")


if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()



================================================
FILE: voice-agents/interrupt_control/configurable_agent.py
================================================
"""Agent with runtime-configurable settings."""

import os
from typing import List

from dotenv import load_dotenv
from loguru import logger

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.clients.types import ToolCall, ToolResult
from smallestai.atoms.agent.events import (
    SDKAgentEndCallEvent,
    SDKAgentControlMuteUserEvent,
    SDKAgentControlUnmuteUserEvent,
)
from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.tools import ToolRegistry, function_tool

load_dotenv()


class ConfigurableAgent(OutputAgentNode):
    """Agent that can dynamically mute/unmute user to control interrupts.
    
    Key mechanism:
    - SDKAgentControlMuteUserEvent: Mute user mic (no interrupts possible)
    - SDKAgentControlUnmuteUserEvent: Unmute user mic (normal behavior)
    - Auto-unmutes after each response completes
    """

    def __init__(self):
        super().__init__(name="configurable-agent")
        
        self.llm = OpenAIClient(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=os.getenv("OPENAI_API_KEY")
        )

        # Track if user is muted (for auto-unmute)
        self.user_muted = False

        # Initialize tools
        self.tool_registry = ToolRegistry()
        self.tool_registry.discover(self)
        self.tool_schemas = self.tool_registry.get_schemas()

        self.context.add_message({
            "role": "system",
            "content": """You are a helpful assistant that can control whether the user can interrupt you.

Tools:
- set_interruptible(False): Mute user so they cannot interrupt your next response
- set_interruptible(True): Unmute user (normal mode)
- check_settings: See current state

RULES:
1. When asked to speak without interruption: call set_interruptible(False), then speak
2. Do NOT announce that you're muting - just do it and speak the content
3. User is auto-unmuted after each response, so mute only lasts for one response

Example: "say 7 words without interruption"
â†’ Call set_interruptible(False)
â†’ Then ONLY say: "Here are exactly seven words for you"
â†’ User auto-unmutes after you finish""",
        })

    async def generate_response(self):
        """Generate response, auto-unmute at end if muted."""
        
        response = await self.llm.chat(
            messages=self.context.messages,
            stream=True,
            tools=self.tool_schemas
        )

        tool_calls: List[ToolCall] = []
        full_response = ""

        async for chunk in response:
            if chunk.content:
                full_response += chunk.content
                yield chunk.content
            if chunk.tool_calls:
                tool_calls.extend(chunk.tool_calls)

        if full_response and not tool_calls:
            self.context.add_message({"role": "assistant", "content": full_response})

        if tool_calls:
            results: List[ToolResult] = await self.tool_registry.execute(
                tool_calls=tool_calls, parallel=True
            )

            self.context.add_messages([
                {
                    "role": "assistant",
                    "content": "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments),
                            },
                        }
                        for tc in tool_calls
                    ],
                },
                *[
                    {"role": "tool", "tool_call_id": tc.id, "content": "" if result.content is None else str(result.content)}
                    for tc, result in zip(tool_calls, results)
                ],
            ])

            final_response = await self.llm.chat(
                messages=self.context.messages, stream=True
            )

            async for chunk in final_response:
                if chunk.content:
                    yield chunk.content
        
        # Auto-unmute after response finishes
        if self.user_muted:
            await self.send_event(SDKAgentControlUnmuteUserEvent())
            self.user_muted = False
            logger.info("[ConfigurableAgent] Response done, user unmuted")

    @function_tool()
    def check_settings(self) -> str:
        """Check if user is currently muted."""
        return f"User muted: {self.user_muted}"

    @function_tool()
    async def set_interruptible(self, enabled: bool) -> str:
        """Mute or unmute the user.
        
        Args:
            enabled: True = user can interrupt (unmuted), False = user cannot interrupt (muted)
        """
        if enabled:
            # Unmute
            await self.send_event(SDKAgentControlUnmuteUserEvent())
            self.user_muted = False
            logger.info("[ConfigurableAgent] User unmuted")
            return "User unmuted. They can now interrupt."
        else:
            # Mute
            await self.send_event(SDKAgentControlMuteUserEvent())
            self.user_muted = True
            logger.info("[ConfigurableAgent] User muted")
            return "User muted. Speak your message now - they cannot interrupt. Will auto-unmute when done."

    @function_tool()
    async def end_call(self) -> None:
        """End the call."""
        await self.send_event(SDKAgentEndCallEvent())
        return None



================================================
FILE: voice-agents/interrupt_control/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/knowledge_base_rag/README.md
================================================
# Knowledge Base RAG

Create and populate Knowledge Bases for RAG-powered agents.

## Features

- **PDF Upload** â€” Upload PDF files via API
- **URL Scraping** â€” Scrape web pages via API
- **KB Management** â€” Create, list, get, delete Knowledge Bases
- **Item Status Tracking** â€” Monitor processing status of uploaded content

## Requirements

> Make sure you've run `uv venv && uv pip install -r requirements.txt` at the repo root first. See the [main README](../../README.md#usage).

## Usage

```bash
uv pip install -r requirements.txt
uv run setup_kb.py
```

## Recommended Usage

- Giving your voice agent access to custom documents and web content for RAG-powered answers
- PDF upload, URL scraping, and KB lifecycle management via API

## Key Snippets

### PDF Upload

```python
from smallestai.atoms import KB

kb = KB()

# Create KB
result = kb.create(name="My KB", description="My docs")
kb_id = result["data"]["_id"]

# Upload PDF
kb.add_file(kb_id, "document.pdf")

# Check status
items = kb.get_items(kb_id)
for item in items["data"]:
    print(f"{item['fileName']}: {item['processingStatus']}")
```

### URL Scraping

```python
from smallestai.atoms import KB

kb = KB()

# Create KB
result = kb.create(name="My KB")
kb_id = result["data"]["_id"]

# Scrape URLs
kb.scrape_urls(kb_id, [
    "https://example.com/docs",
    "https://example.com/faq"
])

# Check status
scraped = kb.get_scraped_urls(kb_id)
```

### KB Management

```python
kb = KB()

# List all KBs
kb.list()

# Get KB details  
kb.get(kb_id)

# Get items
kb.get_items(kb_id)

# Delete KB
kb.delete(kb_id)
```

## Notes

- Only PDF files are supported for upload
- Text upload is not yet available via API (use dashboard)
- Link KB to agent via `globalKnowledgeBaseId` in dashboard

## API Reference

- [Atoms SDK â€” Quick Start](https://atoms-docs.smallest.ai/dev/introduction/quickstart)

## Next Steps

- Link the KB to your agent in the dashboard to enable RAG
- See [Agent with Tools](../agent_with_tools/) for adding function tools alongside KB



================================================
FILE: voice-agents/knowledge_base_rag/requirements.txt
================================================
# Extra dependency for PDF generation in the demo script
smallestai>=4.3.0
reportlab



================================================
FILE: voice-agents/knowledge_base_rag/setup_kb.py
================================================
"""
Knowledge Base Setup

Two ways to add content to a Knowledge Base:
1. PDF Upload - Upload PDF files via API
2. URL Scraping - Scrape web pages via API

Usage:
    python setup_kb.py
"""

import os
from dotenv import load_dotenv
from smallestai.atoms import KB

load_dotenv()


def example_pdf_upload():
    """Upload a PDF file to Knowledge Base."""
    
    print("=" * 50)
    print("PDF UPLOAD EXAMPLE")
    print("=" * 50)
    
    kb = KB()
    
    # Create KB
    result = kb.create(
        name="Starbucks Menu",
        description="Coffee drinks, food, and prices"
    )
    kb_id = result["data"]["_id"]
    print(f"Created KB: {kb_id}")
    
    # Create sample PDF
    try:
        from reportlab.lib.pagesizes import letter
        from reportlab.pdfgen import canvas
        
        pdf_path = "/tmp/menu.pdf"
        c = canvas.Canvas(pdf_path, pagesize=letter)
        
        c.setFont("Helvetica-Bold", 24)
        c.drawString(200, 750, "STARBUCKS MENU")
        
        c.setFont("Helvetica", 12)
        items = [
            "Caffe Latte: $5.25",
            "Cappuccino: $4.95", 
            "Frappuccino: $6.25",
            "Cold Brew: $4.75",
            "Croissant: $3.75",
            "Banana Bread: $3.95",
        ]
        y = 700
        for item in items:
            c.drawString(60, y, item)
            y -= 25
        
        c.save()
        
        # Upload PDF
        kb.add_file(kb_id, pdf_path)
        print(f"Uploaded: {pdf_path}")
        
    except ImportError:
        print("Install reportlab for PDF creation: pip install reportlab")
    
    # Check status
    items = kb.get_items(kb_id)
    for item in items.get("data", []):
        print(f"  {item['fileName']}: {item['processingStatus']}")
    
    return kb_id


def example_url_scrape():
    """Scrape a URL into Knowledge Base."""
    
    print("\n" + "=" * 50)
    print("URL SCRAPING EXAMPLE")
    print("=" * 50)
    
    kb = KB()
    
    # Create KB
    result = kb.create(
        name="Python Docs",
        description="Python programming documentation"
    )
    kb_id = result["data"]["_id"]
    print(f"Created KB: {kb_id}")
    
    # Scrape URL
    url = "https://en.wikipedia.org/wiki/Python_(programming_language)"
    print(f"Scraping: {url}")
    
    kb.scrape_urls(kb_id, [url])
    print("Scrape initiated")
    
    # Check scraped URLs
    scraped = kb.get_scraped_urls(kb_id)
    for host in scraped.get("data", []):
        for u in host.get("scrapedUrls", []):
            print(f"  {u['url']}: {u['processingStatus']}")
    
    return kb_id


if __name__ == "__main__":
    print("\nKnowledge Base Examples\n")
    
    pdf_kb = example_pdf_upload()
    url_kb = example_url_scrape()
    
    print("\n" + "=" * 50)
    print("DONE")
    print("=" * 50)
    print(f"PDF KB:  {pdf_kb}")
    print(f"URL KB:  {url_kb}")
    print("\nLink these to your agent in the dashboard.")



================================================
FILE: voice-agents/language_switching/README.md
================================================
[Binary file]


================================================
FILE: voice-agents/language_switching/app.py
================================================
"""Language Switching Example - Multi-language support with auto-detection."""

from loguru import logger

from language_detector import LanguageDetector
from profanity_filter import ProfanityFilter
from support_agent import SupportAgent

from smallestai.atoms.agent.events import SDKEvent, SDKSystemUserJoinedEvent
from smallestai.atoms.agent.server import AtomsApp
from smallestai.atoms.agent.session import AgentSession


async def setup_session(session: AgentSession):
    """Configure multi-node pipeline with explicit edges.
    
    Pipeline Architecture:
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Root   â”‚â”€â”€â”€â–ºâ”‚ LanguageDetector â”‚â”€â”€â”€â–ºâ”‚ SupportAgent â”‚â”€â”€â”€â–ºâ”‚ ProfanityFilter â”‚â”€â”€â”€â–º Sink
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    Events flow:
    1. Root receives events from WebSocket
    2. LanguageDetector detects language, forwards events
    3. SupportAgent generates responses, queries language info
    4. ProfanityFilter sanitizes responses before TTS
    5. Sink sends to WebSocket
    """
    
    # Create nodes
    language_detector = LanguageDetector()
    support_agent = SupportAgent(language_detector=language_detector)
    profanity_filter = ProfanityFilter()
    
    # Add nodes to session
    session.add_node(language_detector)
    session.add_node(support_agent)
    session.add_node(profanity_filter)
    
    # Create pipeline edges
    # Note: Nodes without incoming edges connect to Root automatically
    # Nodes without outgoing edges connect to Sink automatically
    session.add_edge(language_detector, support_agent)
    session.add_edge(support_agent, profanity_filter)
    
    await session.start()

    @session.on_event("on_event_received")
    async def on_event_received(_, event: SDKEvent):
        logger.info(f"Event received: {event.type}")

        if isinstance(event, SDKSystemUserJoinedEvent):
            greeting = (
                "Hello! Welcome to our support line. "
                "I can help you in multiple languages. "
                "How can I assist you today?"
            )
            support_agent.context.add_message({"role": "assistant", "content": greeting})
            await support_agent.speak(greeting)

    await session.wait_until_complete()
    
    # Log pipeline stats
    logger.info(f"Language detected: {language_detector.get_primary_language()}")
    logger.info(f"Profanity filtered: {profanity_filter.filtered_count} times")
    logger.success("Session complete")


if __name__ == "__main__":
    app = AtomsApp(setup_handler=setup_session)
    app.run()



================================================
FILE: voice-agents/language_switching/language_detector.py
================================================
"""Language detection node that processes user input before the main agent."""

import os
from dotenv import load_dotenv
from loguru import logger

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.events import (
    SDKAgentTranscriptUpdateEvent,
    SDKEvent,
)
from smallestai.atoms.agent.nodes.base import Node

load_dotenv()


class LanguageDetector(Node):
    """Detects language and enriches events for downstream nodes.
    
    This node:
    - Intercepts transcript updates
    - Detects the language of user messages
    - Stores language info for other nodes to query
    - Passes events downstream unchanged
    
    Use cases:
    - Multi-language support
    - Language-based routing
    - Analytics
    """

    def __init__(self):
        super().__init__(name="language-detector")
        self.llm = OpenAIClient(
            model="gpt-4o-mini",
            api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.detected_language: str = "english"
        self.language_confidence: float = 1.0
        self.language_history: list = []

    async def process_event(self, event: SDKEvent):
        """Process event and pass it downstream."""
        
        if isinstance(event, SDKAgentTranscriptUpdateEvent):
            if event.role == "user":
                await self._detect_language(event.content)
        
        # IMPORTANT: Forward event to children
        await self.send_event(event)

    async def _detect_language(self, text: str):
        """Detect language of the text."""
        try:
            response = await self.llm.chat(
                messages=[
                    {
                        "role": "system",
                        "content": """Detect the language of the text.
Respond with JSON: {"language": "english", "confidence": 0.95}
Common languages: english, spanish, french, german, portuguese, chinese, japanese, korean, hindi, arabic
Only respond with the JSON, nothing else."""
                    },
                    {"role": "user", "content": text}
                ],
                stream=False
            )
            
            import json
            result = json.loads(response.content)
            
            self.detected_language = result.get("language", "english").lower()
            self.language_confidence = result.get("confidence", 0.5)
            self.language_history.append(self.detected_language)
            
            logger.info(
                f"[LanguageDetector] Detected: {self.detected_language} "
                f"(confidence: {self.language_confidence:.0%})"
            )
            
        except Exception as e:
            logger.error(f"[LanguageDetector] Detection failed: {e}")
            self.detected_language = "english"
            self.language_confidence = 0.5

    def get_primary_language(self) -> str:
        """Get the most commonly detected language in this session."""
        if not self.language_history:
            return "english"
        
        from collections import Counter
        counts = Counter(self.language_history)
        return counts.most_common(1)[0][0]



================================================
FILE: voice-agents/language_switching/profanity_filter.py
================================================
"""Profanity filter node that sanitizes agent responses."""

import re
from loguru import logger

from smallestai.atoms.agent.events import (
    SDKAgentLLMResponseChunkEvent,
    SDKEvent,
)
from smallestai.atoms.agent.nodes.base import Node


# Simple profanity list (in production, use a proper library)
PROFANITY_WORDS = {
    "damn", "hell", "crap"  # Add more as needed
}


class ProfanityFilter(Node):
    """Filters profanity from agent responses before they reach TTS.
    
    This node:
    - Intercepts LLM response chunks
    - Sanitizes any inappropriate language
    - Passes clean text downstream
    
    Position in pipeline: After OutputAgentNode, before Sink
    """

    def __init__(self):
        super().__init__(name="profanity-filter")
        self.filtered_count: int = 0

    async def process_event(self, event: SDKEvent):
        """Filter response chunks and pass downstream."""
        
        if isinstance(event, SDKAgentLLMResponseChunkEvent):
            # Filter the text
            filtered_text = self._filter_text(event.text)
            
            if filtered_text != event.text:
                self.filtered_count += 1
                logger.warning(f"[ProfanityFilter] Filtered content")
            
            # Create new event with filtered text
            filtered_event = SDKAgentLLMResponseChunkEvent(text=filtered_text)
            await self.send_event(filtered_event)
        else:
            # Pass through unchanged
            await self.send_event(event)

    def _filter_text(self, text: str) -> str:
        """Replace profanity with asterisks."""
        result = text
        
        for word in PROFANITY_WORDS:
            # Case-insensitive replacement
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            replacement = "*" * len(word)
            result = pattern.sub(replacement, result)
        
        return result



================================================
FILE: voice-agents/language_switching/requirements.txt
================================================
# No extra dependencies â€” base SDK from root requirements.txt is sufficient
smallestai>=4.3.0



================================================
FILE: voice-agents/language_switching/support_agent.py
================================================
"""Support agent that works in a processing pipeline."""

import os
from typing import TYPE_CHECKING, List

from dotenv import load_dotenv

from smallestai.atoms.agent.clients.openai import OpenAIClient
from smallestai.atoms.agent.clients.types import ToolCall, ToolResult
from smallestai.atoms.agent.events import SDKAgentEndCallEvent
from smallestai.atoms.agent.nodes import OutputAgentNode
from smallestai.atoms.agent.tools import ToolRegistry, function_tool

if TYPE_CHECKING:
    from language_detector import LanguageDetector

load_dotenv()


class SupportAgent(OutputAgentNode):
    """Support agent that adapts based on pipeline node data.
    
    This agent:
    - Receives events from LanguageDetector node
    - Can query language info to adapt responses
    - Sends responses through ProfanityFilter before TTS
    """

    def __init__(self, language_detector: "LanguageDetector"):
        super().__init__(name="support-agent")
        
        self.language_detector = language_detector
        
        self.llm = OpenAIClient(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=os.getenv("OPENAI_API_KEY")
        )

        # Initialize tools
        self.tool_registry = ToolRegistry()
        self.tool_registry.discover(self)
        self.tool_schemas = self.tool_registry.get_schemas()

        self.context.add_message({
            "role": "system",
            "content": """You are a helpful multilingual support agent.

You can detect and respond in multiple languages:
- Use the get_user_language tool to check what language the user is speaking
- Respond in the same language as the user when appropriate
- Default to English if unsure

Be helpful, concise, and friendly.""",
        })

    async def generate_response(self):
        """Generate language-aware responses."""
        
        response = await self.llm.chat(
            messages=self.context.messages,
            stream=True,
            tools=self.tool_schemas
        )

        tool_calls: List[ToolCall] = []
        full_response = ""

        async for chunk in response:
            if chunk.content:
                full_response += chunk.content
                yield chunk.content
            if chunk.tool_calls:
                tool_calls.extend(chunk.tool_calls)

        if full_response and not tool_calls:
            self.context.add_message({"role": "assistant", "content": full_response})

        if tool_calls:
            results: List[ToolResult] = await self.tool_registry.execute(
                tool_calls=tool_calls, parallel=True
            )

            self.context.add_messages([
                {
                    "role": "assistant",
                    "content": "",
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.name,
                                "arguments": str(tc.arguments),
                            },
                        }
                        for tc in tool_calls
                    ],
                },
                *[
                    {"role": "tool", "tool_call_id": tc.id, "content": "" if result.content is None else str(result.content)}
                    for tc, result in zip(tool_calls, results)
                ],
            ])

            final_response = await self.llm.chat(
                messages=self.context.messages, stream=True
            )

            async for chunk in final_response:
                if chunk.content:
                    yield chunk.content

    @function_tool()
    def get_user_language(self) -> str:
        """Get the detected language of the user.
        
        Use this to determine which language to respond in.
        Returns the detected language and confidence level.
        """
        return (
            f"User is speaking {self.language_detector.detected_language} "
            f"(confidence: {self.language_detector.language_confidence:.0%}). "
            f"Primary language this session: {self.language_detector.get_primary_language()}"
        )

    @function_tool()
    async def end_call(self) -> None:
        """End the call."""
        await self.send_event(SDKAgentEndCallEvent())
        return None


